"""
core/typo_normalizer_safe.py
PRISM Phase 0.3.4 P2.4 - ë„ë©”ì¸ ì‚¬ì „ í™•ëŒ€

âœ… ë³€ê²½ì‚¬í•­:
1. OCR ì˜¤íƒˆì ì¶”ê°€ (GPT ì œì•ˆ)
2. ë„ë©”ì¸ íŠ¹í™” êµì •
"""

import re
import logging

logger = logging.getLogger(__name__)


class TypoNormalizer:
    """Phase 0.3.4 P2.4 ì˜¤íƒˆì ì •ê·œí™” (ë„ë©”ì¸ ì‚¬ì „ í™•ëŒ€)"""
    
    # CRITICAL_FIXES 10ê°œ
    CRITICAL_FIXES = {
        "ê²½ìš©ë²”ìœ„": "ì ìš©ë²”ìœ„",
        "ì„ìš©í•œ": "ì„ìš©ê¶Œ",
        "ì§„ë³¸ë³´ì •": "ì‹ ë¶„ë³´ì¥",
        "ì„±ê³¼ê³„ì¢ŒëŒ€ìƒì": "ì„±ê³¼ê°œì„ ëŒ€ìƒì",
        "ë”°ë¥¸ ì •í•œë‹¤": "ë”°ë¡œ ì •í•œë‹¤",
        "ì •ìš©ë²”ìœ„": "ì ìš©ë²”ìœ„",
        "ì§„ë³´ë³´ì •": "ì‹ ë¶„ë³´ì¥",
        "ê³µê¸‰ì¸ì‚¬ìœ„ì›íšŒ": "ìƒê¸‰ì¸ì‚¬ìœ„ì›íšŒ",
        "ì„±ê³¼ê³„ì œì„ ë°œì": "ì„±ê³¼ê°œì„ ëŒ€ìƒì",
        "ì„±ê³¼ê³„ì œì„ ë°œì‹¬ì‚¬": "ì„±ê³¼ê°œì„ ëŒ€ìƒì",
    }
    
    # GPT ì œì•ˆ: ë„ë©”ì¸ ì‚¬ì „ í™•ëŒ€
    DOMAIN_FIXES = {
        "ê¸° ë³¸ ì • ì‹ ": "ê¸°ë³¸ì •ì‹ ",
        "ìš©ìƒ": "í†µìƒ",
        "ì „ì¡±": "ì „ì†",
        "í•´íŒŒêµ°ì§ì±„ìš©": "ì˜ˆë¹„êµ°ì§€íœ˜ê´€",
        "ìˆ˜ìŠµì„ìš©ì”ë£Œ": "ìˆ˜ìŠµì„ìš©ì",
        "ë³‘ì— ê³„ì‚°": "í¬í•¨ ê³„ì‚°",
        "ë¶€ë¬´": "ë³µë¬´",
        "ì „ì¼ì—°êµ¬ì›": "ì „ì„ì—°êµ¬ì›",
        "ë˜ëŠ”ì˜": "ê³ ë„ì˜",
        "ì‹œì²¨ì •": "ì‹œí—˜ì„±ì ",
        "ì±„ìš©ì†Œì”¨ê²°ê³¼": "ì±„ìš©ì‹ ì²´ê²€ì‚¬",
        "ì‹ ì›ì¡°ì§ê²°ê³¼": "ì‹ ì›ì¡°íšŒê²°ê³¼",
        "ê°ì‚¬ìœ„ì›": "ë¶€íŒ¨ë°©ì§€",
    }
    
    SAFE_PATTERNS = [
        (r'\s+', ' '),
        (r'\n{3,}', '\n\n'),
        (r'^\s+', ''),
        (r'\s+$', ''),
    ]
    
    OCR_PATTERNS = [
        (r'(\d+)\s*\.\s*(\d+)\s*\.\s*(\d+)', r'\1.\2.\3'),
        (r'ì œ\s*(\d+)\s*ì¡°', r'ì œ\1ì¡°'),
        (r'ì œ\s*(\d+)\s*í•­', r'ì œ\1í•­'),
    ]
    
    def __init__(self):
        logger.info("âœ… TypoNormalizer Phase 0.3.4 P2.4 ì´ˆê¸°í™”")
        logger.info(f"   ğŸ“– CRITICAL_FIXES: {len(self.CRITICAL_FIXES)}ê°œ ë£°")
        logger.info(f"   ğŸ“– DOMAIN_FIXES: {len(self.DOMAIN_FIXES)}ê°œ ë£° (ì‹ ê·œ)")
        logger.info(f"   ğŸ“– Safe: {len(self.SAFE_PATTERNS)}ê°œ ë£°")
        logger.info(f"   ğŸ“– OCR: {len(self.OCR_PATTERNS)}ê°œ ë£°")
    
    def normalize(self, text: str) -> str:
        """ì •ê·œí™” ì‹¤í–‰"""
        if not text:
            return text
        
        original_len = len(text)
        result = text
        
        # 1. CRITICAL_FIXES (ë‹¨ì–´ ê²½ê³„ ë³´ì¡´)
        critical_count = 0
        for wrong, correct in self.CRITICAL_FIXES.items():
            matches = result.count(wrong)
            if matches > 0:
                result = result.replace(wrong, correct)
                critical_count += matches
        
        # 2. DOMAIN_FIXES (GPT ì œì•ˆ)
        domain_count = 0
        for wrong, correct in self.DOMAIN_FIXES.items():
            matches = result.count(wrong)
            if matches > 0:
                result = result.replace(wrong, correct)
                domain_count += matches
        
        # 3. Safe íŒ¨í„´
        safe_count = 0
        for pattern, replacement in self.SAFE_PATTERNS:
            before = len(re.findall(pattern, result))
            if before > 0:
                result = re.sub(pattern, replacement, result)
                safe_count += before
        
        # 4. OCR íŒ¨í„´
        ocr_count = 0
        for pattern, replacement in self.OCR_PATTERNS:
            before = len(re.findall(pattern, result))
            if before > 0:
                result = re.sub(pattern, replacement, result)
                ocr_count += before
        
        logger.info(f"âœ… ì •ê·œí™” ì™„ë£Œ:")
        logger.info(f"   Critical: {len(self.CRITICAL_FIXES)}ê°œ ë£° / {critical_count}ê±´ ì¹˜í™˜")
        logger.info(f"   Domain: {len(self.DOMAIN_FIXES)}ê°œ ë£° / {domain_count}ê±´ ì¹˜í™˜")
        logger.info(f"   Safe: {len(self.SAFE_PATTERNS)}ê°œ ë£° / {safe_count}ê±´ ì¹˜í™˜")
        logger.info(f"   OCR: {len(self.OCR_PATTERNS)}ê°œ ë£° / {ocr_count}ê±´ ì¹˜í™˜")
        logger.info(f"   ê¸¸ì´: {original_len} â†’ {len(result)} ({len(result)-original_len:+d})")
        
        return result