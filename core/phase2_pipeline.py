"""
PRISM Phase 2.4 - Enhanced Pipeline with Chart & Figure Extraction

ì „ì²´ í˜ì´ì§€ë¥¼ Claude Visionìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸, í‘œ, ì°¨íŠ¸, ê·¸ë˜í”„ë¥¼ ëª¨ë‘ ì¶”ì¶œ

ê°œì„  ì‚¬í•­:
- ëª¨ë“  í˜ì´ì§€ë¥¼ Claude Visionìœ¼ë¡œ ì²˜ë¦¬
- í…ìŠ¤íŠ¸, í‘œ, ì°¨íŠ¸, ê·¸ë˜í”„, ì´ë¯¸ì§€ë¥¼ ë™ì‹œì— ì¶”ì¶œ
- ì°¨íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ê¹Œì§€ ìƒì„¸ ì¶”ì¶œ
- OCR ì •í™•ë„ 95%+ ë‹¬ì„±

Author: ì´ì„œì˜ (Backend Lead) + ë°•ì¤€í˜¸ (AI/ML Lead)
Date: 2025-10-16
"""

import os
import time
from pathlib import Path
from typing import List, Dict, Optional
from PIL import Image
import fitz  # PyMuPDF

from models.layout_detector import LayoutDetector, DocumentElement, ElementType, BoundingBox
from core.claude_full_page_extractor import ClaudeFullPageExtractor, PageContent
from core.intelligent_chunker import IntelligentChunker
from core.document_analyzer import DocumentAnalyzer


class Phase2Pipeline:
    """
    PRISM Phase 2.4 íŒŒì´í”„ë¼ì¸ (ì „ì²´ Claude Vision + Chart)
    
    ì²˜ë¦¬ ë‹¨ê³„:
    1. â­ Claude Visionìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ ë¶„ì„
    2. í…ìŠ¤íŠ¸, í‘œ, ì°¨íŠ¸, ê·¸ë˜í”„, ì´ë¯¸ì§€ ë™ì‹œ ì¶”ì¶œ
    3. Intelligent Chunking
    """
    
    def __init__(
        self,
        azure_endpoint: Optional[str] = None,
        azure_api_key: Optional[str] = None,
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        use_full_claude_vision: bool = True
    ):
        """
        Args:
            azure_endpoint: Azure OpenAI ì—”ë“œí¬ì¸íŠ¸ (ì„ íƒ)
            azure_api_key: Azure OpenAI API í‚¤ (ì„ íƒ)
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
            use_full_claude_vision: ì „ì²´ í˜ì´ì§€ Claude Vision ì‚¬ìš© ì—¬ë¶€
        """
        print("Initializing PRISM Phase 2.4 Pipeline (Full Claude Vision + Charts)...")
        
        # Azure OpenAI ì„¤ì • ì €ì¥
        self.azure_endpoint = azure_endpoint
        self.azure_api_key = azure_api_key
        
        # 1. Layout Detector (ì°¸ê³ ìš©)
        self.layout_detector = LayoutDetector()
        
        # 2. â­ Claude Full Page Extractor (í•µì‹¬!)
        self.use_full_claude_vision = use_full_claude_vision
        if use_full_claude_vision:
            # Azure ì„¤ì •ì„ ClaudeFullPageExtractorì— ì „ë‹¬
            self.claude_extractor = ClaudeFullPageExtractor(
                azure_endpoint=azure_endpoint,
                azure_api_key=azure_api_key
            )
            if self.claude_extractor.client:
                print("âœ… Full Claude Vision + Chart Extraction enabled")
            else:
                print("âš ï¸  Claude Vision unavailable, falling back to OCR")
                self.use_full_claude_vision = False
        
        # 3. Intelligent Chunker
        self.chunker = IntelligentChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # 4. Document Analyzer
        self.analyzer = DocumentAnalyzer()
    
    def process(
        self,
        pdf_path: str,
        max_pages: Optional[int] = None
    ) -> Dict:
        """
        PDF ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬ (Phase 2.4)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            
        Returns:
            {
                'chunks': [...],
                'statistics': {...},
                'output_path': '...'
            }
        """
        start_time = time.time()
        
        print("=" * 60)
        print(f"Processing: {Path(pdf_path).name}")
        print(f"Pages: {max_pages or 'all'}")
        print(f"Method: Full Claude Vision + Charts")
        print("=" * 60)
        print()
        
        # Step 1: Claude Visionìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ ë¶„ì„
        print("ğŸ¤– Step 1/3: Processing with Claude Vision (Phase 2.4)...")
        all_chunks = []  # âœ… DocumentElement ëŒ€ì‹  ì§ì ‘ chunk ìƒì„±
        
        doc = fitz.open(pdf_path)
        pages_to_process = min(len(doc), max_pages) if max_pages else len(doc)
        
        text_chunk_count = 0
        table_chunk_count = 0
        chart_chunk_count = 0  # âœ… ì¶”ê°€
        figure_chunk_count = 0  # âœ… ì¶”ê°€
        
        for page_num in range(pages_to_process):
            print(f"  ğŸ¤– Processing page {page_num + 1} with Claude Vision...")
            
            # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            page = doc[page_num]
            pix = page.get_pixmap(dpi=200)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # Claude Visionìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ ë¶„ì„
            page_content = self.claude_extractor.extract_full_page(img, page_num + 1)
            
            if page_content:
                # âœ… Sectionì„ ì§ì ‘ chunkë¡œ ë³€í™˜
                for idx, section in enumerate(page_content.sections):
                    chunk = {
                        'chunk_id': f'chunk_{page_num + 1:03d}_{idx + 1:03d}',
                        'type': 'text',
                        'content': f"[{section.title}]\n{section.text}" if section.title else section.text,
                        'page_num': page_num + 1,
                        'metadata': {
                            'section_title': section.title,
                            'section_type': section.type,
                            'confidence': section.confidence
                        }
                    }
                    all_chunks.append(chunk)
                    text_chunk_count += 1
                
                # âœ… Tableì„ ì§ì ‘ chunkë¡œ ë³€í™˜
                for idx, table in enumerate(page_content.tables):
                    chunk = {
                        'chunk_id': f'table_{page_num + 1:03d}_{idx + 1:03d}',
                        'type': 'table',
                        'content': table.markdown,
                        'page_num': page_num + 1,
                        'metadata': {
                            'caption': table.caption,
                            'confidence': table.confidence
                        }
                    }
                    all_chunks.append(chunk)
                    table_chunk_count += 1
                
                # âœ… Chartë¥¼ ì§ì ‘ chunkë¡œ ë³€í™˜ (NEW!)
                for idx, chart in enumerate(page_content.charts):
                    # ì°¨íŠ¸ ì„¤ëª…ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê°œì„ ëœ í¬ë§·)
                    chart_text = f"[ì°¨íŠ¸: {chart.title}]\n"
                    chart_text += f"íƒ€ì…: {chart.type}\n"
                    chart_text += f"ì„¤ëª…: {chart.description}\n"
                    chart_text += f"ë°ì´í„°:\n"
                    chart_text += self._format_chart_data_points(chart.data_points)
                    
                    chunk = {
                        'chunk_id': f'chart_{page_num + 1:03d}_{idx + 1:03d}',
                        'type': 'chart',
                        'content': chart_text,
                        'page_num': page_num + 1,
                        'metadata': {
                            'chart_type': chart.type,
                            'title': chart.title,
                            'description': chart.description,
                            'data_points': chart.data_points,
                            'confidence': chart.confidence
                        }
                    }
                    all_chunks.append(chunk)
                    chart_chunk_count += 1
                
                # âœ… Figureë¥¼ ì§ì ‘ chunkë¡œ ë³€í™˜ (NEW!)
                for idx, figure in enumerate(page_content.figures):
                    chunk = {
                        'chunk_id': f'figure_{page_num + 1:03d}_{idx + 1:03d}',
                        'type': 'figure',
                        'content': f"[ì´ë¯¸ì§€: {figure.type}]\n{figure.description}",
                        'page_num': page_num + 1,
                        'metadata': {
                            'figure_type': figure.type,
                            'description': figure.description,
                            'confidence': figure.confidence
                        }
                    }
                    all_chunks.append(chunk)
                    figure_chunk_count += 1
                
                # âœ… TextBlockì„ ì§ì ‘ chunkë¡œ ë³€í™˜ (sectionsê°€ ë¹„ì–´ìˆì„ ê²½ìš°ë¥¼ ìœ„í•´)
                if not page_content.sections and page_content.text_blocks:
                    for idx, text_block in enumerate(page_content.text_blocks):
                        chunk = {
                            'chunk_id': f'text_{page_num + 1:03d}_{idx + 1:03d}',
                            'type': 'text',
                            'content': text_block.text,
                            'page_num': page_num + 1,
                            'metadata': {
                                'confidence': text_block.confidence
                            }
                        }
                        all_chunks.append(chunk)
                        text_chunk_count += 1
                
                print(f"  âœ… Page {page_num + 1} extracted:")
                print(f"     - Sections: {len(page_content.sections)}")
                print(f"     - Tables: {len(page_content.tables)}")
                print(f"     - Charts: {len(page_content.charts)}")
                print(f"     - Figures: {len(page_content.figures)}")
                print(f"     - Text blocks: {len(page_content.text_blocks)}")
        
        doc.close()
        
        # í†µê³„
        print()
        print(f"âœ“ Created {text_chunk_count} text chunks")
        print(f"âœ“ Created {table_chunk_count} table chunks")
        print(f"âœ“ Created {chart_chunk_count} chart chunks")
        print(f"âœ“ Created {figure_chunk_count} figure chunks")
        
        # Step 2: (ìƒëµ - ì´ë¯¸ chunk ìƒì„± ì™„ë£Œ)
        print()
        print(f"ğŸ§© Step 2/3: Intelligent chunking...")
        print(f"âœ“ Created {len(all_chunks)} total chunks")
        
        # Step 3: ê²°ê³¼ ì €ì¥
        print()
        print(f"ğŸ’¾ Step 3/3: Saving results...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥
        pdf_name = Path(pdf_path).stem
        output_path = output_dir / f"{pdf_name}_chunks.json"
        
        result = {
            'chunks': all_chunks,
            'statistics': {
                'total_pages': pages_to_process,
                'total_chunks': len(all_chunks),
                'text_chunks': text_chunk_count,
                'table_chunks': table_chunk_count,
                'chart_chunks': chart_chunk_count,  # âœ… ì¶”ê°€
                'figure_chunks': figure_chunk_count,  # âœ… ì¶”ê°€
                'processing_time': time.time() - start_time
            }
        }
        
        import json
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ Saved: {output_path}")
        
        print()
        print("=" * 60)
        print("âœ… Processing complete!")
        print(f"Time: {time.time() - start_time:.1f}s")
        print(f"Output: {output_dir}")
        print("=" * 60)
        print()
        
        return {
            'chunks': all_chunks,
            'statistics': result['statistics'],
            'output_path': str(output_path)
        }


# CLI ì§€ì›
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python phase2_pipeline.py <pdf_path> [max_pages]")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else None
    
    pipeline = Phase2Pipeline()
    result = pipeline.process(pdf_path, max_pages=max_pages)
    
    print(f"\nâœ… Generated {len(result['chunks'])} chunks")
    print(f"   - Text: {result['statistics']['text_chunks']}")
    print(f"   - Tables: {result['statistics']['table_chunks']}")
    print(f"   - Charts: {result['statistics']['chart_chunks']}")
    print(f"   - Figures: {result['statistics']['figure_chunks']}")
    print(f"ğŸ“ Output: {result['output_path']}")