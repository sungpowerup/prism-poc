"""
core/phase28_pipeline.py
PRISM Phase 2.8 - ì™„ì „ ê°œì„ íŒ (ì´ˆê¸°í™” ì˜¤ë¥˜ ìˆ˜ì •)
- í•œê¸€ ì¸ì½”ë”© ìˆ˜ì •
- Element ì„¸ë¶€ ë¶„ë¥˜
- ì§€ëŠ¥í˜• ì²­í‚¹
- PDFProcessor ì´ˆê¸°í™” ì˜¤ë¥˜ ìˆ˜ì •
"""

import os
import json
import base64
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import logging

from .pdf_processor import PDFProcessor
from .element_classifier import ElementClassifier
from .vlm_service import VLMService
from .intelligent_chunker import IntelligentChunker

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/phase28.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class Phase28Pipeline:
    """
    Phase 2.8 ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    
    Stage 1: PDF â†’ í˜ì´ì§€ ì´ë¯¸ì§€ â†’ Element ë¶„ë¥˜ â†’ VLM ìº¡ì…˜
    Stage 2: ê¸´ ìº¡ì…˜ â†’ ì§€ëŠ¥í˜• ì²­í‚¹ (ë¬¸ì¥ ë‹¨ìœ„)
    """
    
    def __init__(self, vlm_provider: str = 'azure_openai'):
        """
        Args:
            vlm_provider: 'azure_openai', 'claude', 'local_sllm'
        """
        # 1. VLM ì„œë¹„ìŠ¤ ë¨¼ì € ìƒì„±
        self.vlm_service = VLMService(provider=vlm_provider)
        
        # 2. PDF í”„ë¡œì„¸ì„œ ìƒì„± (VLM ì„œë¹„ìŠ¤ëŠ” ì„ íƒì )
        try:
            self.pdf_processor = PDFProcessor(vlm_service=self.vlm_service)
        except TypeError:
            # vlm_serviceê°€ í•„ìˆ˜ê°€ ì•„ë‹Œ ê²½ìš° ì¸ì ì—†ì´ ìƒì„±
            self.pdf_processor = PDFProcessor()
        
        # 3. Element ë¶„ë¥˜ê¸° ìƒì„±
        self.element_classifier = ElementClassifier(use_vlm=False)
        
        # 4. ì§€ëŠ¥í˜• ì²­ì»¤ ìƒì„±
        self.chunker = IntelligentChunker(
            min_chunk_size=100,
            max_chunk_size=500,
            overlap=50
        )
        
        self.vlm_provider = vlm_provider
        
        logger.info(f"Phase28Pipeline ì´ˆê¸°í™” ì™„ë£Œ (VLM: {vlm_provider})")
    
    def process_pdf(
        self, 
        pdf_path: str, 
        output_dir: str = 'output',
        max_pages: int = None
    ) -> Dict[str, Any]:
        """
        PDF ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = datetime.now()
        logger.info(f"=== Phase 2.8 ì²˜ë¦¬ ì‹œì‘: {pdf_path} ===")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # Stage 1: PDF â†’ í˜ì´ì§€ â†’ VLM ìº¡ì…˜
        stage1_results = self._stage1_page_to_caption(pdf_path, max_pages)
        
        # Stage 2: ìº¡ì…˜ â†’ ì§€ëŠ¥í˜• ì²­í‚¹
        stage2_results = self._stage2_caption_to_chunks(stage1_results)
        
        # í†µê³„ ê³„ì‚°
        processing_time = (datetime.now() - start_time).total_seconds()
        stats = self._calculate_statistics(stage1_results, stage2_results, processing_time)
        
        # ê²°ê³¼ ì·¨í•©
        result = {
            'metadata': {
                'filename': Path(pdf_path).name,
                'total_pages': len(stage1_results),
                'total_chunks': len(stage2_results),
                'processing_time_sec': round(processing_time, 2),
                'vlm_provider': self.vlm_provider,
                'processed_at': datetime.now().isoformat(),
                'chunk_types': stats['chunk_types'],
                'phase': '2.8'
            },
            'stage1_elements': [self._summarize_element(e) for e in stage1_results],
            'stage2_chunks': stage2_results
        }
        
        # JSON ì €ì¥ (UTF-8 BOMìœ¼ë¡œ ì¸ì½”ë”© ë³´ì¥)
        json_path = self._save_json(result, output_dir, Path(pdf_path).stem)
        
        # Markdown ì €ì¥
        md_path = self._save_markdown(result, output_dir, Path(pdf_path).stem)
        
        logger.info(f"=== ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ ===")
        logger.info(f"JSON: {json_path}")
        logger.info(f"Markdown: {md_path}")
        
        return result
    
    def _stage1_page_to_caption(self, pdf_path: str, max_pages: int) -> List[Dict]:
        """
        Stage 1: PDF â†’ í˜ì´ì§€ ì´ë¯¸ì§€ â†’ Element ë¶„ë¥˜ â†’ VLM ìº¡ì…˜
        """
        logger.info("--- Stage 1: í˜ì´ì§€ â†’ VLM ìº¡ì…˜ ---")
        
        # PDF â†’ ì´ë¯¸ì§€
        pages = self.pdf_processor.pdf_to_images(pdf_path, max_pages=max_pages)
        logger.info(f"ì¶”ì¶œëœ í˜ì´ì§€: {len(pages)}ê°œ")
        
        results = []
        
        for page_num, image_data in enumerate(pages, start=1):
            logger.info(f"\n[Page {page_num}] ì²˜ë¦¬ ì¤‘...")
            
            # Element ë¶„ë¥˜ (ì„¸ë¶€ ë¶„ë¥˜ í¬í•¨)
            classification = self.element_classifier.classify(image_data)
            
            logger.info(f"  - Element íƒ€ì…: {classification['element_type']}")
            logger.info(f"  - ì„¸ë¶€ íƒ€ì…: {classification.get('subtypes', [])}")
            logger.info(f"  - ì‹ ë¢°ë„: {classification['confidence']:.2f}")
            
            # VLM ìº¡ì…˜ ìƒì„±
            caption_result = self.vlm_service.generate_caption(
                image_data=image_data,
                element_type=classification['element_type'],
                subtypes=classification.get('subtypes', [])  # ì„¸ë¶€ íƒ€ì… ì „ë‹¬
            )
            
            # UTF-8 ì¸ì½”ë”© ì¬í™•ì¸
            caption_text = self._ensure_utf8(caption_result['caption'])
            
            logger.info(f"  - ìº¡ì…˜ ê¸¸ì´: {len(caption_text)} ê¸€ì")
            logger.info(f"  - ì²˜ë¦¬ ì‹œê°„: {caption_result['processing_time_sec']:.2f}ì´ˆ")
            
            results.append({
                'page_number': page_num,
                'element_type': classification['element_type'],
                'subtypes': classification.get('subtypes', []),
                'confidence': classification['confidence'],
                'caption': caption_text,
                'tokens_used': caption_result.get('tokens_used', 0),
                'processing_time_sec': caption_result['processing_time_sec']
            })
        
        return results
    
    def _stage2_caption_to_chunks(self, stage1_results: List[Dict]) -> List[Dict]:
        """
        Stage 2: VLM ìº¡ì…˜ â†’ ì§€ëŠ¥í˜• ì²­í‚¹ (ë¬¸ì¥ ë‹¨ìœ„)
        """
        logger.info("\n--- Stage 2: ìº¡ì…˜ â†’ ì²­í‚¹ ---")
        
        all_chunks = []
        chunk_counter = 0
        
        for element in stage1_results:
            caption = element['caption']
            page_num = element['page_number']
            element_type = element['element_type']
            
            # ì§€ëŠ¥í˜• ì²­í‚¹ (ë¬¸ì¥ ë‹¨ìœ„)
            chunks = self.chunker.chunk_text(caption)
            
            logger.info(f"[Page {page_num}] {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            for i, chunk_text in enumerate(chunks):
                chunk_counter += 1
                
                all_chunks.append({
                    'chunk_id': f"chunk_{page_num}_{id(chunk_text)}",
                    'page_number': page_num,
                    'element_type': element_type,
                    'content': chunk_text,
                    'metadata': {
                        'section_path': f"Page {page_num}",
                        'source': 'vlm',
                        'chunk_index': i,
                        'start_pos': i * self.chunker.max_chunk_size,
                        'end_pos': (i + 1) * self.chunker.max_chunk_size,
                        'total_chunks': len(chunks)
                    },
                    'model_used': self.vlm_provider,
                    'processing_time_sec': element['processing_time_sec']
                })
        
        logger.info(f"ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return all_chunks
    
    def _ensure_utf8(self, text: str) -> str:
        """
        UTF-8 ì¸ì½”ë”© ë³´ì¥
        
        ë¬¸ì œ í•´ê²°:
        - Azure OpenAI API ì‘ë‹µì˜ ì˜ëª»ëœ ì¸ì½”ë”© ìˆ˜ì •
        - latin1 â†’ UTF-8 ì¬ì¸ì½”ë”©
        """
        try:
            # ì´ë¯¸ ì •ìƒì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if all(ord(c) < 128 or ord(c) > 127 for c in text):
                return text
            
            # latin1ë¡œ ì˜ëª» ë””ì½”ë”©ëœ ê²½ìš° ì¬ì¸ì½”ë”©
            return text.encode('latin1').decode('utf-8')
        except (UnicodeDecodeError, UnicodeEncodeError):
            # ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë°˜í™˜ (ì´ë¯¸ ì •ìƒ)
            return text
    
    def _calculate_statistics(
        self, 
        stage1_results: List[Dict],
        stage2_results: List[Dict],
        processing_time: float
    ) -> Dict:
        """í†µê³„ ê³„ì‚°"""
        chunk_types = {}
        for chunk in stage2_results:
            et = chunk['element_type']
            chunk_types[et] = chunk_types.get(et, 0) + 1
        
        return {
            'chunk_types': chunk_types,
            'total_processing_time': processing_time,
            'avg_time_per_page': processing_time / len(stage1_results) if stage1_results else 0
        }
    
    def _summarize_element(self, element: Dict) -> Dict:
        """Element ìš”ì•½"""
        return {
            'page_number': element['page_number'],
            'element_type': element['element_type'],
            'subtypes': element.get('subtypes', []),
            'confidence': element['confidence'],
            'chunks_count': 1,  # Stage 2ì—ì„œ ê³„ì‚°ë¨
            'tokens_used': element.get('tokens_used', 0),
            'processing_time_sec': round(element['processing_time_sec'], 3)
        }
    
    def _save_json(self, result: Dict, output_dir: str, filename: str) -> str:
        """
        JSON ì €ì¥ (UTF-8 BOM)
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_path = os.path.join(output_dir, f"prism_phase28_{timestamp}.json")
        
        with open(json_path, 'w', encoding='utf-8-sig') as f:  # UTF-8 BOM
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        return json_path
    
    def _save_markdown(self, result: Dict, output_dir: str, filename: str) -> str:
        """
        Markdown ì €ì¥
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        md_path = os.path.join(output_dir, f"prism_phase28_{timestamp}.md")
        
        lines = [
            "# PRISM Phase 2.8 - ë¬¸ì„œ ì¶”ì¶œ ê²°ê³¼\n",
            f"**ìƒì„±ì¼ì‹œ**: {result['metadata']['processed_at']}\n",
            "\n---\n",
            "\n## ğŸ“„ ë¬¸ì„œ ì •ë³´\n",
            f"- **íŒŒì¼ëª…**: {result['metadata']['filename']}",
            f"- **ì´ í˜ì´ì§€**: {result['metadata']['total_pages']}",
            f"- **ì²˜ë¦¬ ì‹œê°„**: {result['metadata']['processing_time_sec']}ì´ˆ",
            f"- **ì´ ì²­í¬**: {result['metadata']['total_chunks']}",
            f"- **Phase**: {result['metadata']['phase']}\n",
            "\n---\n",
            "\n## ğŸ§© ì§€ëŠ¥í˜• ì²­í¬\n"
        ]
        
        for i, chunk in enumerate(result['stage2_chunks'], start=1):
            lines.append(f"\n### ì²­í¬ #{i}\n")
            lines.append(f"**í˜ì´ì§€**: {chunk['page_number']}")
            lines.append(f"**íƒ€ì…**: {chunk['element_type']}")
            lines.append(f"**ëª¨ë¸**: {chunk['model_used']}\n")
            lines.append("**ë‚´ìš©**:\n")
            lines.append("```")
            lines.append(chunk['content'])
            lines.append("```\n")
            lines.append("---\n")
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        return md_path


# CLI ì‹¤í–‰
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python -m core.phase28_pipeline <pdf_path> [output_dir] [max_pages]")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else 'output'
    max_pages = int(sys.argv[3]) if len(sys.argv) > 3 else None
    
    pipeline = Phase28Pipeline(vlm_provider='azure_openai')
    result = pipeline.process_pdf(pdf_path, output_dir, max_pages)
    
    print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ì´ ì²­í¬: {result['metadata']['total_chunks']}ê°œ")
    print(f"ì²˜ë¦¬ ì‹œê°„: {result['metadata']['processing_time_sec']}ì´ˆ")