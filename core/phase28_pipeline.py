"""
PRISM Phase 2.8 Pipeline - VLM í†µí•© ì™„ì„±
Element ë¶„ë¥˜ + VLM ë³€í™˜ + Intelligent Chunking

Author: ì´ì„œì˜ (Backend Lead) + ë°•ì¤€í˜¸ (AI/ML Lead)
Date: 2025-10-21
Version: 2.8
"""

import os
import json
import time
import fitz  # PyMuPDF
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
from PIL import Image
import io
import logging

# Core ëª¨ë“ˆ
from core.element_classifier import ElementClassifier
from core.vlm_service import VLMService
from core.intelligent_chunker import IntelligentChunker

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Phase28Pipeline:
    """
    PRISM Phase 2.8 íŒŒì´í”„ë¼ì¸
    
    Features:
    âœ… Element ìë™ ë¶„ë¥˜ (Chart/Table/Diagram/Text/Image)
    âœ… VLM ê¸°ë°˜ ìì—°ì–´ ë³€í™˜ (ê²½ìŸì‚¬ ìˆ˜ì¤€)
    âœ… Intelligent Chunking (ì˜ë¯¸ ê¸°ë°˜)
    âœ… í’ˆì§ˆ ìë™ í‰ê°€
    """
    
    def __init__(self, vlm_provider: str = "claude"):
        """
        Args:
            vlm_provider: VLM ì œê³µì ('claude', 'azure_openai', 'ollama')
        """
        self.vlm_provider = vlm_provider
        
        print("\n" + "="*60)
        print("ğŸ”· PRISM Phase 2.8 Pipeline Initialization")
        print("="*60)
        
        # ì„œë¸Œ ëª¨ë“ˆ ì´ˆê¸°í™”
        print("ğŸ“Š Element Classifier ì´ˆê¸°í™”...")
        self.classifier = ElementClassifier(use_vlm=True, vlm_threshold=0.7)
        
        print("ğŸ¤– VLM Service ì´ˆê¸°í™”...")
        self.vlm_service = VLMService()
        
        print("ğŸ§© Intelligent Chunker ì´ˆê¸°í™”...")
        self.chunker = IntelligentChunker(
            min_chunk_size=100,
            max_chunk_size=500,
            overlap=50
        )
        
        print("="*60 + "\n")
    
    def process_pdf(
        self,
        pdf_path: str,
        output_dir: Optional[str] = None,
        max_pages: Optional[int] = None
    ) -> Dict:
        """
        PDF ë¬¸ì„œ ì²˜ë¦¬ (VLM í†µí•©)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ (None=ì „ì²´)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        start_time = time.time()
        
        print("\n" + "="*60)
        print(f"ğŸ”· PRISM Phase 2.8 - PDF Processing")
        print("="*60)
        print(f"ğŸ“„ Input: {pdf_path}")
        print(f"ğŸ¤– VLM Provider: {self.vlm_provider}")
        print(f"ğŸ¯ Max Pages: {max_pages or 'All'}")
        print("="*60 + "\n")
        
        # Step 1: PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
        print("ğŸ“„ Step 1: Converting PDF to images...")
        page_images = self._pdf_to_images(pdf_path, max_pages)
        total_pages = len(page_images)
        print(f"   âœ“ Converted {total_pages} page(s)\n")
        
        # Step 2: í˜ì´ì§€ë³„ ì²˜ë¦¬
        all_chunks = []
        stage1_stats = []
        
        for page_num, page_image in enumerate(page_images, start=1):
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“„ Processing Page {page_num}/{total_pages}")
            print(f"{'â”€'*60}")
            
            # í˜ì´ì§€ ì²˜ë¦¬
            page_chunks, stats = self._process_page_vlm(page_image, page_num)
            
            all_chunks.extend(page_chunks)
            stage1_stats.append(stats)
            
            print(f"âœ“ Page {page_num} completed: {len(page_chunks)} chunk(s)")
        
        # Step 3: ê²°ê³¼ ìƒì„±
        elapsed_time = time.time() - start_time
        
        result = {
            'metadata': {
                'filename': Path(pdf_path).name,
                'total_pages': total_pages,
                'total_chunks': len(all_chunks),
                'processing_time_sec': elapsed_time,
                'vlm_provider': self.vlm_provider,
                'processed_at': datetime.now().isoformat(),
                'chunk_types': self._count_chunk_types(all_chunks),
                'phase': '2.8'
            },
            'stage1_elements': stage1_stats,
            'stage2_chunks': all_chunks
        }
        
        # Step 4: íŒŒì¼ ì €ì¥
        if output_dir:
            self._save_results(result, output_dir)
        
        # ì™„ë£Œ ë©”ì‹œì§€
        print("\n" + "="*60)
        print("âœ… Processing Complete!")
        print("="*60)
        print(f"â±ï¸  Total time: {elapsed_time:.1f}s")
        print(f"ğŸ“Š Total chunks: {len(all_chunks)}")
        print(f"ğŸ“ˆ Chunk types: {result['metadata']['chunk_types']}")
        print("="*60)
        
        return result
    
    def _pdf_to_images(
        self,
        pdf_path: str,
        max_pages: Optional[int] = None
    ) -> List[Image.Image]:
        """PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ (PyMuPDF)"""
        
        doc = fitz.open(pdf_path)
        images = []
        
        total_pages = len(doc) if max_pages is None else min(max_pages, len(doc))
        
        for page_num in range(total_pages):
            page = doc[page_num]
            
            # ê³ í•´ìƒë„ ë Œë”ë§ (DPI 300)
            mat = fitz.Matrix(300/72, 300/72)
            pix = page.get_pixmap(matrix=mat)
            
            # PIL Imageë¡œ ë³€í™˜
            img_data = pix.tobytes("png")
            image = Image.open(io.BytesIO(img_data))
            
            images.append(image)
        
        doc.close()
        
        return images
    
    def _process_page_vlm(
        self,
        page_image: Image.Image,
        page_num: int
    ) -> tuple:
        """
        í˜ì´ì§€ ì²˜ë¦¬ (VLM í†µí•©)
        
        Args:
            page_image: PIL Image
            page_num: í˜ì´ì§€ ë²ˆí˜¸
            
        Returns:
            (chunks, stats)
        """
        
        print("ğŸ” Step 1: Element Classification...")
        
        # Element ë¶„ë¥˜
        classification = self.classifier.classify(page_image)
        
        element_type = classification.element_type
        confidence = classification.confidence
        
        print(f"   âœ“ Type: {element_type} (confidence: {confidence:.2f})")
        
        # VLM ë³€í™˜
        print("ğŸ¤– Step 2: VLM Transformation...")
        
        try:
            # ì´ë¯¸ì§€ â†’ bytes
            img_buffer = io.BytesIO()
            page_image.save(img_buffer, format='PNG')
            image_bytes = img_buffer.getvalue()
            
            # VLM í˜¸ì¶œ
            vlm_result = self.vlm_service.generate_caption(
                image_data=image_bytes,
                element_type=element_type
            )
            
            content = vlm_result['caption']
            tokens_used = vlm_result.get('tokens_used', 0)
            processing_time = vlm_result.get('processing_time_ms', 0) / 1000
            
            print(f"   âœ“ Generated: {len(content)} chars")
            print(f"   âœ“ Tokens: {tokens_used}, Time: {processing_time:.2f}s")
        
        except Exception as e:
            logger.error(f"âŒ VLM ë³€í™˜ ì‹¤íŒ¨: {e}")
            content = f"[Element ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}]"
            tokens_used = 0
            processing_time = 0
        
        # ì²­í‚¹
        print("ğŸ§© Step 3: Intelligent Chunking...")
        
        chunks = self.chunker.chunk_text(
            text=content,
            metadata={
                'page_number': page_num,
                'element_type': element_type,
                'confidence': confidence,
                'source': 'vlm'
            }
        )
        
        print(f"   âœ“ Created {len(chunks)} chunk(s)")
        
        # í†µê³„
        stats = {
            'page_number': page_num,
            'element_type': element_type,
            'confidence': confidence,
            'chunks_count': len(chunks),
            'tokens_used': tokens_used,
            'processing_time_sec': processing_time
        }
        
        return chunks, stats
    
    def _count_chunk_types(self, chunks: List[Dict]) -> Dict[str, int]:
        """ì²­í¬ íƒ€ì…ë³„ ê°œìˆ˜"""
        
        type_counts = {}
        for chunk in chunks:
            chunk_type = chunk.get('element_type', 'unknown')
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        return type_counts
    
    def _save_results(self, result: Dict, output_dir: str):
        """ê²°ê³¼ ì €ì¥"""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ì €ì¥
        json_path = output_path / f"result_phase28_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # Markdown ì €ì¥
        md_path = output_path / f"result_phase28_{timestamp}.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_markdown(result))
        
        print(f"\nğŸ’¾ Results saved:")
        print(f"   - JSON: {json_path}")
        print(f"   - MD: {md_path}")
    
    def _generate_markdown(self, result: Dict) -> str:
        """Markdown ìƒì„±"""
        
        md = []
        
        # í—¤ë”
        md.append("# PRISM Phase 2.8 - ë¬¸ì„œ ì¶”ì¶œ ê²°ê³¼\n")
        md.append(f"**ìƒì„±ì¼ì‹œ**: {result['metadata']['processed_at'][:19].replace('T', ' ')}\n")
        md.append("---\n\n")
        
        # ë©”íƒ€ë°ì´í„°
        meta = result['metadata']
        md.append("## ğŸ“„ ë¬¸ì„œ ì •ë³´\n\n")
        md.append(f"- **íŒŒì¼ëª…**: {meta['filename']}\n")
        md.append(f"- **ì´ í˜ì´ì§€**: {meta['total_pages']}\n")
        md.append(f"- **ì²˜ë¦¬ ì‹œê°„**: {meta['processing_time_sec']:.2f}ì´ˆ\n")
        md.append(f"- **ì´ ì²­í¬**: {meta['total_chunks']}\n")
        md.append(f"- **VLM í”„ë¡œë°”ì´ë”**: {meta['vlm_provider']}\n")
        md.append(f"- **Phase**: {meta['phase']}\n")
        md.append("\n---\n\n")
        
        # Stage 1 í†µê³„
        md.append("## ğŸ“Š Stage 1: Element ë¶„ë¥˜ í†µê³„\n\n")
        
        for stat in result.get('stage1_elements', []):
            md.append(f"### í˜ì´ì§€ {stat['page_number']}\n")
            md.append(f"- **íƒ€ì…**: {stat['element_type']}\n")
            md.append(f"- **ì‹ ë¢°ë„**: {stat['confidence']:.2f}\n")
            md.append(f"- **ì²­í¬ ìˆ˜**: {stat['chunks_count']}\n")
            md.append(f"- **í† í° ì‚¬ìš©**: {stat['tokens_used']}\n")
            md.append(f"- **ì²˜ë¦¬ ì‹œê°„**: {stat['processing_time_sec']:.2f}s\n\n")
        
        md.append("---\n\n")
        
        # Stage 2 ì²­í¬
        md.append("## ğŸ§© Stage 2: ì§€ëŠ¥í˜• ì²­í¬\n\n")
        
        for i, chunk in enumerate(result.get('stage2_chunks', []), 1):
            md.append(f"### ì²­í¬ #{i}\n\n")
            md.append(f"**í˜ì´ì§€**: {chunk['page_number']}\n")
            md.append(f"**íƒ€ì…**: {chunk['element_type']}\n")
            md.append(f"**ëª¨ë¸**: {chunk['model_used']}\n\n")
            md.append("**ë‚´ìš©**:\n\n")
            md.append("```\n")
            md.append(chunk['content'])
            md.append("\n```\n\n")
            md.append("---\n\n")
        
        return ''.join(md)


# ========== CLI ì‹¤í–‰ ==========

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python phase28_pipeline.py <pdf_path> [output_dir] [max_pages]")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else 'output'
    max_pages = int(sys.argv[3]) if len(sys.argv) > 3 else None
    
    pipeline = Phase28Pipeline()
    result = pipeline.process_pdf(pdf_path, output_dir, max_pages)
    
    print("\nâœ… Done!")