"""
core/post_merge_normalizer.py
PRISM Phase 0 Hotfix - Post Merge Normalizer with Page Marker Removal

âœ… Phase 0 ê¸´ê¸‰ ìˆ˜ì •:
1. íŽ˜ì´ì§€ ë§ˆì»¤ ì œê±° íŒ¨í„´ í™•ìž¥
2. ë°˜ë³µ ì œëª© ì œê±° ("ì¸ì‚¬ê·œì •")
3. ë¶„í• ëœ ë‹¨ì–´ ì²˜ë¦¬ ("402-3 ìš©ì„")

Author: ì´ì„œì˜ (Backend Lead)
Date: 2025-11-06
Version: Phase 0 Hotfix
"""

import re
import logging
from typing import Dict, Any
from collections import OrderedDict

logger = logging.getLogger(__name__)


class PostMergeNormalizer:
    """
    Phase 0 í›„ì²˜ë¦¬ ì •ê·œí™” (íŽ˜ì´ì§€ ë§ˆì»¤ ì œê±° ê°•í™”)
    
    âœ… Phase 0 ê°œì„ :
    - íŽ˜ì´ì§€ ë§ˆì»¤ íŒ¨í„´ 5ì¢… í™•ìž¥
    - ë°˜ë³µ ì œëª© ì œê±°
    - ì•ˆì „ ê°€ë“œ (ë‹¨ë… ë¼ì¸ë§Œ)
    """
    
    # ê³ ë¹ˆë„ ìš©ì–´ ì‚¬ì „
    HIGH_FREQ_TERMS = OrderedDict([
        ('ì„±ê³¼ê³„ìž¬ë‹¨ìƒìž', 'ì„±ê³¼ê°œì„ ëŒ€ìƒìž'),
        ('ê³µê¸ˆê´€ë¦¬ìœ„ì›íšŒ', 'ìƒê¸‰ì¸ì‚¬ìœ„ì›íšŒ'),
        ('ì§ì›ì— ê²Œ', 'ì§ì›ì—ê²Œ'),
        ('ë¶€ì—¬í•  ìˆ˜ìžˆëŠ”', 'ë¶€ì—¬í•  ìˆ˜ ìžˆëŠ”'),
        ('ê°€ ì§„ë‹¤', 'ê°€ì§„ë‹¤'),
        ('ì— ê²Œ', 'ì—ê²Œ'),
        ('ì—ì„œ', 'ì—ì„œ'),
    ])
    
    # âœ… Phase 0: íŽ˜ì´ì§€ ë§ˆì»¤ íŒ¨í„´ (í™•ìž¥)
    PAGE_MARKER_PATTERNS = [
        r'^\s*\d{3,4}-\d{1,2}\s*$',           # "402-3"
        r'^\s*Page\s+\d+\s*$',                # "Page 1"
        r'^\s*[-â€”â€“_*]{3,}\s*$',              # "---", "___"
        r'^\s*ì¸ì‚¬ê·œì •\s*$',                  # "ì¸ì‚¬ê·œì •" (ë°˜ë³µ ì œëª©)
        r'^\s*\d{3,4}-\d{1,2}\s*[ê°€-íž£]{1,2}\s*$',  # "402-3 ìš©ì„" (ë¶„í•  ë‹¨ì–´)
    ]
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… PostMergeNormalizer Phase 0 ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ðŸ“– ê³ ë¹ˆë„ ì‚¬ì „: {len(self.HIGH_FREQ_TERMS)}ê°œ")
        logger.info(f"   ðŸ” íŽ˜ì´ì§€ ë§ˆì»¤ íŒ¨í„´: {len(self.PAGE_MARKER_PATTERNS)}ê°œ")
    
    def normalize(self, content: str, doc_type: str = 'general') -> str:
        """
        âœ… Phase 0: í›„ì²˜ë¦¬ ì •ê·œí™” (íŽ˜ì´ì§€ ë§ˆì»¤ ì œê±°)
        
        Args:
            content: Markdown í…ìŠ¤íŠ¸
            doc_type: ë¬¸ì„œ íƒ€ìž…
        
        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        logger.info(f"   ðŸ”§ PostMergeNormalizer Phase 0 ì‹œìž‘ (doc_type: {doc_type})")
        
        original_len = len(content)
        
        # 1) âœ… Phase 0: íŽ˜ì´ì§€ ë§ˆì»¤ ì œê±° (ë¼ì¸ë³„)
        lines = content.split('\n')
        cleaned_lines = []
        removed_count = 0
        
        for line in lines:
            is_marker = False
            
            for pattern in self.PAGE_MARKER_PATTERNS:
                if re.match(pattern, line):
                    is_marker = True
                    removed_count += 1
                    logger.debug(f"      íŽ˜ì´ì§€ ë§ˆì»¤ ì œê±°: '{line.strip()}'")
                    break
            
            if not is_marker:
                cleaned_lines.append(line)
        
        content = '\n'.join(cleaned_lines)
        
        logger.info(f"   ðŸ—‘ï¸ íŽ˜ì´ì§€ ë§ˆì»¤ ì œê±°: {removed_count}ê°œ ë¼ì¸")
        
        # 2) ê³ ë¹ˆë„ ìš©ì–´ ì‚¬ì „ (statute ëª¨ë“œë§Œ)
        if doc_type == 'statute':
            for wrong, correct in self.HIGH_FREQ_TERMS.items():
                if wrong in content:
                    count = content.count(wrong)
                    content = content.replace(wrong, correct)
                    logger.debug(f"      ìš©ì–´ êµì •: '{wrong}' â†’ '{correct}' ({count}íšŒ)")
        
        # 3) ì¤„ë°”ê¿ˆ ì •ê·œí™”
        content = self._normalize_newlines(content)
        
        # 4) ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        content = self._normalize_lists(content)
        
        # 5) ê³¼ë„í•œ ê³µë°± ì œê±°
        content = re.sub(r' {2,}', ' ', content)
        
        normalized_len = len(content)
        
        logger.info(f"   âœ… ì •ê·œí™” ì™„ë£Œ: {original_len} â†’ {normalized_len} ê¸€ìž")
        
        return content
    
    def _normalize_newlines(self, content: str) -> str:
        """
        ì¤„ë°”ê¿ˆ ì •ê·œí™”
        
        Args:
            content: ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # ì¡°ë¬¸ í—¤ë” ì•žë’¤ ì •ë¦¬
        content = re.sub(r'\n+(#{1,3}\s*ì œ\s*\d+\s*ì¡°)', r'\n\n\1', content)
        content = re.sub(r'(#{1,3}\s*ì œ\s*\d+\s*ì¡°[^\n]*)\n+', r'\1\n', content)
        
        # ì œnìž¥ í—¤ë” ì•žë’¤ ì •ë¦¬
        content = re.sub(r'\n+(#{0,3}\s*ì œ\s*\d+\s*ìž¥)', r'\n\n\1', content)
        content = re.sub(r'(#{0,3}\s*ì œ\s*\d+\s*ìž¥[^\n]*)\n+', r'\1\n', content)
        
        return content
    
    def _normalize_lists(self, content: str) -> str:
        """
        ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        
        Args:
            content: ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (1. 2. 3.)
        content = re.sub(r'(\d+)\s*\.\s*', r'\1. ', content)
        
        # í˜¸ ë¦¬ìŠ¤íŠ¸ (ê°€. ë‚˜. ë‹¤.)
        content = re.sub(r'([ê°€-íž£])\s*\.\s*', r'\1. ', content)
        
        return content
    
    def get_stats(self, original: str, normalized: str) -> Dict[str, Any]:
        """ì •ê·œí™” í†µê³„"""
        corrections = 0
        
        for wrong in self.HIGH_FREQ_TERMS.keys():
            corrections += original.count(wrong)
        
        return {
            'original_length': len(original),
            'normalized_length': len(normalized),
            'corrections': corrections,
            'rules_count': len(self.HIGH_FREQ_TERMS)
        }