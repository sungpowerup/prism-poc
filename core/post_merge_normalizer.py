"""
core/post_merge_normalizer.py
PRISM Phase 5.7.8.1 - PostMergeNormalizer (ê¸´ê¸‰ íŒ¨ì¹˜ - ìˆœì„œ ëª…ì‹œ)

âœ… Phase 5.7.8.1 ê¸´ê¸‰ ìˆ˜ì •:
1. OrderedDictë¡œ ì‚¬ì „ ìˆœì„œ ëª…ì‹œ
2. Longest-First ì •ì±… ì ìš©
3. í—¤ë” ë¼ì¸ ë³´í˜¸ (### ì œnì¡°ëŠ” ìŠ¤í‚µ)
4. ì ìš© ìˆœì„œ ë¡œê¹…

ğŸ¯ í•´ê²° ë¬¸ì œ:
- ë³µí•© ë„ì–´ì“°ê¸° íŒ¨í„´ ë¯¸ì ìš©
- í—¤ë” ë¼ì¸ ì˜¤ìˆ˜ì • ë°©ì§€
- Dict ìˆœì„œ ë¶ˆì•ˆì •

Author: ì´ì„œì˜ (Backend Lead) + GPT ì œì•ˆ ë°˜ì˜
Date: 2025-11-05
Version: 5.7.8.1 Hotfix
"""

import re
import logging
from typing import Dict, Any
from collections import OrderedDict

logger = logging.getLogger(__name__)


class PostMergeNormalizer:
    """
    Phase 5.7.8.1 í›„ì²˜ë¦¬ ì •ê·œí™” (ìˆœì„œ ìµœì í™”)
    
    í•µì‹¬ ê°œì„ :
    - OrderedDictë¡œ ìˆœì„œ ë³´ì¥
    - Longest-First ì •ì±…
    - í—¤ë” ë¼ì¸ ë³´í˜¸
    
    ì—­í• :
    - Fallback í›„ í…ìŠ¤íŠ¸ ì •ë¦¬
    - ë„ì–´ì“°ê¸° ë³µì›
    - ì¤„ë°”ê¿ˆ ì •ê·œí™”
    """
    
    # âœ… Phase 5.7.8.1: OrderedDictë¡œ ìˆœì„œ ëª…ì‹œ (Longest-First)
    HIGH_FREQ_TERMS = OrderedDict([
        # ========================================
        # ğŸ”¥ ë³µí•© íŒ¨í„´ (ê¸´ ê²ƒë¶€í„°) - ìµœìš°ì„  ì ìš©
        # ========================================
        
        # Phase 5.7.8: ê³ ë¹ˆë„ ë„ì–´ì“°ê¸° (ë¯¸ì†¡ ì œì•ˆ)
        ('1ëª…ì˜ì§ì›ì—ê²Œë¶€ì—¬í• ìˆ˜ìˆëŠ”', '1ëª…ì˜ ì§ì›ì—ê²Œ ë¶€ì—¬í•  ìˆ˜ ìˆëŠ”'),
        ('ì§ì›ì—ê²Œë¶€ì—¬í• ìˆ˜ìˆëŠ”', 'ì§ì›ì—ê²Œ ë¶€ì—¬í•  ìˆ˜ ìˆëŠ”'),
        ('1ëª…ì˜ ì§ì›ì—ê²Œë¶€ì—¬í• ìˆ˜ìˆëŠ”', '1ëª…ì˜ ì§ì›ì—ê²Œ ë¶€ì—¬í•  ìˆ˜ ìˆëŠ”'),
        ('ì§ì›ì— ê²Œë¶€ì—¬í• ìˆ˜ìˆëŠ”', 'ì§ì›ì—ê²Œ ë¶€ì—¬í•  ìˆ˜ ìˆëŠ”'),
        ('ë¶€ì—¬í• ìˆ˜ìˆëŠ”', 'ë¶€ì—¬í•  ìˆ˜ ìˆëŠ”'),
        ('1ëª…ì˜ì§ì›ì—ê²Œ', '1ëª…ì˜ ì§ì›ì—ê²Œ'),
        ('ì§ì›ì—ê²Œ', 'ì§ì›ì—ê²Œ'),  # ì •ìƒ (ë³´ì¡´)
        
        # Phase 5.7.7.2: ë³µí•© íŒ¨í„´
        ('ì§ë¬´ì˜ì¢…ë¥˜', 'ì§ë¬´ì˜ ì¢…ë¥˜'),
        ('ê·¸ë°–ì—', 'ê·¸ ë°–ì—'),
        
        # ========================================
        # ğŸ“Œ ì¤‘ê°„ íŒ¨í„´
        # ========================================
        
        # Phase 5.7.7.1: ì¡°ë¬¸ í‘œí˜„
        ('ì œ1ì¡°', 'ì œ1ì¡°'),  # ì •ìƒ (ë³´ì¡´)
        ('ì œ 1ì¡°', 'ì œ1ì¡°'),  # ê³µë°± ì œê±°
        ('ì œ  1ì¡°', 'ì œ1ì¡°'),  # ê³µë°± 2ê°œ ì œê±°
        
        # Phase 5.7.6: ë‹¨ì–´ ê²½ê³„
        ('ê°€ì§„ë‹¤', 'ê°€ì§„ë‹¤'),  # ì •ìƒ (ë³´ì¡´)
        ('ê°€ ì§„ë‹¤', 'ê°€ì§„ë‹¤'),
        
        # ========================================
        # ğŸ”» ë‹¨ìˆœ íŒ¨í„´ (ì§§ì€ ê²ƒ) - ë§¨ ë§ˆì§€ë§‰ ì ìš©
        # ========================================
        
        ('í• ìˆ˜ìˆëŠ”', 'í•  ìˆ˜ ìˆëŠ”'),
        ('í• ìˆ˜ì—†ëŠ”', 'í•  ìˆ˜ ì—†ëŠ”'),
        ('ìˆ˜ìˆëŠ”', 'ìˆ˜ ìˆëŠ”'),
        ('ìˆ˜ì—†ëŠ”', 'ìˆ˜ ì—†ëŠ”'),
        ('ì—ê²Œ', 'ì—ê²Œ'),  # ì •ìƒ (ë³´ì¡´)
        ('ì— ê²Œ', 'ì—ê²Œ'),
        ('ì—ì„œ', 'ì—ì„œ'),  # ì •ìƒ (ë³´ì¡´)
        ('ì— ì„œ', 'ì—ì„œ'),
    ])
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… PostMergeNormalizer v5.7.8.1 ì´ˆê¸°í™” ì™„ë£Œ (ìˆœì„œ ìµœì í™”)")
        logger.info(f"   ğŸ“– ê³ ë¹ˆë„ ì‚¬ì „: {len(self.HIGH_FREQ_TERMS)}ê°œ (OrderedDict)")
        logger.info("   ğŸ¯ ì ìš© ì •ì±…: Longest-First (ê¸´ íŒ¨í„´ ìš°ì„ )")
        logger.info("   ğŸ›¡ï¸ í—¤ë” ë³´í˜¸: ### ì œnì¡° ë¼ì¸ ìŠ¤í‚µ")
    
    def normalize(self, content: str, doc_type: str = 'general') -> str:
        """
        âœ… Phase 5.7.8.1: í›„ì²˜ë¦¬ ì •ê·œí™” (í—¤ë” ë³´í˜¸)
        
        Args:
            content: Markdown í…ìŠ¤íŠ¸
            doc_type: ë¬¸ì„œ íƒ€ì…
        
        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        logger.info(f"   ğŸ”§ PostMergeNormalizer v5.7.8.1 ì‹œì‘ (doc_type: {doc_type})")
        
        original_len = len(content)
        
        # âœ… Phase 5.7.8.1: í—¤ë” ë¼ì¸ ë³´í˜¸
        lines = content.split('\n')
        protected_lines = []
        
        for line in lines:
            # í—¤ë” ë¼ì¸ ê°ì§€ (### ì œnì¡°)
            if re.match(r'^\s*#{1,3}\s*ì œ\s*\d+\s*ì¡°', line):
                # í—¤ë”ëŠ” ê·¸ëŒ€ë¡œ ë³´ì¡´
                protected_lines.append(line)
                logger.debug(f"      í—¤ë” ë³´í˜¸: {line[:50]}")
            else:
                # ì¼ë°˜ ë¼ì¸ë§Œ ì •ê·œí™”
                normalized_line = self._normalize_line(line, doc_type)
                protected_lines.append(normalized_line)
        
        content = '\n'.join(protected_lines)
        
        # ì¤„ë°”ê¿ˆ ì •ê·œí™”
        content = self._normalize_newlines(content)
        
        # ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        content = self._normalize_lists(content)
        
        normalized_len = len(content)
        
        logger.info(f"   âœ… ì •ê·œí™” ì™„ë£Œ: {original_len} â†’ {normalized_len} ê¸€ì")
        
        return content
    
    def _normalize_line(self, line: str, doc_type: str) -> str:
        """
        ê°œë³„ ë¼ì¸ ì •ê·œí™”
        
        Args:
            line: ì›ë³¸ ë¼ì¸
            doc_type: ë¬¸ì„œ íƒ€ì…
        
        Returns:
            ì •ê·œí™”ëœ ë¼ì¸
        """
        original_line = line
        
        # ê³ ë¹ˆë„ ìš©ì–´ ì‚¬ì „ ì ìš© (ê·œì • ëª¨ë“œë§Œ)
        if doc_type == 'statute':
            for wrong, correct in self.HIGH_FREQ_TERMS.items():
                if wrong in line:
                    line = line.replace(wrong, correct)
        
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        line = re.sub(r' {2,}', ' ', line)
        
        return line
    
    def _normalize_newlines(self, content: str) -> str:
        """
        ì¤„ë°”ê¿ˆ ì •ê·œí™”
        
        Args:
            content: ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # ì¡°ë¬¸ í—¤ë” ì•ë’¤ ì •ë¦¬
        content = re.sub(r'\n+(#{1,3}\s*ì œ\s*\d+\s*ì¡°)', r'\n\n\1', content)
        content = re.sub(r'(#{1,3}\s*ì œ\s*\d+\s*ì¡°[^\n]*)\n+', r'\1\n', content)
        
        return content
    
    def _normalize_lists(self, content: str) -> str:
        """
        ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        
        Args:
            content: ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ì •ê·œí™” (1. 2. 3.)
        content = re.sub(r'(\d+)\s*\.\s*', r'\1. ', content)
        
        # í˜¸ ë¦¬ìŠ¤íŠ¸ ì •ê·œí™” (ê°€. ë‚˜. ë‹¤.)
        content = re.sub(r'([ê°€-í£])\s*\.\s*', r'\1. ', content)
        
        return content
    
    def get_stats(self, original: str, normalized: str) -> Dict[str, Any]:
        """
        ì •ê·œí™” í†µê³„
        
        Args:
            original: ì›ë³¸ í…ìŠ¤íŠ¸
            normalized: ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        
        Returns:
            í†µê³„ ì •ë³´
        """
        corrections = 0
        
        # ê³ ë¹ˆë„ ìš©ì–´ êµì • ê°œìˆ˜
        for wrong in self.HIGH_FREQ_TERMS.keys():
            corrections += original.count(wrong)
        
        return {
            'original_length': len(original),
            'normalized_length': len(normalized),
            'corrections': corrections,
            'rules_count': len(self.HIGH_FREQ_TERMS)
        }