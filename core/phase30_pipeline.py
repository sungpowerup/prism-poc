"""
core/phase30_pipeline.py
PRISM Phase 3.0 - Main Pipeline

âœ… ìµœì¢… ì™„ë²½ ìˆ˜ì •:
1. VLM analyze_image() íŒŒë¼ë¯¸í„°: image_data (not image_base64)
2. SectionChunker ë©”ì†Œë“œ: chunk_extractions() (not chunk)
3. region_id ìë™ ìƒì„±
4. all_extractions ë°ì´í„° êµ¬ì¡° ìˆ˜ì • (region_type ì¶”ê°€)

ì‹¤í–‰: Phase30Pipeline(vlm_provider).process_document(pdf_path)
"""

import os
import json
import time
import base64
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import logging
import io
import numpy as np
from PIL import Image

from core.pdf_processor import PDFProcessor
from core.layout_detector import LayoutDetector
from core.vlm_service import VLMService
from core.section_chunker import SectionChunker
from prompts.chart_prompt import ChartPrompts

logger = logging.getLogger(__name__)


class Phase30Pipeline:
    """
    PRISM Phase 3.0 íŒŒì´í”„ë¼ì¸
    - Layout Detection (CV ê¸°ë°˜)
    - Region-based VLM Analysis
    - Section-aware Chunking
    """
    
    def __init__(self, vlm_provider: str = "azure_openai"):
        """
        Phase 3.0 íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            vlm_provider: VLM í”„ë¡œë°”ì´ë” ('azure_openai', 'claude', 'ollama')
        """
        logger.info("="*60)
        logger.info("PRISM Phase 3.0 Pipeline ì´ˆê¸°í™”")
        logger.info("="*60)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.pdf_processor = PDFProcessor()
        logger.info("âœ… PDF Processor")
        
        self.vlm_service = VLMService(provider=vlm_provider)
        logger.info(f"âœ… VLM Service ({vlm_provider})")
        
        self.layout_detector = LayoutDetector()
        logger.info("âœ… Layout Detector")
        
        self.chunker = SectionChunker()
        logger.info("âœ… Section-aware Chunker")
        
        logger.info("="*60 + "\n")
    
    def process_document(
        self,
        pdf_path: str,
        max_pages: int = None
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ì²˜ë¦¬ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        
        filename = Path(pdf_path).name
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“„ PRISM Phase 3.0 ë¬¸ì„œ ì²˜ë¦¬")
        logger.info(f"   íŒŒì¼: {filename}")
        logger.info(f"{'='*60}")
        
        # Stage 1: PDF â†’ Base64 Images
        logger.info(f"\n--- Stage 1: PDF â†’ Base64 Images ---")
        base64_images = self.pdf_processor.pdf_to_images(pdf_path, max_pages=max_pages)
        
        logger.info(f"âœ… {len(base64_images)}ê°œ í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")
        
        # Base64 ë¬¸ìì—´ â†’ numpy array ë³€í™˜
        page_images = []
        
        for idx, base64_str in enumerate(base64_images, 1):
            try:
                # Base64 ë””ì½”ë”©
                if base64_str.startswith('data:image'):
                    base64_str = base64_str.split(',', 1)[1]
                
                img_bytes = base64.b64decode(base64_str)
                pil_img = Image.open(io.BytesIO(img_bytes))
                np_img = np.array(pil_img.convert('RGB'))
                
                page_images.append(np_img)
                
                logger.info(f"  í˜ì´ì§€ {idx}: Base64 â†’ numpy array ë³€í™˜ ì™„ë£Œ {np_img.shape}")
                
            except Exception as e:
                logger.error(f"  í˜ì´ì§€ {idx} ë³€í™˜ ì‹¤íŒ¨: {e}")
                raise
        
        logger.info(f"âœ… {len(page_images)}ê°œ í˜ì´ì§€ numpy ë³€í™˜ ì™„ë£Œ\n")
        
        # Stage 2~3: í˜ì´ì§€ë³„ ì²˜ë¦¬
        all_regions = []
        all_extractions = []
        region_id_counter = 0
        
        for page_num, page_image in enumerate(page_images, 1):
            logger.info(f"{'='*60}")
            logger.info(f"í˜ì´ì§€ {page_num}/{len(page_images)} ì²˜ë¦¬")
            logger.info(f"{'='*60}")
            
            # Stage 2: Layout Detection
            logger.info(f"\n--- Stage 2: Layout Detection ---")
            
            try:
                regions = self.layout_detector.detect_regions(page_image, page_num)
                
                # region_id ë° page_number ì¶”ê°€
                for region in regions:
                    region_id_counter += 1
                    region['region_id'] = f"region_{region_id_counter:04d}"
                    region['page_number'] = page_num
                
                all_regions.extend(regions)
                
                logger.info(f"âœ… {len(regions)}ê°œ ì˜ì—­ ê°ì§€ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ Layout Detection ì‹¤íŒ¨: {e}")
                continue
            
            # Stage 3: Region-based Extraction
            logger.info(f"\n--- Stage 3: Region-based Extraction ---")
            
            for i, region in enumerate(regions, 1):
                logger.info(f"\n[Region {i}/{len(regions)}] {region['type']}")
                
                try:
                    # ì˜ì—­ í¬ë¡­
                    x, y, w, h = region['bbox']
                    
                    # ê²½ê³„ ì²´í¬
                    h_img, w_img = page_image.shape[:2]
                    x = max(0, min(x, w_img))
                    y = max(0, min(y, h_img))
                    w = max(1, min(w, w_img - x))
                    h = max(1, min(h, h_img - y))
                    
                    roi = page_image[y:y+h, x:x+w]
                    
                    # numpy array â†’ PIL Image â†’ Base64
                    pil_roi = Image.fromarray(roi)
                    buffer = io.BytesIO()
                    pil_roi.save(buffer, format='PNG')
                    buffer.seek(0)
                    roi_base64 = base64.b64encode(buffer.read()).decode('utf-8')
                    
                    # VLM ë¶„ì„
                    element_type = region['type']
                    prompt = ChartPrompts.get_prompt_for_type(element_type)
                    
                    # âœ… ìˆ˜ì •: VLM í˜¸ì¶œ íŒŒë¼ë¯¸í„° (image_data)
                    content = self.vlm_service.analyze_image(
                        image_data=roi_base64,
                        prompt=prompt
                    )
                    
                    # UTF-8 ì¸ì½”ë”© í™•ì¸
                    if content:
                        try:
                            content.encode('utf-8')
                        except UnicodeEncodeError:
                            logger.warning(f"  âš ï¸ UTF-8 ì¸ì½”ë”© ì˜¤ë¥˜ ê°ì§€, ì¬ì¸ì½”ë”© ì‹œë„")
                            content = content.encode('latin1').decode('utf-8', errors='ignore')
                    
                    # âœ… ìˆ˜ì •: extraction ë°ì´í„° êµ¬ì¡° (region_type ì¶”ê°€)
                    extraction = {
                        'region_id': region['region_id'],
                        'page_number': page_num,
                        'region_type': element_type,  # SectionChunkerê°€ ìš”êµ¬í•˜ëŠ” í‚¤
                        'type': element_type,  # í˜¸í™˜ì„± ìœ ì§€
                        'bbox': region['bbox'],
                        'confidence': region['confidence'],
                        'content': content or '',
                        'metadata': region.get('metadata', {})
                    }
                    
                    all_extractions.append(extraction)
                    
                    logger.info(f"  âœ… VLM ë¶„ì„ ì™„ë£Œ ({len(content or '')}ì)")
                    
                except Exception as e:
                    logger.error(f"  âŒ Region ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
        
        # Stage 4: Section-aware Chunking
        logger.info(f"\n{'='*60}")
        logger.info("--- Stage 4: Section-aware Chunking ---")
        logger.info(f"{'='*60}")
        
        try:
            # âœ… ìˆ˜ì •: chunk_extractions() ë©”ì†Œë“œ í˜¸ì¶œ
            chunks = self.chunker.chunk_extractions(all_extractions)
            logger.info(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n")
        except Exception as e:
            logger.error(f"âŒ Chunking ì‹¤íŒ¨: {e}")
            logger.exception(e)
            chunks = []
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        processing_time = time.time() - start_time
        
        # ê²°ê³¼ ì¡°í•©
        result = {
            'metadata': {
                'filename': filename,
                'total_pages': len(page_images),
                'total_regions': len(all_regions),
                'total_chunks': len(chunks),
                'processing_time_sec': round(processing_time, 2),
                'vlm_provider': self.vlm_service.provider,
                'processed_at': datetime.now().isoformat()
            },
            'regions': all_regions,
            'extractions': all_extractions,
            'chunks': chunks
        }
        
        logger.info(f"{'='*60}")
        logger.info("âœ… Phase 3.0 ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"   ì´ í˜ì´ì§€: {len(page_images)}ê°œ")
        logger.info(f"   ê°ì§€ëœ ì˜ì—­: {len(all_regions)}ê°œ")
        logger.info(f"   ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ")
        logger.info(f"   ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        logger.info(f"{'='*60}\n")
        
        return result


# í…ŒìŠ¤íŠ¸
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python -m core.phase30_pipeline <pdf_path>")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    from dotenv import load_dotenv
    load_dotenv()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = Phase30Pipeline(vlm_provider='azure_openai')
    result = pipeline.process_document(pdf_path, max_pages=3)
    
    # ê²°ê³¼ ì¶œë ¥
    print(json.dumps(result, ensure_ascii=False, indent=2))