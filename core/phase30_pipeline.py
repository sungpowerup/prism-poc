"""
PRISM Phase 3.0 - Main Pipeline (ìˆ˜ì •)
ì°¨íŠ¸ íƒ€ì… êµ¬ë¶„ + UTF-8 ì²˜ë¦¬
"""

import os
import json
import time
import base64
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import logging

from core.pdf_processor import PDFProcessor
from core.layout_detector import LayoutDetector
from core.vlm_service import VLMService
from core.section_chunker import SectionChunker
from prompts.chart_prompt import ChartPrompts

logger = logging.getLogger(__name__)


class Phase30Pipeline:
    """
    PRISM Phase 3.0 íŒŒì´í”„ë¼ì¸
    - ì°¨íŠ¸ íƒ€ì… êµ¬ë¶„
    - UTF-8 ì¸ì½”ë”© ì²˜ë¦¬
    - íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸
    """
    
    def __init__(self, vlm_provider: str = "azure_openai"):
        """
        Phase 3.0 íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            vlm_provider: VLM í”„ë¡œë°”ì´ë”
        """
        logger.info("="*60)
        logger.info("PRISM Phase 3.0 Pipeline ì´ˆê¸°í™”")
        logger.info("="*60)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.pdf_processor = PDFProcessor()
        logger.info("âœ… PDF Processor")
        
        self.vlm_service = VLMService(provider=vlm_provider)
        logger.info(f"âœ… VLM Service ({vlm_provider})")
        
        self.layout_detector = LayoutDetector()
        logger.info("âœ… Layout Detector")
        
        self.chunker = SectionChunker()
        logger.info("âœ… Section-aware Chunker")
        
        logger.info("="*60 + "\n")
    
    def process_document(
        self,
        pdf_path: str,
        max_pages: int = None
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ì²˜ë¦¬ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        
        filename = Path(pdf_path).name
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“„ PRISM Phase 3.0 ë¬¸ì„œ ì²˜ë¦¬")
        logger.info(f"   íŒŒì¼: {filename}")
        logger.info(f"{'='*60}")
        
        # Stage 1: PDF â†’ Images
        logger.info(f"\n--- Stage 1: PDF â†’ Page Images ---")
        page_images = self.pdf_processor.pdf_to_images(pdf_path)
        
        if max_pages:
            page_images = page_images[:max_pages]
        
        # PIL Image â†’ numpy array ë³€í™˜
        import numpy as np
        from PIL import Image
        
        converted_images = []
        for img in page_images:
            if isinstance(img, str):
                # Base64 ë¬¸ìì—´ì¸ ê²½ìš°
                img_bytes = base64.b64decode(img)
                pil_img = Image.open(io.BytesIO(img_bytes))
                np_img = np.array(pil_img)
            elif hasattr(img, 'convert'):
                # PIL Imageì¸ ê²½ìš°
                np_img = np.array(img.convert('RGB'))
            else:
                # ì´ë¯¸ numpy arrayì¸ ê²½ìš°
                np_img = img
            
            converted_images.append(np_img)
        
        page_images = converted_images
        
        logger.info(f"âœ… {len(page_images)}ê°œ í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ (numpy array)\n")
        
        # Stage 2~3: í˜ì´ì§€ë³„ ì²˜ë¦¬
        all_regions = []
        all_extractions = []
        
        for page_num, page_image in enumerate(page_images, 1):
            logger.info(f"{'='*60}")
            logger.info(f"í˜ì´ì§€ {page_num}/{len(page_images)} ì²˜ë¦¬")
            logger.info(f"{'='*60}")
            
            # Stage 2: Layout Detection
            logger.info(f"\n--- Stage 2: Layout Detection ---")
            regions = self.layout_detector.detect_regions(page_image, page_num)
            
            # ê° ì˜ì—­ì— page_number ì¶”ê°€
            for region in regions:
                region['page_number'] = page_num
            
            all_regions.extend(regions)
            
            # Stage 3: Region-based Extraction
            logger.info(f"\n--- Stage 3: Region-based Extraction ---")
            
            for i, region in enumerate(regions, 1):
                logger.info(f"\n[Region {i}/{len(regions)}] {region['type']}")
                
                # ì˜ì—­ í¬ë¡­
                x, y, w, h = region['bbox']
                roi = page_image[y:y+h, x:x+w]
                
                # Base64 ì¸ì½”ë”©
                from PIL import Image
                import io
                
                pil_roi = Image.fromarray(roi)
                buffer = io.BytesIO()
                pil_roi.save(buffer, format='PNG')
                roi_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                
                # VLM ë¶„ì„
                element_type = region['type']
                prompt = ChartPrompts.get_prompt_for_type(element_type)
                
                try:
                    content = self.vlm_service.analyze_image(
                        roi_base64,
                        element_type,
                        prompt
                    )
                    
                    # UTF-8 ê²€ì¦
                    content = self._ensure_utf8(content)
                    
                    # íŠ¹ìˆ˜ ì²˜ë¦¬: ì§€ë„ íƒ€ì…
                    if element_type == "map":
                        content = self._process_map_content(content)
                    
                    logger.info(f"   ì¶”ì¶œ ì™„ë£Œ: {len(content)}ì")
                
                except Exception as e:
                    logger.error(f"   ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    content = f"[{element_type} ì¶”ì¶œ ì‹¤íŒ¨]"
                
                # ì¶”ì¶œ ê²°ê³¼ ì €ì¥
                extraction = {
                    'page_number': page_num,
                    'region_number': i,
                    'region_type': element_type,
                    'bbox': region['bbox'],
                    'content': content,
                    'metadata': region.get('metadata', {})
                }
                
                all_extractions.append(extraction)
        
        # Stage 4: Section-aware Chunking
        logger.info(f"\n{'='*60}")
        logger.info(f"--- Stage 4: Section-aware Chunking ---")
        logger.info(f"{'='*60}")
        
        chunks = self.chunker.chunk_extractions(all_extractions)
        
        logger.info(f"\nâœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        
        # ê²°ê³¼ ìƒì„±
        total_time = time.time() - start_time
        
        result = {
            'metadata': {
                'filename': filename,
                'phase': '3.0',
                'vlm_provider': self.vlm_service.provider,
                'processed_at': datetime.now().isoformat(),
                'total_pages': len(page_images),
                'total_regions': len(all_regions),
                'total_chunks': len(chunks),
                'processing_time_sec': round(total_time, 2)
            },
            'regions': [
                {
                    'region_id': f"page{r['page_number']}_{r['type']}{i}",
                    'bbox': r['bbox'],
                    'type': r['type'],
                    'confidence': r['confidence'],
                    'metadata': r.get('metadata', {})
                }
                for i, r in enumerate(all_regions, 1)
            ],
            'extractions': all_extractions,
            'chunks': chunks
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(result)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… Phase 3.0 ì²˜ë¦¬ ì™„ë£Œ")
        logger.info(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"   ê°ì§€ëœ ì˜ì—­: {len(all_regions)}ê°œ")
        logger.info(f"   ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ")
        logger.info(f"{'='*60}")
        
        return result
    
    def _process_map_content(self, content: str) -> str:
        """
        ì§€ë„ ì½˜í…ì¸  íŠ¹ìˆ˜ ì²˜ë¦¬
        
        Args:
            content: VLM ì‘ë‹µ
            
        Returns:
            ì²˜ë¦¬ëœ ì½˜í…ì¸ 
        """
        try:
            # JSON í˜•ì‹ ì¶”ì¶œ ì‹œë„
            data = self.vlm_service.extract_map_data(content)
            
            if 'regions' in data and data['regions']:
                # JSON ë°ì´í„°ë¥¼ ìì—°ì–´ë¡œ ë³€í™˜
                lines = ["[ì§€ì—­ë³„ ë¶„í¬]"]
                for region in data['regions']:
                    name = region.get('name', '')
                    value = region.get('value', '')
                    lines.append(f"- {name}: {value}")
                
                return "\n".join(lines)
            else:
                # JSONì´ ì•„ë‹Œ ìì—°ì–´ í…ìŠ¤íŠ¸
                return content
        
        except Exception as e:
            logger.error(f"   ì§€ë„ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return content
    
    def _ensure_utf8(self, text: str) -> str:
        """
        UTF-8 ì¸ì½”ë”© ë³´ì¥
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            UTF-8 í…ìŠ¤íŠ¸
        """
        try:
            text.encode('utf-8').decode('utf-8')
            return text
        except UnicodeDecodeError:
            # Latin-1 â†’ UTF-8 ë³€í™˜
            try:
                return text.encode('latin-1').decode('utf-8')
            except:
                # ê°•ì œ ë³€í™˜
                return text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
    
    def _save_results(self, result: Dict[str, Any]):
        """
        ê²°ê³¼ ì €ì¥ (JSON + Markdown)
        
        Args:
            result: ì²˜ë¦¬ ê²°ê³¼
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"prism_phase30_{timestamp}"
        
        # JSON ì €ì¥
        json_path = output_dir / f"{base_name}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {json_path}")
        
        # Markdown ì €ì¥
        md_path = output_dir / f"{base_name}.md"
        md_content = self._generate_markdown(result)
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        logger.info(f"ğŸ’¾ ë§ˆí¬ë‹¤ìš´: {md_path}")
    
    def _generate_markdown(self, result: Dict[str, Any]) -> str:
        """
        Markdown ìƒì„±
        
        Args:
            result: ì²˜ë¦¬ ê²°ê³¼
            
        Returns:
            Markdown ë¬¸ìì—´
        """
        lines = [
            "# PRISM Phase 3.0 - êµ¬ì¡°í™”ëœ ë¬¸ì„œ ì¶”ì¶œ\n",
            f"**ìƒì„±ì¼ì‹œ**: {result['metadata']['processed_at']}\n",
            "---\n",
            "## ğŸ“„ ë¬¸ì„œ ì •ë³´\n",
            f"- **íŒŒì¼ëª…**: {result['metadata']['filename']}",
            f"- **ì´ í˜ì´ì§€**: {result['metadata']['total_pages']}ê°œ",
            f"- **ì´ ì˜ì—­**: {result['metadata']['total_regions']}ê°œ",
            f"- **ì´ ì²­í¬**: {result['metadata']['total_chunks']}ê°œ",
            f"- **ì²˜ë¦¬ ì‹œê°„**: {result['metadata']['processing_time_sec']}ì´ˆ",
            f"- **Phase**: {result['metadata']['phase']}\n",
            "## ğŸ¯ ê°ì§€ëœ ì˜ì—­\n"
        ]
        
        # ì˜ì—­ ëª©ë¡
        for i, region in enumerate(result['regions'], 1):
            lines.append(f"### Region #{i}: {region['type']}\n")
            lines.append(f"- **ID**: {region['region_id']}")
            lines.append(f"- **ì‹ ë¢°ë„**: {region['confidence']*100:.2f}%")
            lines.append(f"- **ìœ„ì¹˜**: {tuple(region['bbox'])}\n")
        
        # ì²­í¬ ëª©ë¡
        lines.append("## ğŸ§© ì²­í¬\n")
        
        for i, chunk in enumerate(result['chunks'], 1):
            lines.append(f"### ì²­í¬ #{i}\n")
            lines.append(f"- **ID**: {chunk['chunk_id']}")
            lines.append(f"- **íƒ€ì…**: {chunk['metadata']['chunk_type']}")
            lines.append(f"- **ì„¹ì…˜**: {chunk['metadata']['section_path']}")
            lines.append(f"- **í˜ì´ì§€**: {chunk['metadata']['page_number']}\n")
            lines.append("```")
            lines.append(chunk['content'])
            lines.append("```\n")
        
        return "\n".join(lines)