"""
core/tree_builder.py - PRISM Phase 0.8.5 Pattern Fix
TreeBuilder ì¡°ë¬¸ íŒ¨í„´ ìˆ˜ì •

Phase 0.8.5 í•µì‹¬ ìˆ˜ì •:
- âœ… ARTICLE_PATTERNì—ì„œ ^ (ì¤„ ì‹œì‘) ì•µì»¤ ì œê±°
- âœ… ëª¨ë“  ìœ„ì¹˜ì—ì„œ ì¡°ë¬¸ ê°ì§€ ê°€ëŠ¥
- âœ… ì œ4ì¡° ëˆ„ë½ ë° ì œ28ì¡° ìœ ë ¹ ì¡°ë¬¸ ë¬¸ì œ í•´ê²°

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-19
Version: Phase 0.8.5 Pattern Fix
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)


class TreeBuilder:
    """
    Phase 0.8.5 TreeBuilder - íŒ¨í„´ ìˆ˜ì •íŒ
    
    í•µì‹¬ ìˆ˜ì •:
    - ARTICLE_PATTERN ìœ ì—°í™” (ì¤„ ì‹œì‘ ì•µì»¤ ì œê±°)
    - PDF ì¶”ì¶œ í…ìŠ¤íŠ¸ì˜ ë‹¤ì–‘í•œ í¬ë§· ì§€ì›
    """
    
    # âœ… Phase 0.8.5: ìˆ˜ì •ëœ íŒ¨í„´ (ì¤„ ì‹œì‘ ì•µì»¤ ì œê±°)
    # ì›ë˜: r'^\s{0,3}#{0,6}\s*(ì œ\s?\d+ì¡°(?:ì˜\s?\d+)?)(?:\s*\(([^)]+)\))?'
    # ìˆ˜ì •: ì¤„ ì–´ë””ì„œë“  ë§¤ì¹­ ê°€ëŠ¥
    ARTICLE_PATTERN = re.compile(
        r'(ì œ\d+ì¡°(?:ì˜\d+)?)\s*[\(\[]([^\)\]]+)[\)\]]',
        re.MULTILINE
    )
    
    # í•­Â·í˜¸ íŒ¨í„´
    CLAUSE_PATTERN = re.compile(
        r'(?:^|\n)\s*(?:([â‘ -â‘³])|ì œ?\s*(\d+)\s*í•­)',
        re.MULTILINE
    )
    
    ITEM_PATTERN = re.compile(r'(?:^|\n)\s*(\d{1,2})[.)]', re.MULTILINE)
    
    # Chapter íŒ¨í„´
    CHAPTER_PATTERN = re.compile(r'(ì œ\s*\d+\s*ì¥)\s*(.+)?')
    
    # ì‚­ì œ ì¡°ë¬¸ íŒ¨í„´
    DELETED_PATTERN = re.compile(r'<ì‚­ì œ\s*(\d{4}\.\d{2}\.\d{2})>')
    
    # ê°œì •ì¼ íŒ¨í„´
    AMENDED_PATTERN = re.compile(r'\[.*?(\d{4}\.\d{2}\.\d{2}).*?\]')
    
    # í˜ì´ì§€ êµ¬ë¶„ì íŒ¨í„´
    PAGE_DIVIDER_PATTERNS = [
        re.compile(r'^#{1,3}\s*Page\s+\d+\s*$', re.IGNORECASE),
        re.compile(r'^Page\s+\d+\s*$', re.IGNORECASE),
        re.compile(r'^[-â€”â€“_]{3,}\s*$'),
        re.compile(r'^[*]{3,}\s*$'),
        re.compile(r'^[=]{3,}\s*$'),
    ]
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… TreeBuilder v0.8.5 ì´ˆê¸°í™” ì™„ë£Œ (Pattern Fix)")
    
    def build(
        self,
        markdown: str,
        document_title: str = "",
        enacted_date: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Markdownì„ Treeë¡œ ë³€í™˜
        """
        logger.info(f"ğŸŒ² TreeBuilder ì‹œì‘: {document_title}")
        
        # í˜ì´ì§€ êµ¬ë¶„ì ì œê±°
        markdown, removed_count = self._clean_page_dividers(markdown)
        logger.info(f"   ğŸ—‘ï¸ í˜ì´ì§€ êµ¬ë¶„ì ì œê±°: {removed_count}ê°œ ë¼ì¸")
        
        # ì¡°ë¬¸ íŒŒì‹±
        articles = self._parse_articles(markdown)
        logger.info(f"   ğŸ“„ ì¡°ë¬¸ íŒŒì‹± ì™„ë£Œ: {len(articles)}ê°œ")
        
        # ë©”íƒ€ë°ì´í„°
        metadata = {
            'title': document_title,
            'enacted_date': enacted_date or '',
            'extracted_at': datetime.now().isoformat(),
            'version': '0.8.5'
        }
        
        # Document ìŠ¤í‚¤ë§ˆ
        document = {
            'document': {
                'metadata': metadata,
                'tree': articles
            }
        }
        
        logger.info(f"âœ… TreeBuilder ì™„ë£Œ")
        return document
    
    def _clean_page_dividers(self, markdown: str) -> Tuple[str, int]:
        """í˜ì´ì§€ êµ¬ë¶„ì ì œê±°"""
        lines = markdown.split('\n')
        cleaned_lines = []
        removed_count = 0
        
        for line in lines:
            line_stripped = line.strip()
            
            if not line_stripped:
                cleaned_lines.append(line)
                continue
            
            is_divider = False
            for pattern in self.PAGE_DIVIDER_PATTERNS:
                if pattern.match(line_stripped):
                    is_divider = True
                    removed_count += 1
                    break
            
            if not is_divider:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines), removed_count
    
    def _parse_articles(self, markdown: str) -> List[Dict[str, Any]]:
        """
        ì¡°ë¬¸ íŒŒì‹± (Phase 0.8.5 ìˆ˜ì •íŒ)
        
        ì „ëµ:
        1. ë¨¼ì € ëª¨ë“  ì¡°ë¬¸ ìœ„ì¹˜ë¥¼ ì°¾ê¸°
        2. ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
        3. ê° ì¡°ë¬¸ ë‚´ìš© ì¶”ì¶œ
        """
        articles = []
        
        # 1. ëª¨ë“  ì¡°ë¬¸ ìœ„ì¹˜ ì°¾ê¸°
        matches = list(self.ARTICLE_PATTERN.finditer(markdown))
        
        if not matches:
            logger.warning("   âš ï¸ ì¡°ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return articles
        
        # 2. ì²« ë“±ì¥ë§Œ ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
        seen = set()
        unique_matches = []
        for m in matches:
            article_no = m.group(1)
            if article_no not in seen:
                seen.add(article_no)
                unique_matches.append(m)
        
        # 3. ì¥(Chapter) ì¶”ì¶œ
        chapter_matches = list(self.CHAPTER_PATTERN.finditer(markdown))
        chapter_map = {}  # position -> chapter_name
        for cm in chapter_matches:
            chapter_map[cm.start()] = cm.group(1) + (' ' + cm.group(2).strip() if cm.group(2) else '')
        
        # 4. ê° ì¡°ë¬¸ íŒŒì‹±
        current_chapter = ""
        
        for i, m in enumerate(unique_matches):
            article_no = m.group(1)
            article_title = m.group(2) or ''
            start_pos = m.end()
            
            # ë‹¤ìŒ ì¡°ë¬¸ê¹Œì§€ ë˜ëŠ” ëê¹Œì§€
            if i + 1 < len(unique_matches):
                end_pos = unique_matches[i + 1].start()
            else:
                # ë§ˆì§€ë§‰ ì¡°ë¬¸: [ë³„í‘œ] ì „ê¹Œì§€
                annex_match = re.search(r'\[ë³„í‘œ', markdown[start_pos:])
                if annex_match:
                    end_pos = start_pos + annex_match.start()
                else:
                    end_pos = len(markdown)
            
            # ì¡°ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content = markdown[start_pos:end_pos].strip()
            
            # ì¥ ê²°ì • (ì´ ì¡°ë¬¸ ì´ì „ì˜ ê°€ì¥ ê°€ê¹Œìš´ ì¥)
            for ch_pos in sorted(chapter_map.keys()):
                if ch_pos < m.start():
                    current_chapter = chapter_map[ch_pos]
            
            # ê°œì •ì¼ ì¶”ì¶œ
            amended_dates = self.AMENDED_PATTERN.findall(content)
            
            # Article ë…¸ë“œ ìƒì„±
            article = {
                'level': 'article',
                'article_no': article_no,
                'article_title': article_title,
                'content': content,
                'chapter': current_chapter,
                'children': [],
                'metadata': {
                    'amended_dates': list(set(amended_dates)),
                    'is_deleted': bool(self.DELETED_PATTERN.search(content)),
                },
                'position': {
                    'start': m.start(),
                    'end': end_pos
                }
            }
            
            articles.append(article)
            logger.debug(f"      ì¡°ë¬¸: {article_no}({article_title}) @ {m.start()}")
        
        return articles


# í•˜ìœ„ í˜¸í™˜ì„±
parse_markdown = TreeBuilder.build