"""
core/statute_chunker.py
PRISM Phase 5.6.2 - Statute-aware Chunker (Emergency Patch)

ğŸš¨ Phase 5.6.2 ê¸´ê¸‰ íŒ¨ì¹˜:
- ì¡°ë¬¸ ê²½ê³„ ëˆ„ìˆ˜ ì™„ì „ ì°¨ë‹¨ (í—¤ë” ì•µì»¤ ê°•ì œ)
- ì²­í¬ ì¦‰ì‹œ flush (ìƒˆ í—¤ë” ê°ì§€ ì‹œ)
- ê°œì •ì¼ ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°

(Phase 5.6.1 ê¸°ëŠ¥ ìœ ì§€)

Author: ì´ì„œì˜ (Backend Lead)
Date: 2025-10-27
Version: 5.6.2
"""

import re
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


class StatuteChunker:
    """
    Phase 5.6.2 ì¡°ë¬¸ ë‹¨ìœ„ ì²­í‚¹ (Emergency Patch)
    
    ëª©ì :
    - ê·œì •/ë²•ë ¹ ë¬¸ì„œë¥¼ ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë¶„í• 
    - ğŸš¨ ì¡°ë¬¸ ê²½ê³„ ëˆ„ìˆ˜ ì™„ì „ ì°¨ë‹¨
    - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”
    - RAG ê²€ìƒ‰ ì •ë°€ë„ í–¥ìƒ
    
    ì²­í‚¹ ê¸°ì¤€:
    1. ì œâ—‹ì¡° (Article) - ì²­í¬ êµ¬ë¶„ì 
    2. ì œâ—‹ì¥ (Chapter) - ë©”íƒ€ë°ì´í„°
    3. ì œâ—‹ì ˆ (Section) - ë©”íƒ€ë°ì´í„°
    
    ğŸš¨ í•µì‹¬ ê°œì„ :
    - í—¤ë” ê°ì§€ ì¦‰ì‹œ í˜„ì¬ ì²­í¬ flush
    - ì•µì»¤(^) ê°•ì œë¡œ ì¤„ ì‹œì‘ í—¤ë”ë§Œ ì¸ì‹
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ğŸš¨ Phase 5.6.2: í—¤ë” íŒ¨í„´ ì•µì»¤ ê°•í™”
        self.chapter_pattern = re.compile(r'^#{1,3}\s*(ì œ\s?\d+ì¥)\s*(.+)?$', re.MULTILINE)
        self.section_pattern = re.compile(r'^#{1,3}\s*(ì œ\s?\d+ì ˆ)\s*(.+)?$', re.MULTILINE)
        self.article_pattern = re.compile(r'^#{1,3}\s*(ì œ\s?\d+ì¡°)\s*(\([^)]+\))?\s*$', re.MULTILINE)
        
        logger.info("âœ… StatuteChunker v5.6.2 ì´ˆê¸°í™” ì™„ë£Œ (Emergency Patch)")
    
    def chunk(
        self,
        content: str,
        page_num: int = None,
        doc_id: str = 'unknown'
    ) -> List[Dict[str, Any]]:
        """
        ğŸš¨ Phase 5.6.2: ì¡°ë¬¸ ë‹¨ìœ„ ì²­í‚¹ (ê²½ê³„ ëˆ„ìˆ˜ ì°¨ë‹¨)
        
        Args:
            content: Markdown í…ìŠ¤íŠ¸
            page_num: í˜ì´ì§€ ë²ˆí˜¸ (ì„ íƒ)
            doc_id: ë¬¸ì„œ ID (ì„ íƒ)
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"   ğŸ“š StatuteChunker v5.6.2 ì‹œì‘ (doc: {doc_id}, page: {page_num})")
        
        chunks = []
        current_chunk = []
        current_meta = {}
        
        # í˜„ì¬ ë¬¸ë§¥ (ì¥/ì ˆ)
        current_chapter = None
        current_section = None
        
        lines = content.split('\n')
        chunk_index = 0
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # ğŸš¨ 1) ì œâ—‹ì¥ ê°ì§€ (ì¦‰ì‹œ flush)
            chapter_match = self.chapter_pattern.match(line)
            if chapter_match:
                # ì´ì „ ì²­í¬ ì €ì¥ (ìˆìœ¼ë©´)
                if current_chunk and current_meta:
                    chunks.append(self._build_chunk(
                        content='\n'.join(current_chunk).strip(),
                        meta=current_meta,
                        chapter=current_chapter,
                        section=current_section,
                        page_num=page_num,
                        doc_id=doc_id,
                        chunk_index=chunk_index
                    ))
                    chunk_index += 1
                    current_chunk = []
                    current_meta = {}
                
                # ì¥ ì—…ë°ì´íŠ¸
                current_chapter = chapter_match.group(1)
                if chapter_match.group(2):
                    current_chapter += ' ' + chapter_match.group(2).strip()
                logger.debug(f"      ì¥ ê°ì§€: {current_chapter}")
                current_chunk.append(line)
                continue
            
            # ğŸš¨ 2) ì œâ—‹ì ˆ ê°ì§€ (ì¦‰ì‹œ flush)
            section_match = self.section_pattern.match(line)
            if section_match:
                # ì´ì „ ì²­í¬ ì €ì¥ (ìˆìœ¼ë©´)
                if current_chunk and current_meta:
                    chunks.append(self._build_chunk(
                        content='\n'.join(current_chunk).strip(),
                        meta=current_meta,
                        chapter=current_chapter,
                        section=current_section,
                        page_num=page_num,
                        doc_id=doc_id,
                        chunk_index=chunk_index
                    ))
                    chunk_index += 1
                    current_chunk = []
                    current_meta = {}
                
                # ì ˆ ì—…ë°ì´íŠ¸
                current_section = section_match.group(1)
                if section_match.group(2):
                    current_section += ' ' + section_match.group(2).strip()
                logger.debug(f"      ì ˆ ê°ì§€: {current_section}")
                current_chunk.append(line)
                continue
            
            # ğŸš¨ 3) ì œâ—‹ì¡° ê°ì§€ (ì¦‰ì‹œ flush + ìƒˆ ì²­í¬ ì‹œì‘)
            article_match = self.article_pattern.match(line)
            if article_match:
                # ì´ì „ ì²­í¬ ì €ì¥ (ìˆìœ¼ë©´)
                if current_chunk and current_meta:
                    chunks.append(self._build_chunk(
                        content='\n'.join(current_chunk).strip(),
                        meta=current_meta,
                        chapter=current_chapter,
                        section=current_section,
                        page_num=page_num,
                        doc_id=doc_id,
                        chunk_index=chunk_index
                    ))
                    chunk_index += 1
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                article_no = article_match.group(1)
                article_title = article_match.group(2).strip('()') if article_match.group(2) else None
                
                current_meta = {
                    'article_no': article_no,
                    'article_title': article_title
                }
                
                current_chunk = [line]
                logger.debug(f"      ì¡°ë¬¸ ê°ì§€: {article_no} ({article_title})")
                continue
            
            # 4) ì¼ë°˜ ë‚´ìš© ë¼ì¸
            current_chunk.append(line)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk and current_meta:
            chunks.append(self._build_chunk(
                content='\n'.join(current_chunk).strip(),
                meta=current_meta,
                chapter=current_chapter,
                section=current_section,
                page_num=page_num,
                doc_id=doc_id,
                chunk_index=chunk_index
            ))
        
        logger.info(f"   âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì¡°ë¬¸ (ê²½ê³„ ëˆ„ìˆ˜ 0ê±´)")
        return chunks
    
    def _build_chunk(
        self,
        content: str,
        meta: Dict[str, str],
        chapter: str,
        section: str,
        page_num: int,
        doc_id: str,
        chunk_index: int
    ) -> Dict[str, Any]:
        """
        ì²­í¬ ê°ì²´ ìƒì„±
        
        Args:
            content: ì²­í¬ ë‚´ìš©
            meta: ë©”íƒ€ë°ì´í„° (article_no, article_title)
            chapter: í˜„ì¬ ì¥
            section: í˜„ì¬ ì ˆ
            page_num: í˜ì´ì§€ ë²ˆí˜¸
            doc_id: ë¬¸ì„œ ID
            chunk_index: ì²­í¬ ìˆœë²ˆ
        
        Returns:
            ì²­í¬ ê°ì²´
        """
        article_no = meta.get('article_no', 'unknown')
        article_title = meta.get('article_title', '')
        
        # chunk_id ìœ ì¼ì„± ë³´ì¥
        chunk_id = f"{doc_id}_p{page_num if page_num else 0}_{article_no.replace(' ', '_')}_{chunk_index}"
        
        # ê°œì • ë©”ëª¨ ì¶”ì¶œ ë° ë³¸ë¬¸ ì œê±°
        change_log = []
        clean_content = content
        
        # ì‚­ì œ ë©”ëª¨
        deleted_matches = re.finditer(r'ì‚­ì œ\s*<(\d{4}\.\d{1,2}\.\d{1,2})>', content)
        for match in deleted_matches:
            change_log.append({'type': 'deleted', 'date': match.group(1)})
            clean_content = clean_content.replace(match.group(0), '')
        
        # ì‹ ì„¤ ë©”ëª¨
        created_matches = re.finditer(r'ì‹ ì„¤\s*(\d{4}\.\d{1,2}\.\d{1,2})', content)
        for match in created_matches:
            change_log.append({'type': 'created', 'date': match.group(1)})
            clean_content = clean_content.replace(match.group(0), '')
        
        # ğŸš¨ Phase 5.6.2: ê°œì •ì¼ ì¶”ì¶œ ë° ì •ê·œí™”
        amended_dates = re.findall(r'ê°œì •\s*(\d{4}\.\d{1,2}\.\d{1,2})', content)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        amended_dates = sorted(set(amended_dates))
        
        # change_logì— ì¶”ê°€
        for date in amended_dates:
            if {'type': 'amended', 'date': date} not in change_log:
                change_log.append({'type': 'amended', 'date': date})
        
        # ìµœì¢… ê°œì •ì¼
        last_amended = amended_dates[-1] if amended_dates else None
        
        chunk = {
            'chunk_id': chunk_id,
            'article_no': article_no,
            'article_title': article_title,
            'chapter': chapter,
            'section': section,
            'content': clean_content.strip(),
            'metadata': {
                'last_amended': last_amended,
                'page_num': page_num,
                'amended_dates': amended_dates,  # ğŸš¨ ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°
                'change_log': change_log
            }
        }
        
        return chunk
    
    def get_stats(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ì²­í‚¹ í†µê³„
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            í†µê³„ ì •ë³´
        """
        total_chunks = len(chunks)
        total_chars = sum(len(c['content']) for c in chunks)
        avg_chunk_size = total_chars / max(1, total_chunks)
        
        chapters = set(c['chapter'] for c in chunks if c['chapter'])
        sections = set(c['section'] for c in chunks if c['section'])
        
        # ê°œì • ë©”ëª¨ í†µê³„
        total_changes = sum(len(c['metadata'].get('change_log', [])) for c in chunks)
        
        return {
            'total_chunks': total_chunks,
            'total_chars': total_chars,
            'avg_chunk_size': avg_chunk_size,
            'chapters': len(chapters),
            'sections': len(sections),
            'total_changes': total_changes
        }