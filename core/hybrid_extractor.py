"""
core/hybrid_extractor.py
PRISM Phase 5.3.0 - Hybrid Extractor

ëª©ì : CV íŒíŠ¸ + VLM ë©”íƒ€ í”„ë¡¬í”„íŠ¸ + KVS ì €ì¥
GPT ì œì•ˆ í†µí•©:
1. DSL ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
2. ê°•í™”ëœ ê²€ì¦
3. KVS ë³„ë„ í˜ì´ë¡œë“œ ì €ì¥
"""

import logging
import re
import json
from typing import Dict, Any, Optional
from pathlib import Path

# Phase 5.3.0 ëª¨ë“ˆ import (ì •ë¦¬ë¨)
from .quick_layout_analyzer import QuickLayoutAnalyzer
from .prompt_rules import PromptRules
from .kvs_normalizer import KVSNormalizer  # GPT ì œì•ˆ #4

logger = logging.getLogger(__name__)


class HybridExtractor:
    """
    Phase 5.3.0 í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œê¸°
    
    ì „ëµ:
    1. QuickLayoutAnalyzerë¡œ êµ¬ì¡° íŒíŠ¸ íšë“
    2. PromptRules DSLë¡œ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„± (GPT ì œì•ˆ)
    3. VLM 1íšŒ í˜¸ì¶œë¡œ ì™„ì „ ì¶”ì¶œ
    4. PromptRulesë¡œ ê°•í™” ê²€ì¦ (GPT ì œì•ˆ)
    5. KVS ë³„ë„ ì¶”ì¶œ ë° ì €ì¥ (GPT ì œì•ˆ)
    """
    
    def __init__(self, vlm_service):
        """
        Args:
            vlm_service: VLMServiceV50 ì¸ìŠ¤í„´ìŠ¤
        """
        self.vlm = vlm_service
        self.analyzer = QuickLayoutAnalyzer()
        self.max_retries = 1  # GPT ì œì•ˆ: ì¬ì¶”ì¶œ 1íšŒë§Œ
        logger.info("âœ… HybridExtractor ì´ˆê¸°í™” (Phase 5.3.0)")
    
    def extract(self, image_data: str, page_num: int = 1) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            image_data: Base64 ì¸ì½”ë”© ì´ë¯¸ì§€
            page_num: í˜ì´ì§€ ë²ˆí˜¸
            
        Returns:
            {
                'content': str,           # Markdown ë³¸ë¬¸
                'kvs': Dict,              # Key-Value Structured (GPT ì œì•ˆ)
                'confidence': float,
                'doc_type': str,
                'hints': Dict,
                'quality_score': float,
                'validation': Dict,
                'metrics': Dict           # ê´€ì¸¡ì„± (GPT ì œì•ˆ)
            }
        """
        logger.info(f"ğŸ¯ Page {page_num}: Phase 5.3.0 Hybrid ì¶”ì¶œ ì‹œì‘")
        
        import time
        start_time = time.time()
        
        try:
            # Step 1: CV íŒíŠ¸ ìƒì„± (0.5ì´ˆ)
            cv_start = time.time()
            hints = self.analyzer.analyze(image_data)
            cv_time = time.time() - cv_start
            logger.info(f"   ğŸ“ CV íŒíŠ¸ ({cv_time:.2f}ì´ˆ): {hints}")
            
            # Step 2: DSL ê¸°ë°˜ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„± (GPT ì œì•ˆ)
            prompt = PromptRules.build_prompt(hints)
            logger.debug(f"   ğŸ“ DSL í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {len(prompt)} ê¸€ì")
            
            # Step 3: VLM ì¶”ì¶œ (3ì´ˆ)
            vlm_start = time.time()
            content = self._call_vlm(image_data, prompt)
            vlm_time = time.time() - vlm_start
            logger.info(f"   âœ… VLM ì¶”ì¶œ ì™„ë£Œ ({vlm_time:.2f}ì´ˆ): {len(content)} ê¸€ì")
            
            # Step 4: ì˜¤íƒˆì êµì • (GPT ì œì•ˆ)
            content = PromptRules.correct_typos(content)
            
            # Step 5: ê°•í™” ê²€ì¦ (GPT ì œì•ˆ)
            validation = PromptRules.validate_extraction(content, hints)
            
            retry_count = 0
            if not validation['passed'] and retry_count < self.max_retries:
                logger.warning(f"   âš ï¸ ê²€ì¦ ì‹¤íŒ¨: {validation['missing']}")
                logger.info(f"   â™»ï¸ ì¬ì¶”ì¶œ ì‹œì‘ (ì‹œë„ {retry_count + 1}/{self.max_retries})")
                
                # Step 6: ì¬ì¶”ì¶œ (ì„ íƒì )
                retry_start = time.time()
                content = self._focused_reextraction(
                    image_data,
                    hints,
                    content,
                    validation['missing']
                )
                retry_time = time.time() - retry_start
                logger.info(f"   âœ… ì¬ì¶”ì¶œ ì™„ë£Œ ({retry_time:.2f}ì´ˆ): {len(content)} ê¸€ì")
                
                # ì¬ê²€ì¦
                validation = PromptRules.validate_extraction(content, hints)
                retry_count += 1
            
            # Step 7: KVS ì¶”ì¶œ (GPT ì œì•ˆ #3)
            kvs = self._extract_kvs(content, hints)
            
            # Step 7.5: KVS ì •ê·œí™” (GPT ì œì•ˆ #4)
            if kvs:
                kvs = KVSNormalizer.normalize_kvs(kvs)
                logger.info(f"   ğŸ“Š KVS ì •ê·œí™” ì™„ë£Œ: {len(kvs)}ê°œ í•­ëª©")
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_quality(content, hints, validation)
            
            # ê´€ì¸¡ì„± ë©”íŠ¸ë¦­ (GPT ì œì•ˆ)
            total_time = time.time() - start_time
            metrics = {
                'cv_time': cv_time,
                'vlm_time': vlm_time,
                'total_time': total_time,
                'retry_count': retry_count,
                'content_length': len(content),
                'kvs_count': len(kvs)
            }
            
            return {
                'content': content,
                'kvs': kvs,
                'confidence': 0.9 if validation['passed'] else 0.7,
                'doc_type': self._infer_doc_type(hints),
                'hints': hints,
                'quality_score': quality_score,
                'validation': validation,
                'metrics': metrics
            }
            
        except Exception as e:
            logger.error(f"   âŒ Hybrid ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise
    
    def _call_vlm(self, image_data: str, prompt: str) -> str:
        """VLM í˜¸ì¶œ (Azure OpenAI ë˜ëŠ” Claude)"""
        return self.vlm.call(image_data, prompt)
    
    def _focused_reextraction(
        self,
        image_data: str,
        hints: Dict,
        prev_content: str,
        missing: list[str]
    ) -> str:
        """
        ëˆ„ë½ ìš”ì†Œ ì§‘ì¤‘ ì¬ì¶”ì¶œ (GPT ì œì•ˆ: ëˆ„ë½ ì„¹ì…˜ë§Œ ê°•ì œ)
        
        ì „ëµ: PromptRulesì˜ retry í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        """
        # DSL ê¸°ë°˜ ì¬ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
        retry_prompt = PromptRules.build_retry_prompt(hints, missing, prev_content)
        
        # VLM ì¬í˜¸ì¶œ
        additional = self._call_vlm(image_data, retry_prompt)
        
        # ê¸°ì¡´ + ì¶”ê°€ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        merged = self._merge_content(prev_content, additional)
        
        return merged
    
    def _merge_content(self, prev: str, additional: str) -> str:
        """
        ê¸°ì¡´ ë‚´ìš©ê³¼ ì¶”ê°€ ë‚´ìš© ë³‘í•© (ì¤‘ë³µ ì œê±°)
        
        GPT ì œì•ˆ: [RETRY] í—¤ë”ë¡œ êµ¬ë¶„
        """
        # [RETRY] ì„¹ì…˜ë§Œ ì¶”ì¶œ
        retry_sections = []
        for line in additional.split('\n'):
            if '[RETRY]' in line or retry_sections:
                retry_sections.append(line)
        
        if retry_sections:
            # ê¸°ì¡´ + [RETRY] ì„¹ì…˜
            return prev + '\n\n' + '\n'.join(retry_sections)
        else:
            # [RETRY] í—¤ë” ì—†ìœ¼ë©´ ì „ì²´ ì¶”ê°€
            return prev + '\n\n## ì¶”ê°€ ì¶”ì¶œ ë‚´ìš©\n' + additional
    
    def _extract_kvs(self, content: str, hints: Dict) -> Dict[str, str]:
        """
        Key-Value Structured ë°ì´í„° ì¶”ì¶œ (GPT ì œì•ˆ #3)
        
        ëª©ì : RAG í•„ë“œ ê²€ìƒ‰ ìµœì í™”
        
        Returns:
            {
                'ë°°ì°¨ê°„ê²©': '27ë¶„',
                'ì²«ì°¨': '05:30',
                'ë§‰ì°¨': '22:40',
                ...
            }
        """
        if not hints.get('has_numbers'):
            return {}
        
        kvs = {}
        
        # KVS íŒ¨í„´ ë§¤ì¹­
        patterns = [
            # "í‚¤: ê°’" í˜•ì‹
            (r'([ê°€-í£a-zA-Z\s]+):\s*([0-9:ë¶„ì›%ëª…ëŒ€ì´ˆ]+)', 1, 2),
            # "í‚¤ ê°’" í˜•ì‹ (ë„ì–´ì“°ê¸°)
            (r'(ë°°ì°¨ê°„ê²©|ì²«ì°¨|ë§‰ì°¨|ë…¸ì„ ë²ˆí˜¸)\s+([0-9:ë¶„]+)', 1, 2),
            # "í‚¤ëŠ” ê°’" í˜•ì‹
            (r'([ê°€-í£]+)ëŠ”\s+([0-9:ë¶„ì›%ëª…ëŒ€ì´ˆ]+)', 1, 2),
        ]
        
        for pattern, key_group, val_group in patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                key = match.group(key_group).strip()
                value = match.group(val_group).strip()
                
                # ì¤‘ìš” í‚¤ë§Œ ì €ì¥
                important_keys = ['ë°°ì°¨ê°„ê²©', 'ì²«ì°¨', 'ë§‰ì°¨', 'ë…¸ì„ ë²ˆí˜¸', 'ë³€ê²½ ì „']
                if any(ik in key for ik in important_keys):
                    kvs[key] = value
        
        return kvs
    
    def _calculate_quality(
        self,
        content: str,
        hints: Dict,
        validation: Dict
    ) -> float:
        """
        í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        
        GPT ì œì•ˆ: ê²€ì¦ ì ìˆ˜ í†µí•©
        """
        base_score = 100.0
        
        # ê¸¸ì´ ì²´í¬
        if len(content) < 100:
            base_score -= 40
        elif len(content) < 500:
            base_score -= 20
        
        # ê²€ì¦ ì ìˆ˜ ë°˜ì˜
        if validation['scores']:
            avg_validation = sum(validation['scores'].values()) / len(validation['scores'])
            base_score = (base_score + avg_validation) / 2
        
        # ê²½ê³  í˜ë„í‹°
        warning_penalty = len(validation.get('warnings', [])) * 5
        base_score -= warning_penalty
        
        return max(0.0, min(100.0, base_score))
    
    def _infer_doc_type(self, hints: Dict) -> str:
        """íŒíŠ¸ë¡œ ë¬¸ì„œ íƒ€ì… ì¶”ë¡ """
        if hints['has_map'] and hints['diagram_count'] > 0:
            return 'diagram'
        elif hints['has_table']:
            return 'chart_statistics'
        elif hints['has_text'] and not hints['has_table']:
            return 'text_document'
        else:
            return 'mixed'
    
    def save_kvs_payload(
        self,
        kvs: Dict[str, str],
        doc_id: str,
        page_num: int,
        output_dir: Path
    ) -> Optional[Path]:
        """
        KVS ë³„ë„ í˜ì´ë¡œë“œ ì €ì¥ (GPT ì œì•ˆ #3)
        
        ëª©ì : RAG í•„ë“œ ê²€ìƒ‰ìš© JSON íŒŒì¼ ìƒì„±
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if not kvs:
            return None
        
        payload = {
            'doc_id': doc_id,
            'page': page_num,
            'chunk_id': f'{doc_id}_p{page_num}_kvs',
            'type': 'kvs',
            'kvs': kvs,
            'rank_hint': 3  # GPT ì œì•ˆ: í•„ë“œ ê°€ì¤‘ì¹˜
        }
        
        output_path = output_dir / f'{doc_id}_p{page_num}_kvs.json'
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   ğŸ’¾ KVS í˜ì´ë¡œë“œ ì €ì¥: {output_path}")
        return output_path
