"""
PRISM Phase 2.6 - Pipeline with 2-Pass Extractor + Multi-LLM Support

ê°œì„ ì‚¬í•­:
1. 2-Pass ì „ëµ (Layout â†’ Element)
2. Multi-LLM ì§€ì› (Claude, Azure OpenAI, Ollama)
3. íƒ€ì´í‹€/í˜ì´ì§€ë²ˆí˜¸ ì¶”ì¶œ
4. ì½ê¸° ìˆœì„œ ë³´ì¡´

Author: ì´ì„œì˜ (Backend Lead) + ë°•ì¤€í˜¸ (AI/ML Lead)
Date: 2025-10-17
"""

import os
import time
from pathlib import Path
from typing import List, Dict, Optional
from PIL import Image
import fitz  # PyMuPDF

from models.layout_detector import LayoutDetector
from core.claude_2pass_extractor import Claude2PassExtractor, PageContent
from core.intelligent_chunker import IntelligentChunker
from core.document_analyzer import DocumentAnalyzer


class Phase26Pipeline:
    """
    PRISM Phase 2.6 íŒŒì´í”„ë¼ì¸
    
    Features:
    - 2-Pass ì¶”ì¶œ ì „ëµ
    - Multi-LLM ì§€ì›
    - ì™„ë²½í•œ ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ
    """
    
    def __init__(
        self,
        llm_provider: str = "claude",  # "claude", "azure", "ollama"
        azure_endpoint: Optional[str] = None,
        azure_api_key: Optional[str] = None,
        ollama_base_url: Optional[str] = None,
        chunk_size: int = 512,
        chunk_overlap: int = 50
    ):
        """
        Args:
            llm_provider: LLM ì œê³µì ("claude", "azure", "ollama")
            azure_endpoint: Azure OpenAI ì—”ë“œí¬ì¸íŠ¸
            azure_api_key: Azure OpenAI API í‚¤
            ollama_base_url: Ollama ì„œë²„ URL
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
        """
        print(f"Initializing PRISM Phase 2.6 Pipeline (2-Pass Strategy)...")
        print(f"LLM Provider: {llm_provider.upper()}")
        
        self.llm_provider = llm_provider.lower()
        
        # 1. Layout Detector (ì°¸ê³ ìš©)
        self.layout_detector = LayoutDetector()
        
        # 2. LLM Extractor ì„ íƒ
        if self.llm_provider == "claude":
            self.extractor = Claude2PassExtractor()
            if self.extractor.client:
                print("âœ… Claude 2-Pass Extractor enabled")
            else:
                print("âš ï¸  Claude API unavailable")
                self.extractor = None
        
        elif self.llm_provider == "azure":
            # TODO: Azure OpenAI Vision êµ¬í˜„
            print("âš ï¸  Azure OpenAI Vision not yet implemented")
            self.extractor = None
        
        elif self.llm_provider == "ollama":
            # TODO: Ollama Vision êµ¬í˜„
            print("âš ï¸  Ollama Vision not yet implemented")
            self.extractor = None
        
        else:
            print(f"âŒ Unknown LLM provider: {llm_provider}")
            self.extractor = None
        
        # 3. Intelligent Chunker
        self.chunker = IntelligentChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # 4. Document Analyzer
        self.analyzer = DocumentAnalyzer()
        
        print("âœ… Phase 2.6 Pipeline initialized")

    def _convert_page_to_chunks(
        self,
        page_content: PageContent
    ) -> List[Dict]:
        """
        PageContentë¥¼ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Phase 2.6 ê°œì„ :
        - í˜ì´ì§€ íƒ€ì´í‹€ ì¶”ê°€
        - í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
        - ì½ê¸° ìˆœì„œ ë³´ì¡´
        """
        chunks = []
        chunk_counter = 1
        
        page_num = page_content.page_num
        
        # 0. â­ í˜ì´ì§€ íƒ€ì´í‹€ (ìµœìš°ì„ )
        if page_content.page_title:
            chunk_id = f"title_{page_num:03d}_{chunk_counter:03d}"
            chunks.append({
                "chunk_id": chunk_id,
                "type": "title",
                "content": page_content.page_title,
                "page_num": page_num,
                "metadata": {
                    "section_type": "page_title",
                    "confidence": 0.99
                }
            })
            chunk_counter += 1
        
        # 1. í…ìŠ¤íŠ¸ ì²­í¬
        for text_block in page_content.text_blocks:
            chunk_id = f"chunk_{page_num:03d}_{chunk_counter:03d}"
            chunks.append({
                "chunk_id": chunk_id,
                "type": "text",
                "content": text_block.text,
                "page_num": page_num,
                "metadata": {
                    "section_title": "",
                    "section_type": "text",
                    "confidence": text_block.confidence
                }
            })
            chunk_counter += 1
        
        # 2. í‘œ ì²­í¬
        for table in page_content.tables:
            chunk_id = f"table_{page_num:03d}_{chunk_counter:03d}"
            chunks.append({
                "chunk_id": chunk_id,
                "type": "table",
                "content": table.markdown,
                "page_num": page_num,
                "metadata": {
                    "caption": table.caption,
                    "confidence": table.confidence
                }
            })
            chunk_counter += 1
        
        # 3. ì°¨íŠ¸ ì²­í¬
        for chart in page_content.charts:
            chunk_id = f"chart_{page_num:03d}_{chunk_counter:03d}"
            
            content_lines = [
                f"[ì°¨íŠ¸: {chart.title}]",
                f"íƒ€ì…: {chart.type}",
                f"ì„¤ëª…: {chart.description}",
                "ë°ì´í„°:"
            ]
            
            if chart.data_points:
                first_point = chart.data_points[0] if chart.data_points else {}
                if 'category' in first_point and 'values' in first_point:
                    # ê·¸ë£¹ ë°ì´í„°
                    content_lines.append("")
                    for group in chart.data_points:
                        content_lines.append(f"[{group['category']}]")
                        for value in group['values']:
                            unit = value.get('unit', '')
                            content_lines.append(f"  - {value['label']}: {value['value']}{unit}")
                        content_lines.append("")
                else:
                    # ë‹¨ìˆœ ë°ì´í„°
                    for point in chart.data_points:
                        label = point.get('label', '')
                        value = point.get('value', '')
                        unit = point.get('unit', '')
                        content_lines.append(f"  - {label}: {value}{unit}")
            else:
                content_lines.append("  - (ë°ì´í„° ì—†ìŒ)")
            
            content = "\n".join(content_lines)
            
            chunks.append({
                "chunk_id": chunk_id,
                "type": "chart",
                "content": content,
                "page_num": page_num,
                "metadata": {
                    "chart_type": chart.type,
                    "title": chart.title,
                    "description": chart.description,
                    "data_points": chart.data_points,
                    "confidence": chart.confidence
                }
            })
            chunk_counter += 1
        
        # 4. ì´ë¯¸ì§€/ë‹¤ì´ì–´ê·¸ë¨ ì²­í¬
        for figure in page_content.figures:
            chunk_id = f"figure_{page_num:03d}_{chunk_counter:03d}"
            chunks.append({
                "chunk_id": chunk_id,
                "type": "figure",
                "content": f"[ì´ë¯¸ì§€: {figure.type}]\n{figure.description}",
                "page_num": page_num,
                "metadata": {
                    "figure_type": figure.type,
                    "description": figure.description,
                    "confidence": figure.confidence
                }
            })
            chunk_counter += 1
        
        # 5. â­ í˜ì´ì§€ ë²ˆí˜¸ (ìµœí•˜ë‹¨)
        if page_content.page_number:
            chunk_id = f"page_number_{page_num:03d}_{chunk_counter:03d}"
            chunks.append({
                "chunk_id": chunk_id,
                "type": "page_number",
                "content": page_content.page_number,
                "page_num": page_num,
                "metadata": {
                    "section_type": "page_number",
                    "confidence": 0.99
                }
            })
            chunk_counter += 1
        
        return chunks

    def process(
        self,
        pdf_path: str,
        max_pages: Optional[int] = None
    ) -> Dict:
        """
        PDF ë¬¸ì„œ ì²˜ë¦¬ (Phase 2.6 - 2-Pass)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "="*60)
        print("PRISM Phase 2.6 - Document Processing (2-Pass)")
        print("="*60)
        
        if not self.extractor:
            print("âŒ No extractor available")
            return {
                'chunks': [],
                'statistics': {
                    'error': 'No extractor available'
                }
            }
        
        start_time = time.time()
        
        # 1. PDF ì—´ê¸°
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        if max_pages:
            total_pages = min(total_pages, max_pages)
        
        print(f"\nğŸ“„ ë¬¸ì„œ: {Path(pdf_path).name}")
        print(f"ğŸ“Š ì´ í˜ì´ì§€: {total_pages}")
        print(f"ğŸ¤– LLM: {self.llm_provider.upper()}")
        
        all_chunks = []
        stats = {
            'total_pages': total_pages,
            'text_chunks': 0,
            'table_chunks': 0,
            'chart_chunks': 0,
            'figure_chunks': 0,
            'title_chunks': 0,
            'page_number_chunks': 0
        }
        
        # 2. í˜ì´ì§€ë³„ ì²˜ë¦¬
        for page_num in range(total_pages):
            print(f"\n{'='*60}")
            print(f"ğŸ“„ Processing Page {page_num + 1}/{total_pages}")
            print(f"{'='*60}")
            
            # 2.1 í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            page = doc[page_num]
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # 2.2 â­ 2-Pass ì¶”ì¶œ
            page_content = self.extractor.extract(img, page_num + 1)
            
            if page_content:
                # PageContent â†’ ì²­í¬ ë³€í™˜
                page_chunks = self._convert_page_to_chunks(page_content)
                all_chunks.extend(page_chunks)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                for chunk in page_chunks:
                    chunk_type = chunk['type']
                    if chunk_type == 'text':
                        stats['text_chunks'] += 1
                    elif chunk_type == 'table':
                        stats['table_chunks'] += 1
                    elif chunk_type == 'chart':
                        stats['chart_chunks'] += 1
                    elif chunk_type == 'figure':
                        stats['figure_chunks'] += 1
                    elif chunk_type == 'title':
                        stats['title_chunks'] += 1
                    elif chunk_type == 'page_number':
                        stats['page_number_chunks'] += 1
            else:
                print(f"âš ï¸  Page {page_num + 1} extraction failed")
        
        doc.close()
        
        # 3. í†µê³„ ê³„ì‚°
        end_time = time.time()
        stats['processing_time'] = end_time - start_time
        stats['total_chunks'] = len(all_chunks)
        
        # 4. ê²°ê³¼ ë°˜í™˜
        result = {
            'chunks': all_chunks,
            'statistics': stats
        }
        
        print("\n" + "="*60)
        print("âœ… Processing Complete (Phase 2.6)")
        print("="*60)
        print(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {stats['processing_time']:.1f}ì´ˆ")
        print(f"ğŸ“Š ì´ ì²­í¬: {stats['total_chunks']}ê°œ")
        print(f"   - íƒ€ì´í‹€: {stats['title_chunks']}ê°œ")
        print(f"   - í…ìŠ¤íŠ¸: {stats['text_chunks']}ê°œ")
        print(f"   - í‘œ: {stats['table_chunks']}ê°œ")
        print(f"   - ì°¨íŠ¸: {stats['chart_chunks']}ê°œ")
        print(f"   - ì´ë¯¸ì§€: {stats['figure_chunks']}ê°œ")
        print(f"   - í˜ì´ì§€ë²ˆí˜¸: {stats['page_number_chunks']}ê°œ")
        
        return result


# ============================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("PRISM Phase 2.6 - Pipeline Test")
    print("="*60 + "\n")
    
    # Claudeë¡œ í…ŒìŠ¤íŠ¸
    pipeline = Phase26Pipeline(llm_provider="claude")
    
    test_pdf = "input/test_parser_02.pdf"
    
    if Path(test_pdf).exists():
        print(f"âœ… Test PDF found: {test_pdf}")
        result = pipeline.process(test_pdf, max_pages=3)
        
        # ê²°ê³¼ ì €ì¥
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        
        import json
        with open(output_dir / "test_phase26_result.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Results saved to: output/test_phase26_result.json")
    else:
        print(f"âŒ Test PDF not found: {test_pdf}")