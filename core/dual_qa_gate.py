"""
core/dual_qa_gate.py
PRISM Phase 0.4.0 P0-3.1 - Hotfix (SemanticChunkerì™€ íŒ¨í„´ í†µí•©)

âœ… P0-3.1 ê¸´ê¸‰ ìˆ˜ì •:
1. SemanticChunkerì™€ ì™„ì „íˆ ë™ì¼í•œ íŒ¨í„´ ì‚¬ìš©
2. ê³µë°±/íŠ¹ìˆ˜ë¬¸ì í—ˆìš© ê°•í™”
3. ì´ì¤‘ ê²€ì¦ ë¡œì§ ìœ ì§€

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT í”¼ë“œë°± ë°˜ì˜
Date: 2025-11-13
Version: Phase 0.4.0 P0-3.1
"""

import re
import logging
from typing import Dict, Set, List
from pathlib import Path

logger = logging.getLogger(__name__)


def extract_pdf_text_layer(pdf_path: str) -> str:
    """
    PDF í…ìŠ¤íŠ¸ ë ˆì´ì–´ ì¶”ì¶œ (VLM ê±°ì¹˜ì§€ ì•ŠìŒ)
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ìˆœìˆ˜ í…ìŠ¤íŠ¸
    """
    try:
        import pypdfium2 as pdfium
        
        pdf = pdfium.PdfDocument(pdf_path)
        text_parts = []
        
        for page_num in range(len(pdf)):
            page = pdf[page_num]
            textpage = page.get_textpage()
            text = textpage.get_text_range()
            text_parts.append(text)
        
        full_text = '\n'.join(text_parts)
        logger.info(f"   ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(full_text)}ì")
        
        return full_text
    
    except Exception as e:
        logger.error(f"   âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""


class DualQAGate:
    """
    PDF ì›ë³¸ vs VLM ê²°ê³¼ ì´ì¤‘ ê²€ì¦
    
    âœ… P0-3.1: SemanticChunkerì™€ ì™„ì „íˆ ë™ì¼í•œ íŒ¨í„´ ì‚¬ìš©
    """
    
    # ============================================
    # âœ… P0-3.1: SemanticChunkerì™€ ì™„ì „íˆ ë™ì¼í•œ íŒ¨í„´
    # ============================================
    NUM = r'\d+(?:ì˜\d+)?'
    
    # Strict: ì œNì¡°( í˜•ì‹
    # âœ… SemanticChunkerì™€ ë™ì¼: ì•ì— ê³µë°±/íŠ¹ìˆ˜ë¬¸ì í—ˆìš©
    ARTICLE_STRICT = re.compile(
        rf'^[\sâŸ¨<\[]*(ì œ\s*{NUM}\s*ì¡°)\s*\(',
        re.MULTILINE
    )
    
    # Loose: ì œNì¡° ë‹¨ë…
    # âœ… SemanticChunkerì™€ ë™ì¼: ì•ì— ê³µë°±/íŠ¹ìˆ˜ë¬¸ì í—ˆìš©
    ARTICLE_LOOSE = re.compile(
        rf'^[\sâŸ¨<\[]*(ì œ\s*{NUM}\s*ì¡°)(?=\s|$)',
        re.MULTILINE
    )
    
    def __init__(self):
        logger.info("âœ… DualQAGate Phase 0.4.0 P0-3.1 ì´ˆê¸°í™” (Hotfix)")
        logger.info("   ğŸ”¬ PDF vs VLM ì´ì¤‘ ê²€ì¦ (SemanticChunker íŒ¨í„´ í†µí•©)")
    
    def validate(self, pdf_text: str, vlm_markdown: str) -> Dict:
        """
        PDF ì›ë³¸ vs VLM ê²°ê³¼ ê²€ì¦
        
        Args:
            pdf_text: pypdfium2ë¡œ ì¶”ì¶œí•œ ìˆœìˆ˜ PDF í…ìŠ¤íŠ¸
            vlm_markdown: VLMì´ ìƒì„±í•œ ìµœì¢… Markdown
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("ğŸ”¬ DualQA ê²€ì¦ ì‹œì‘")
        
        # 1. PDF í…ìŠ¤íŠ¸ì—ì„œ ì¡°ë¬¸ í—¤ë” ì¶”ì¶œ
        pdf_articles = self._extract_article_headers(pdf_text, source="PDF")
        
        # 2. VLM Markdownì—ì„œ ì¡°ë¬¸ í—¤ë” ì¶”ì¶œ
        vlm_articles = self._extract_article_headers(vlm_markdown, source="VLM")
        
        # 3. ì°¨ì´ ë¶„ì„
        missing_in_vlm = pdf_articles - vlm_articles  # PDFì—ëŠ” ìˆëŠ”ë° VLMì— ì—†ìŒ
        extra_in_vlm = vlm_articles - pdf_articles    # VLMì—ëŠ” ìˆëŠ”ë° PDFì— ì—†ìŒ
        matched = pdf_articles & vlm_articles         # ì¼ì¹˜
        
        # 4. ë§¤ì¹­ë¥  ê³„ì‚°
        if len(pdf_articles) == 0:
            match_rate = 1.0 if len(vlm_articles) == 0 else 0.0
        else:
            match_rate = len(matched) / len(pdf_articles)
        
        # 5. ê²°ê³¼ ì •ë¦¬
        result = {
            'pdf_count': len(pdf_articles),
            'vlm_count': len(vlm_articles),
            'matched_count': len(matched),
            'missing_in_vlm': sorted(missing_in_vlm),
            'extra_in_vlm': sorted(extra_in_vlm),
            'match_rate': match_rate,
            'qa_flags': []
        }
        
        # 6. QA í”Œë˜ê·¸ ìƒì„±
        if match_rate < 0.95:
            result['qa_flags'].append('article_mismatch')
        
        if missing_in_vlm:
            result['qa_flags'].append('vlm_missing_articles')
        
        if extra_in_vlm:
            result['qa_flags'].append('vlm_extra_articles')
        
        # 7. ë¡œê·¸ ì¶œë ¥
        logger.info("âœ… DualQA ê²€ì¦ ì™„ë£Œ:")
        logger.info(f"   ğŸ“Š PDF ì¡°ë¬¸: {len(pdf_articles)}ê°œ")
        logger.info(f"   ğŸ“Š VLM ì¡°ë¬¸: {len(vlm_articles)}ê°œ")
        logger.info(f"   ğŸ“Š ì¼ì¹˜: {len(matched)}ê°œ")
        logger.info(f"   ğŸ“Š ë§¤ì¹­ë¥ : {match_rate:.1%}")
        
        if missing_in_vlm:
            logger.error(f"   âŒ VLM ëˆ„ë½: {sorted(missing_in_vlm)}")
            logger.error(f"      â†’ PDFì—ëŠ” ìˆì§€ë§Œ VLMì´ ì¶”ì¶œí•˜ì§€ ëª»í•œ ì¡°ë¬¸ì…ë‹ˆë‹¤!")
        
        if extra_in_vlm:
            logger.warning(f"   âš ï¸ VLM ì¶”ê°€: {sorted(extra_in_vlm)}")
            logger.warning(f"      â†’ VLMì´ ë§Œë“¤ì–´ë‚¸ ì¡°ë¬¸ì…ë‹ˆë‹¤ (PDF ì›ë³¸ì— ì—†ìŒ)")
        
        if result['qa_flags']:
            logger.error(f"   ğŸš¨ QA í”Œë˜ê·¸: {result['qa_flags']}")
            logger.error(f"      â†’ ì›ë¬¸ ë¶ˆì¼ì¹˜! ìˆ˜ë™ ê²€ìˆ˜ í•„ìš”í•©ë‹ˆë‹¤!")
        else:
            logger.info("   âœ… ì›ë¬¸ ì¼ì¹˜ (QA í†µê³¼)")
        
        return result
    
    def _extract_article_headers(self, text: str, source: str = "") -> Set[str]:
        """
        ì¡°ë¬¸ í—¤ë” ì¶”ì¶œ (SemanticChunkerì™€ ì™„ì „íˆ ë™ì¼í•œ ë¡œì§)
        
        Args:
            text: í…ìŠ¤íŠ¸
            source: ì†ŒìŠ¤ëª… (ë¡œê¹…ìš©)
        
        Returns:
            ì¡°ë¬¸ í—¤ë” ì§‘í•© (ì˜ˆ: {'ì œ1ì¡°', 'ì œ2ì¡°', ...})
        """
        headers = set()
        
        # 1. Strict íŒ¨í„´ (ì œNì¡°( í˜•ì‹)
        for m in self.ARTICLE_STRICT.finditer(text):
            matched = m.group(1).strip()
            # ê³µë°± ì •ê·œí™” (ì œ 1 ì¡° â†’ ì œ1ì¡°)
            matched = re.sub(r'\s+', '', matched)
            headers.add(matched)
        
        # 2. Loose íŒ¨í„´ (ì œNì¡° ë‹¨ë…)
        for m in self.ARTICLE_LOOSE.finditer(text):
            matched = m.group(1).strip()
            # ê³µë°± ì •ê·œí™”
            matched = re.sub(r'\s+', '', matched)
            
            # âœ… ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§ (SemanticChunkerì™€ ë™ì¼)
            pos = m.start()
            
            # íŒ¨í„´ 1: "ì œNì¡°ì œMí•­" (ì¡°ë¬¸ ì°¸ì¡°)
            context_start = max(0, pos - 20)
            context_end = min(len(text), pos + len(matched) + 20)
            context = text[context_start:context_end]
            
            if re.search(r'ì œ\d+ì¡°ì œ\d+[í•­í˜¸]', context):
                continue  # ì¸ë¼ì¸ ì°¸ì¡° ì œì™¸
            
            # íŒ¨í„´ 2: "ì œNì¡° ë° ì œMì¡°" (ë‚˜ì—´)
            if re.search(r'ì œ\d+ì¡°\s*[ë°ê³¼]\s*ì œ\d+ì¡°', context):
                continue  # ë‚˜ì—´ ì œì™¸
            
            # íŒ¨í„´ 3: ë¬¸ì¥ ì¤‘ê°„ (ì•ì— í•œê¸€ì´ ë°”ë¡œ ë¶™ìŒ)
            if pos > 0 and re.match(r'[ê°€-í£]', text[pos-1]):
                continue  # ë¬¸ì¥ ì¤‘ê°„ ì œì™¸
            
            headers.add(matched)
        
        # 3. ë¡œê·¸ ì¶œë ¥
        headers_list = sorted(headers)
        logger.info(f"   ğŸ“– {source} ì¡°ë¬¸ í—¤ë”: {len(headers_list)}ê°œ")
        if headers_list:
            logger.info(f"      ìƒ˜í”Œ: {headers_list[:5]}")
        
        return headers