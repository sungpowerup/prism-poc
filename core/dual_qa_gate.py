"""
core/dual_qa_gate.py
PRISM Phase 0.4.0 P0-3b - Dual QA Gate

âœ… GPT í”¼ë“œë°± ë°˜ì˜:
1. PDF ì›ë³¸ vs VLM ê²°ê³¼ ì´ì¤‘ ê²€ì¦
2. VLMì„ ê±°ì¹˜ì§€ ì•Šì€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
3. ê´€ì°° ëª¨ë“œ (í•˜ë“œ fail ê¸ˆì§€)
4. ê²½ê³  + ë©”íƒ€ë°ì´í„° í”Œë˜ê·¸ë§Œ

Author: ì´ì„œì˜ (Backend Lead) + GPT ë³´ì •
Date: 2025-11-13
Version: Phase 0.4.0 P0-3b
"""

import re
import logging
from typing import Dict, Set, List
from pathlib import Path

logger = logging.getLogger(__name__)


class DualQAGate:
    """
    PDF ì›ë³¸ vs VLM ê²°ê³¼ ì´ì¤‘ ê²€ì¦
    
    âœ… GPT í•µì‹¬:
    - VLMì„ ì§„ì‹¤ë¡œ ê°€ì •í•˜ì§€ ì•ŠìŒ
    - PDF í…ìŠ¤íŠ¸ ë ˆì´ì–´ì™€ ì§ì ‘ ë¹„êµ
    - ë¶ˆì¼ì¹˜ëŠ” ê²½ê³ ë§Œ (í•˜ë“œ fail ê¸ˆì§€)
    """
    
    # ============================================
    # ì¡°ë¬¸ í—¤ë” íŒ¨í„´ (semantic_chunkerì™€ ë™ì¼)
    # ============================================
    NUM = r'\d+(?:ì˜\d+)?'
    AFTER_JO_NOT_NUM = r'(?!\s*ì œ?\s*\d)'
    
    # Strict: ì œNì¡°( í˜•ì‹
    ARTICLE_STRICT = re.compile(
        rf'(ì œ\s*{NUM}\s*ì¡°){AFTER_JO_NOT_NUM}(?=\s*\()',
        re.MULTILINE
    )
    
    # Loose: ì œNì¡° ë‹¨ë…
    ARTICLE_LOOSE = re.compile(
        rf'(ì œ\s*{NUM}\s*ì¡°){AFTER_JO_NOT_NUM}(?=\s|$)',
        re.MULTILINE
    )
    
    def __init__(self):
        logger.info("âœ… DualQAGate Phase 0.4.0 P0-3b ì´ˆê¸°í™”")
        logger.info("   ğŸ”¬ PDF vs VLM ì´ì¤‘ ê²€ì¦ (ê´€ì°° ëª¨ë“œ)")
    
    def validate(self, pdf_text: str, vlm_markdown: str) -> Dict:
        """
        PDF ì›ë³¸ vs VLM ê²°ê³¼ ê²€ì¦
        
        Args:
            pdf_text: pypdfium2ë¡œ ì¶”ì¶œí•œ ìˆœìˆ˜ PDF í…ìŠ¤íŠ¸
            vlm_markdown: VLMì´ ìƒì„±í•œ ìµœì¢… Markdown
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("ğŸ”¬ DualQA ê²€ì¦ ì‹œì‘")
        
        # 1. PDF í…ìŠ¤íŠ¸ì—ì„œ ì¡°ë¬¸ í—¤ë” ì¶”ì¶œ
        pdf_articles = self._extract_article_headers(pdf_text, source="PDF")
        
        # 2. VLM Markdownì—ì„œ ì¡°ë¬¸ í—¤ë” ì¶”ì¶œ
        vlm_articles = self._extract_article_headers(vlm_markdown, source="VLM")
        
        # 3. ì°¨ì´ ë¶„ì„
        missing_in_vlm = pdf_articles - vlm_articles  # PDFì—ëŠ” ìˆëŠ”ë° VLMì— ì—†ìŒ
        extra_in_vlm = vlm_articles - pdf_articles    # VLMì—ëŠ” ìˆëŠ”ë° PDFì— ì—†ìŒ
        matched = pdf_articles & vlm_articles         # ì¼ì¹˜
        
        # 4. ë§¤ì¹­ë¥  ê³„ì‚°
        if len(pdf_articles) == 0:
            match_rate = 1.0 if len(vlm_articles) == 0 else 0.0
        else:
            match_rate = len(matched) / len(pdf_articles)
        
        # 5. ê²°ê³¼ ì •ë¦¬
        result = {
            'pdf_count': len(pdf_articles),
            'vlm_count': len(vlm_articles),
            'matched_count': len(matched),
            'missing_in_vlm': sorted(missing_in_vlm),
            'extra_in_vlm': sorted(extra_in_vlm),
            'match_rate': match_rate,
            'qa_flags': []
        }
        
        # 6. QA í”Œë˜ê·¸ ìƒì„±
        if match_rate < 0.95:
            result['qa_flags'].append('article_mismatch')
        
        if missing_in_vlm:
            result['qa_flags'].append('vlm_missing_articles')
        
        if extra_in_vlm:
            result['qa_flags'].append('vlm_extra_articles')
        
        # 7. ë¡œê¹…
        self._log_result(result)
        
        return result
    
    def _extract_article_headers(self, text: str, source: str = "TEXT") -> Set[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì¡°ë¬¸ í—¤ë” ì¶”ì¶œ
        
        âœ… GPT í•µì‹¬: ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§
        """
        headers = set()
        
        # Strict íŒ¨í„´ìœ¼ë¡œ ì¶”ì¶œ
        for m in self.ARTICLE_STRICT.finditer(text):
            header = m.group(1).strip()
            header = re.sub(r'\s+', '', header)  # ê³µë°± ì œê±°
            headers.add(header)
        
        # Loose íŒ¨í„´ìœ¼ë¡œ ë³´ê°• (ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§)
        loose_candidates = []
        for m in self.ARTICLE_LOOSE.finditer(text):
            pos = m.start()
            header = m.group(1).strip()
            header = re.sub(r'\s+', '', header)
            
            if header not in headers:
                loose_candidates.append((pos, header))
        
        # ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§
        loose_candidates = self._filter_inline_references(text, loose_candidates)
        for _, header in loose_candidates:
            headers.add(header)
        
        logger.info(f"   ğŸ“– {source} ì¡°ë¬¸ í—¤ë”: {len(headers)}ê°œ")
        if headers:
            sample = sorted(headers)[:5]
            logger.info(f"      ìƒ˜í”Œ: {sample}")
        
        return headers
    
    def _filter_inline_references(self, text: str, candidates: List[tuple]) -> List[tuple]:
        """
        ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§
        
        ì œ28ì¡°ì— ë”°ë¥¸, ì œ73ì¡°ì œ1í•­ ê°™ì€ ì°¸ì¡° ì œê±°
        """
        filtered = []
        
        for pos, matched in candidates:
            # ì „í›„ ì»¨í…ìŠ¤íŠ¸
            start = max(0, pos - 50)
            end = min(len(text), pos + 100)
            context = text[start:end]
            
            # ì¸ë¼ì¸ ì°¸ì¡° íŒ¨í„´
            inline_patterns = [
                rf'{re.escape(matched)}\s*ì œ\s*\d+í•­',      # ì œ73ì¡°ì œ1í•­
                rf'{re.escape(matched)}\s*ì—\s*ë”°ë¥¸',       # ì œ34ì¡°ì— ë”°ë¥¸
                rf'{re.escape(matched)}\s*ë°',              # ì œ41ì¡° ë°
                rf'{re.escape(matched)}\s*ë˜ëŠ”',            # ì œ28ì¡° ë˜ëŠ”
                rf'{re.escape(matched)}\s*ì˜\s*ê·œì •',       # ì œ35ì¡°ì˜ ê·œì •
                rf'{re.escape(matched)}\s*ê³¼',              # ì œ28ì¡°ê³¼
            ]
            
            is_inline = any(re.search(p, context) for p in inline_patterns)
            
            if not is_inline:
                filtered.append((pos, matched))
        
        return filtered
    
    def _log_result(self, result: Dict) -> None:
        """
        ê²€ì¦ ê²°ê³¼ ë¡œê¹…
        
        âœ… GPT í•µì‹¬: ê´€ì°° ëª¨ë“œ (ERROR ë ˆë²¨ì´ì§€ë§Œ ì¤‘ë‹¨ ì—†ìŒ)
        """
        logger.info("âœ… DualQA ê²€ì¦ ì™„ë£Œ:")
        logger.info(f"   ğŸ“Š PDF ì¡°ë¬¸: {result['pdf_count']}ê°œ")
        logger.info(f"   ğŸ“Š VLM ì¡°ë¬¸: {result['vlm_count']}ê°œ")
        logger.info(f"   ğŸ“Š ì¼ì¹˜: {result['matched_count']}ê°œ")
        logger.info(f"   ğŸ“Š ë§¤ì¹­ë¥ : {result['match_rate']:.1%}")
        
        if result['missing_in_vlm']:
            logger.error(f"   âŒ VLM ëˆ„ë½: {result['missing_in_vlm']}")
            logger.error(f"      â†’ PDFì—ëŠ” ìˆì§€ë§Œ VLMì´ ì¶”ì¶œí•˜ì§€ ëª»í•œ ì¡°ë¬¸ì…ë‹ˆë‹¤!")
        
        if result['extra_in_vlm']:
            logger.warning(f"   âš ï¸ VLM ì¶”ê°€: {result['extra_in_vlm']}")
            logger.warning(f"      â†’ VLMì´ ë§Œë“¤ì–´ë‚¸ ì¡°ë¬¸ì…ë‹ˆë‹¤ (PDF ì›ë³¸ì— ì—†ìŒ)")
        
        if result['qa_flags']:
            logger.error(f"   ğŸš¨ QA í”Œë˜ê·¸: {result['qa_flags']}")
            logger.error(f"      â†’ ì›ë¬¸ ë¶ˆì¼ì¹˜! ìˆ˜ë™ ê²€ìˆ˜ í•„ìš”í•©ë‹ˆë‹¤!")
        else:
            logger.info(f"   âœ… QA í”Œë˜ê·¸: ì—†ìŒ (ì›ë³¸ê³¼ ì¼ì¹˜)")


# ============================================
# ìœ í‹¸ë¦¬í‹°: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
# ============================================

def extract_pdf_text_layer(pdf_path: str) -> str:
    """
    pypdfium2ë¡œ PDF í…ìŠ¤íŠ¸ ë ˆì´ì–´ ì¶”ì¶œ
    
    âœ… GPT í•µì‹¬: VLMì„ ê±°ì¹˜ì§€ ì•Šì€ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    """
    try:
        import pypdfium2 as pdfium
    except ImportError:
        logger.error("âŒ pypdfium2 ì—†ìŒ - DualQA ë¶ˆê°€")
        return ""
    
    pdf_path = Path(pdf_path)
    if not pdf_path.exists():
        logger.error(f"âŒ PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")
        return ""
    
    try:
        pdf = pdfium.PdfDocument(str(pdf_path))
        all_text = []
        
        for page_num in range(len(pdf)):
            page = pdf[page_num]
            textpage = page.get_textpage()
            text = textpage.get_text_range()
            all_text.append(text)
        
        combined = '\n'.join(all_text)
        logger.info(f"   ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(combined)}ì")
        
        return combined
        
    except Exception as e:
        logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""
