"""
core/document_classifier.py
PRISM Phase 0.9.8.4 - ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ê¸° ê°œì„ 

GPT ë¯¸ì†¡ë‹˜ ì„¤ê³„ ê¸°ë°˜
ëª©í‘œ: íƒ€ì… ë¶„ë¥˜ ì •í™•ë„ 25% â†’ 80% ì´ìƒ

ê°œì„  ì‚¬í•­:
1. ì„œì‹(form) ìš°ì„  ê°ì§€
2. ì´ë¯¸ì§€ ì¤‘ì‹¬ ë¬¸ì„œ ê°ì§€ (text density < 300)
3. í†µê³„ ë¬¸ì„œ ìˆ«ì ë°€ë„ threshold ì™„í™” (15% â†’ 10%)
4. í‘œ êµ¬ì¡° + ì§§ì€ ì¤„ ë¹„ìœ¨ ì¡°í•© ë¶„ë¥˜
5. ì¡°ë¬¸ ì˜¤ê°ì§€ ë°©ì§€ ("ì œNì¡°"ë³´ë‹¤ "ì„œì‹" ìš°ì„ )

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT ë¯¸ì†¡ë‹˜
Date: 2025-11-28
Version: Phase 0.9.8.4
"""

import re
import logging
from typing import Dict, Any, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)


class DocumentClassifier:
    """
    Phase 0.9.8.4 ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ê¸°
    
    GPT ë¯¸ì†¡ë‹˜ ê°œì„ ì•ˆ:
    - ìš°ì„  ìˆœìœ„ ì¡°ì • (ì„œì‹ â†’ ë²•ë ¹ â†’ ì´ë¯¸ì§€ â†’ í†µê³„)
    - í…ìŠ¤íŠ¸ ë°€ë„ ê¸°ë°˜ ì´ë¯¸ì§€ ë¬¸ì„œ ê°ì§€
    - í†µê³„ ë¬¸ì„œ ê¸°ì¤€ ì™„í™”
    - ì¡°ë¬¸ ì˜¤ê°ì§€ ë°©ì§€
    """
    
    # íŒ¨í„´ ì •ì˜
    ARTICLE_PATTERN = re.compile(r'ì œ\s*\d+\s*ì¡°')
    ANNEX_PATTERN = re.compile(r'ë³„í‘œ|ë³„ì§€')
    FORM_PATTERN = re.compile(r'ë³„ì§€\s*[ì œ]?\s*\d+\s*í˜¸\s*ì„œì‹')
    
    # Phase 0.9.8.4: ê°œì„ ëœ Threshold
    THRESHOLDS = {
        'text_density_image': 300,      # í˜ì´ì§€ë‹¹ 300ì ë¯¸ë§Œ â†’ image_heavy
        'digit_density_stats': 0.10,    # 10% ì´ìƒ â†’ stats_chart (ê¸°ì¡´ 15% â†’ ì™„í™”)
        'table_score_threshold': 0.50,  # í‘œ ê°ì§€ ê¸°ì¤€
        'short_line_stats': 0.70,       # ì§§ì€ ì¤„ ë¹„ìœ¨ 70% ì´ìƒ
    }
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… DocumentClassifier Phase 0.9.8.4 ì´ˆê¸°í™”")
        logger.info("   ğŸ¯ ëª©í‘œ: íƒ€ì… ë¶„ë¥˜ ì •í™•ë„ 25% â†’ 80% ì´ìƒ")
    
    def classify(
        self,
        text: str,
        page_count: int,
        metadata: Dict[str, Any] = None
    ) -> Tuple[str, float, Dict[str, Any]]:
        """
        âœ… Phase 0.9.8.4: ê°œì„ ëœ ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜
        
        ìš°ì„  ìˆœìœ„:
        1. ì–‘ì‹ ë¬¸ì„œ (ì„œì‹ í‚¤ì›Œë“œ ìš°ì„ )
        2. ë²•ë ¹ ë¬¸ì„œ (ì¡°ë¬¸ + ë³„í‘œ)
        3. ì´ë¯¸ì§€ ì¤‘ì‹¬ (í…ìŠ¤íŠ¸ ë°€ë„ ê·¹ì €)
        4. í†µê³„/ì°¨íŠ¸ (ìˆ«ì ë°€ë„ OR í‘œ êµ¬ì¡°)
        5. ì¼ë°˜ ë¬¸ì„œ
        
        Args:
            text: ë¬¸ì„œ í…ìŠ¤íŠ¸
            page_count: í˜ì´ì§€ ìˆ˜
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (table_score ë“±)
        
        Returns:
            (ë¬¸ì„œíƒ€ì…, ì‹ ë¢°ë„, íŠ¹ì§•dict)
        """
        metadata = metadata or {}
        
        logger.info(f"ğŸ” Phase 0.9.8.4: ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ì‹œì‘")
        logger.info(f"   ğŸ“Š í…ìŠ¤íŠ¸: {len(text):,}ì, í˜ì´ì§€: {page_count}ê°œ")
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self._extract_features(text, page_count, metadata)
        
        # ============================================
        # âœ… Phase 0.9.8.4: ê°œì„ ëœ ìš°ì„  ìˆœìœ„ ë¶„ë¥˜
        # ============================================
        
        # 1ìˆœìœ„: ì–‘ì‹ ë¬¸ì„œ (ì„œì‹ í‚¤ì›Œë“œ ìš°ì„ )
        if self._is_form(features):
            doc_type = 'form'
            confidence = 0.9
            reason = "ì„œì‹ í‚¤ì›Œë“œ ê°ì§€"
            logger.info(f"   ğŸ·ï¸ 1ìˆœìœ„ ë§¤ì¹­: {doc_type} ({confidence:.0%}) - {reason}")
            return doc_type, confidence, features
        
        # 2ìˆœìœ„: ë²•ë ¹ ë¬¸ì„œ (ì¡°ë¬¸ + ë³„í‘œ)
        if self._is_law_annex(features):
            doc_type = 'law_annex'
            confidence = 0.9
            reason = "ì¡°ë¬¸ êµ¬ì¡° + ë³„í‘œ ê°ì§€"
            logger.info(f"   ğŸ·ï¸ 2ìˆœìœ„ ë§¤ì¹­: {doc_type} ({confidence:.0%}) - {reason}")
            return doc_type, confidence, features
        
        # 3ìˆœìœ„: ì´ë¯¸ì§€ ì¤‘ì‹¬ (í…ìŠ¤íŠ¸ ë°€ë„ ê·¹ì €)
        if self._is_image_heavy(features):
            doc_type = 'image_heavy'
            confidence = 0.8
            reason = f"í…ìŠ¤íŠ¸ ë°€ë„ ê·¹ì € ({features['avg_text_per_page']:.0f}ì/í˜ì´ì§€)"
            logger.info(f"   ğŸ·ï¸ 3ìˆœìœ„ ë§¤ì¹­: {doc_type} ({confidence:.0%}) - {reason}")
            return doc_type, confidence, features
        
        # 4ìˆœìœ„: í†µê³„/ì°¨íŠ¸ (ìˆ«ì ë°€ë„ OR í‘œ êµ¬ì¡°)
        if self._is_stats_chart(features):
            doc_type = 'stats_chart'
            confidence = 0.7
            reason = self._get_stats_reason(features)
            logger.info(f"   ğŸ·ï¸ 4ìˆœìœ„ ë§¤ì¹­: {doc_type} ({confidence:.0%}) - {reason}")
            return doc_type, confidence, features
        
        # 5ìˆœìœ„: ì¼ë°˜ ë¬¸ì„œ
        doc_type = 'general'
        confidence = 0.5
        reason = "ê¸°ë³¸ê°’"
        logger.info(f"   ğŸ·ï¸ ê¸°ë³¸ ë¶„ë¥˜: {doc_type} ({confidence:.0%}) - {reason}")
        
        return doc_type, confidence, features
    
    def _extract_features(
        self,
        text: str,
        page_count: int,
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """íŠ¹ì§• ì¶”ì¶œ"""
        
        # ê¸°ë³¸ íŠ¹ì§•
        char_count = len(text)
        avg_text_per_page = char_count / max(page_count, 1)
        
        # íŒ¨í„´ ê°ì§€
        has_articles = bool(self.ARTICLE_PATTERN.search(text))
        has_annex = bool(self.ANNEX_PATTERN.search(text))
        has_form_keyword = bool(self.FORM_PATTERN.search(text))
        
        # ìˆ«ì ë°€ë„ (ì²« 1000ì ê¸°ì¤€)
        sample_text = text[:1000]
        if sample_text:
            digit_density = sum(c.isdigit() for c in sample_text) / len(sample_text)
        else:
            digit_density = 0.0
        
        # ë¼ì¸ ë¶„ì„
        lines = text.split('\n')
        non_empty_lines = [l for l in lines if l.strip()]
        
        if non_empty_lines:
            short_lines = sum(1 for line in non_empty_lines if 0 < len(line.strip()) < 50)
            short_line_ratio = short_lines / len(non_empty_lines)
        else:
            short_line_ratio = 0.0
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´
        table_score = metadata.get('table_score', 0.0)
        
        features = {
            'char_count': char_count,
            'page_count': page_count,
            'avg_text_per_page': avg_text_per_page,
            'has_articles': has_articles,
            'has_annex': has_annex,
            'has_form_keyword': has_form_keyword,
            'digit_density': round(digit_density, 3),
            'short_line_ratio': round(short_line_ratio, 2),
            'table_score': table_score,
        }
        
        logger.debug(f"   ğŸ“ˆ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ:")
        logger.debug(f"      - í…ìŠ¤íŠ¸ ë°€ë„: {avg_text_per_page:.0f}ì/í˜ì´ì§€")
        logger.debug(f"      - ì¡°ë¬¸ êµ¬ì¡°: {has_articles}")
        logger.debug(f"      - ë³„í‘œ/ë³„ì§€: {has_annex}")
        logger.debug(f"      - ì„œì‹ í‚¤ì›Œë“œ: {has_form_keyword}")
        logger.debug(f"      - ìˆ«ì ë°€ë„: {digit_density:.1%}")
        logger.debug(f"      - ì§§ì€ ì¤„ ë¹„ìœ¨: {short_line_ratio:.1%}")
        logger.debug(f"      - Table Score: {table_score:.2f}")
        
        return features
    
    def _is_form(self, features: Dict[str, Any]) -> bool:
        """
        âœ… Phase 0.9.8.4: ì–‘ì‹ ë¬¸ì„œ ê°ì§€
        
        ì¡°ê±´:
        - "ë³„ì§€ Ní˜¸ ì„œì‹" í‚¤ì›Œë“œ ì¡´ì¬
        - ë³„í‘œ/ë³„ì§€ í‚¤ì›Œë“œ ì¡´ì¬
        """
        return features['has_form_keyword'] and features['has_annex']
    
    def _is_law_annex(self, features: Dict[str, Any]) -> bool:
        """
        âœ… Phase 0.9.8.4: ë²•ë ¹ ë¬¸ì„œ ê°ì§€
        
        ì¡°ê±´:
        - ì¡°ë¬¸ êµ¬ì¡° ì¡´ì¬ ("ì œNì¡°")
        - ë³„í‘œ/ë³„ì§€ í‚¤ì›Œë“œ ì¡´ì¬
        - ì„œì‹ í‚¤ì›Œë“œ ì—†ìŒ (ìš°ì„  ìˆœìœ„ ë¶„ë¦¬)
        """
        return (
            features['has_articles'] and 
            features['has_annex'] and 
            not features['has_form_keyword']  # âœ… ì„œì‹ ì œì™¸
        )
    
    def _is_image_heavy(self, features: Dict[str, Any]) -> bool:
        """
        âœ… Phase 0.9.8.4: ì´ë¯¸ì§€ ì¤‘ì‹¬ ë¬¸ì„œ ê°ì§€
        
        ì¡°ê±´:
        - í˜ì´ì§€ë‹¹ í‰ê·  í…ìŠ¤íŠ¸ < 300ì
        """
        return features['avg_text_per_page'] < self.THRESHOLDS['text_density_image']
    
    def _is_stats_chart(self, features: Dict[str, Any]) -> bool:
        """
        âœ… Phase 0.9.8.4: í†µê³„/ì°¨íŠ¸ ë¬¸ì„œ ê°ì§€
        
        ì¡°ê±´ (OR):
        - ìˆ«ì ë°€ë„ >= 10% (ê¸°ì¡´ 15% â†’ ì™„í™”)
        - (í‘œ ê°ì§€ AND ì§§ì€ ì¤„ ë¹„ìœ¨ >= 70%)
        """
        # ì¡°ê±´ 1: ìˆ«ì ë°€ë„
        digit_condition = features['digit_density'] >= self.THRESHOLDS['digit_density_stats']
        
        # ì¡°ê±´ 2: í‘œ êµ¬ì¡° + ì§§ì€ ì¤„
        table_condition = (
            features['table_score'] >= self.THRESHOLDS['table_score_threshold'] and
            features['short_line_ratio'] >= self.THRESHOLDS['short_line_stats']
        )
        
        return digit_condition or table_condition
    
    def _get_stats_reason(self, features: Dict[str, Any]) -> str:
        """í†µê³„/ì°¨íŠ¸ ë¶„ë¥˜ ì´ìœ  ì„¤ëª…"""
        reasons = []
        
        if features['digit_density'] >= self.THRESHOLDS['digit_density_stats']:
            reasons.append(f"ìˆ«ì ë°€ë„ {features['digit_density']:.1%}")
        
        if (features['table_score'] >= self.THRESHOLDS['table_score_threshold'] and
            features['short_line_ratio'] >= self.THRESHOLDS['short_line_stats']):
            reasons.append(f"í‘œ êµ¬ì¡° (score={features['table_score']:.2f}, short_line={features['short_line_ratio']:.0%})")
        
        return " OR ".join(reasons) if reasons else "ê¸°ì¤€ ë¯¸ë‹¬"


# ============================================
# í…ŒìŠ¤íŠ¸/ê²€ì¦ í•¨ìˆ˜
# ============================================

def test_classifier():
    """ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸"""
    import logging
    logging.basicConfig(level=logging.INFO)
    
    classifier = DocumentClassifier()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            'name': 'ì¡°ë¬¸_í‘œ (ë²•ë ¹)',
            'text': 'ì œ1ì¡°(ëª©ì ) ì´ ê·œì •ì€...\në³„í‘œ1 ì„ìš©í•˜ê³ ìí•˜ëŠ”ì¸ì›ìˆ˜\n1 2 3 4 5',
            'page_count': 6,
            'metadata': {'table_score': 0.80},
            'expected': 'law_annex'
        },
        {
            'name': 'í†µê³„í‘œ_ì°¨íŠ¸',
            'text': '2024ë…„ ì •ë³´ê³µê°œ ì ‘ìˆ˜ í˜„í™©\n2,323,664ê±´\nì¤‘ì•™í–‰ì •ê¸°ê´€ 983,045ê±´',
            'page_count': 3,
            'metadata': {'table_score': 0.60},
            'expected': 'stats_chart'
        },
        {
            'name': 'ì´ë¯¸ì§€ ì¤‘ì‹¬',
            'text': 'LOTTE GIMPO NIKE\nSITE MAP',
            'page_count': 6,
            'metadata': {'table_score': 0.30},
            'expected': 'image_heavy'
        },
        {
            'name': 'ì–‘ì‹ (Form)',
            'text': '[ë³„ì§€ ì œ1í˜¸ ì„œì‹]\nì¼ë°˜í˜„í™© ë° ì—°í˜\níšŒì‚¬ëª…:',
            'page_count': 4,
            'metadata': {'table_score': 0.50},
            'expected': 'form'
        },
    ]
    
    print("\n" + "="*60)
    print("DocumentClassifier Phase 0.9.8.4 í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    passed = 0
    for i, test in enumerate(test_cases, 1):
        print(f"\n[{i}] {test['name']}")
        
        doc_type, confidence, features = classifier.classify(
            test['text'],
            test['page_count'],
            test['metadata']
        )
        
        status = "âœ…" if doc_type == test['expected'] else "âŒ"
        print(f"   {status} ê²°ê³¼: {doc_type} (ì‹ ë¢°ë„: {confidence:.0%})")
        print(f"      ì˜ˆìƒ: {test['expected']}")
        
        if doc_type == test['expected']:
            passed += 1
    
    print(f"\n{'='*60}")
    print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}/{len(test_cases)} í†µê³¼ ({passed/len(test_cases):.0%})")
    print(f"{'='*60}\n")
    
    return passed == len(test_cases)


if __name__ == '__main__':
    success = test_classifier()
    exit(0 if success else 1)