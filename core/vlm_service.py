"""
core/vlm_service.py
PRISM Phase 0.2.1 ê¸´ê¸‰ íŒ¨ì¹˜ - Azure OpenAI ì‘ë‹µ íŒŒì‹± ìˆ˜ì •

ğŸš¨ ê¸´ê¸‰ ìˆ˜ì • ì‚¬í•­:
1. Azure OpenAI API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ìˆ˜ì •
   - BEFORE: response.content[0].text (âŒ AttributeError)
   - AFTER: response.choices[0].message.content (âœ… ì •ìƒ)
2. call_with_retryì—ì„œë„ ë™ì¼í•˜ê²Œ ìˆ˜ì •

ì›ì¸: Azure OpenAI SDKì˜ ChatCompletion ê°ì²´ êµ¬ì¡° ì˜¤í•´
ê²°ê³¼: VLM Fallback 100% â†’ ì˜ˆìƒ 0~10%

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + GPT í”¼ë“œë°± ë°˜ì˜
Date: 2025-11-06
Version: Phase 0.2.1 Hotfix
"""

import os
import time
import logging
import base64
import re
from typing import Dict, Any, Optional
from pathlib import Path
from dotenv import load_dotenv

# âœ… .env íŒŒì¼ ë¡œë“œ
load_dotenv()

logger = logging.getLogger(__name__)


class VLMServiceV50:
    """
    Phase 0.2.1 VLM ì„œë¹„ìŠ¤ (íŒŒì‹± ì˜¤ë¥˜ ìˆ˜ì •)
    
    âœ… Phase 0.2.1 ê¸´ê¸‰ íŒ¨ì¹˜:
    - Azure OpenAI ì‘ë‹µ íŒŒì‹± ìˆ˜ì •
    - ì¡°ë¬¸ ë²ˆí˜¸ ê²€ì¦ ìœ ì§€
    - ì¬ì‹œë„ ë¡œì§ ìœ ì§€
    """
    
    # âœ… Phase 0.2: ì¡°ë¬¸ ë²ˆí˜¸ íŒ¨í„´
    ARTICLE_PATTERN = re.compile(r'ì œ\s?(\d+)ì¡°(?:ì˜\s?(\d+))?')
    
    # âœ… Phase 0.2: í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ (ë¸”ë™ë¦¬ìŠ¤íŠ¸)
    PAGE_NUMBER_PATTERN = re.compile(r'\b\d{3,4}-\d{1,2}\b')
    
    # í—ˆìš© ê°€ëŠ¥í•œ ì¡°ë¬¸ ë²ˆí˜¸ ë²”ìœ„
    VALID_ARTICLE_RANGE = (1, 200)
    
    def __init__(self, provider: str = 'azure_openai'):
        """
        ì´ˆê¸°í™”
        
        Args:
            provider: VLM ì œê³µì ('azure_openai', 'openai', 'local_sllm')
        """
        self.provider = provider
        
        if provider == 'azure_openai':
            from openai import AzureOpenAI
            
            # í™˜ê²½ë³€ìˆ˜ ì²´í¬
            api_key = os.getenv("AZURE_OPENAI_API_KEY")
            endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
            deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4.1")
            
            if not api_key:
                raise ValueError("âŒ AZURE_OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            
            if not endpoint:
                raise ValueError("âŒ AZURE_OPENAI_ENDPOINT í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            
            logger.info(f"   ğŸ”‘ API Key: {'*' * 20}{api_key[-4:] if len(api_key) > 4 else '****'}")
            logger.info(f"   ğŸŒ Endpoint: {endpoint}")
            logger.info(f"   ğŸ¤– Deployment: {deployment}")
            
            self.client = AzureOpenAI(
                api_key=api_key,
                api_version="2024-12-01-preview",
                azure_endpoint=endpoint
            )
            self.model = deployment
        
        elif provider == 'openai':
            from openai import OpenAI
            
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.model = "gpt-4o"
        
        elif provider == 'local_sllm':
            import ollama
            self.client = ollama
            self.model = os.getenv("OLLAMA_MODEL", "llama3.2-vision")
        
        else:
            raise ValueError(f"Unknown provider: {provider}")
        
        logger.info(f"âœ… VLM Service Phase 0.2 ì´ˆê¸°í™” ì™„ë£Œ: {provider}")
    
    def call(self, image_data: str, prompt: str) -> str:
        """
        VLM í˜¸ì¶œ (ë‹¨ì¼ ì‹œë„)
        
        Args:
            image_data: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€
            prompt: VLM í”„ë¡¬í”„íŠ¸
        
        Returns:
            ì¶”ì¶œëœ Markdown
        """
        try:
            if self.provider == 'local_sllm':
                response = self.client.chat(
                    model=self.model,
                    messages=[
                        {
                            'role': 'user',
                            'content': prompt,
                            'images': [image_data]
                        }
                    ]
                )
                return response['message']['content'].strip()
            
            else:  # Azure OpenAI or OpenAI
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{image_data}"
                                    }
                                },
                                {"type": "text", "text": prompt}
                            ]
                        }
                    ]
                )
                
                # âœ… Phase 0.2.1 ê¸´ê¸‰ ìˆ˜ì •: ì˜¬ë°”ë¥¸ íŒŒì‹±
                # BEFORE (ì˜¤ë¥˜): return response.content[0].text.strip()
                # AFTER (ì •ìƒ): return response.choices[0].message.content.strip()
                return response.choices[0].message.content.strip()
        
        except Exception as e:
            logger.error(f"âŒ VLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise
    
    def call_with_retry(
        self,
        image_data: str,
        prompt: str,
        page_role: str = "general",
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        âœ… Phase 0.2: VLM ì¬ì‹œë„ + ì¡°ë¬¸ ë²ˆí˜¸ ê²€ì¦
        
        ì „ëµ:
        1. ì¼ë°˜ í˜ì´ì§€: ì¬ì‹œë„ 1íšŒ
        2. ê°œì •ì´ë ¥ í˜ì´ì§€: ì¬ì‹œë„ 2íšŒ (ì¤‘ìš”í•˜ë¯€ë¡œ)
        3. ì¬ì‹œë„ ì‹œ í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™”
        4. ì¡°ë¬¸ ë²ˆí˜¸ ì‚¬í›„ ê²€ì¦
        
        Args:
            image_data: Base64 ì´ë¯¸ì§€
            prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
            page_role: í˜ì´ì§€ ì—­í•  ('general', 'revision_table')
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
        Returns:
            {
                'content': ì¶”ì¶œëœ í…ìŠ¤íŠ¸,
                'fallback': True/False,
                'retry_count': ì¬ì‹œë„ íšŸìˆ˜
            }
        """
        # ê°œì •ì´ë ¥ í˜ì´ì§€ëŠ” ì¬ì‹œë„ 2íšŒ, ì¼ë°˜ í˜ì´ì§€ëŠ” 1íšŒ
        if page_role == "revision_table":
            budget = 2
            logger.info(f"      ğŸ¯ ê°œì •ì´ì—­ í˜ì´ì§€ - ì¬ì‹œë„ ì˜ˆì‚° {budget}íšŒ")
        else:
            budget = 1
        
        for attempt in range(budget + 1):
            try:
                if attempt == 0:
                    # ì²« ì‹œë„: ì›ë³¸ í”„ë¡¬í”„íŠ¸
                    current_prompt = prompt
                else:
                    # ì¬ì‹œë„: í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™”
                    logger.info(f"      ğŸ”„ ì¬ì‹œë„ {attempt}/{budget} - í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™”")
                    current_prompt = self._simplify_prompt(prompt)
                    time.sleep(2)  # Rate limit ëŒ€ë¹„
                
                # VLM í˜¸ì¶œ
                content = self.call(image_data, current_prompt)
                
                # ê¸¸ì´ ê²€ì¦
                if len(content.strip()) < 50:
                    logger.warning(f"      âš ï¸ VLM ì‘ë‹µ ë„ˆë¬´ ì§§ìŒ ({len(content)} ê¸€ì)")
                    if attempt < budget:
                        continue
                    else:
                        logger.error(f"      âŒ VLM ì˜¤ë¥˜: ì‘ë‹µ ê¸¸ì´ ë¶€ì¡±")
                        return {'content': '', 'fallback': True, 'retry_count': attempt}
                
                # âœ… Phase 0.2: ì¡°ë¬¸ ë²ˆí˜¸ ê²€ì¦
                if not self._validate_article_numbers(content):
                    logger.warning(f"      âš ï¸ ì¡°ë¬¸ ë²ˆí˜¸ ê²€ì¦ ì‹¤íŒ¨ (í˜ì´ì§€ ë²ˆí˜¸ ì˜¤ì¸ì‹ ì˜ì‹¬)")
                    if attempt < budget:
                        continue
                
                # ì„±ê³µ
                return {
                    'content': content,
                    'fallback': False,
                    'retry_count': attempt
                }
            
            except Exception as e:
                logger.error(f"      âŒ VLM ì˜¤ë¥˜: {e}")
                if attempt < budget:
                    continue
                else:
                    return {'content': '', 'fallback': True, 'retry_count': attempt}
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return {'content': '', 'fallback': True, 'retry_count': budget}
    
    def _simplify_prompt(self, original: str) -> str:
        """
        ì¬ì‹œë„ìš© í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™”
        
        Args:
            original: ì›ë³¸ í”„ë¡¬í”„íŠ¸
        
        Returns:
            ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸
        """
        return """ì´ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ Markdownìœ¼ë¡œ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.

**ê·œì¹™:**
1. ì›ë³¸ ë‚´ìš© ê·¸ëŒ€ë¡œ ì¶”ì¶œ
2. ì¡°ë¬¸ ë²ˆí˜¸ëŠ” "ì œâ—‹ì¡°" í˜•ì‹ ìœ ì§€
3. í‘œê°€ ìˆìœ¼ë©´ Markdown í‘œ í˜•ì‹ìœ¼ë¡œ
4. ë©”íƒ€ ì„¤ëª… ê¸ˆì§€

ì¶œë ¥ë§Œ ì œê³µí•˜ì„¸ìš”."""
    
    def _validate_article_numbers(self, content: str) -> bool:
        """
        âœ… Phase 0.2: ì¡°ë¬¸ ë²ˆí˜¸ ê²€ì¦
        
        í˜ì´ì§€ ë²ˆí˜¸(402-1, 402-2)ë¥¼ ì¡°ë¬¸ ë²ˆí˜¸ë¡œ ì˜¤ì¸ì‹í–ˆëŠ”ì§€ í™•ì¸
        
        Args:
            content: VLM ì¶œë ¥ í…ìŠ¤íŠ¸
        
        Returns:
            True: ì •ìƒ, False: í˜ì´ì§€ ë²ˆí˜¸ ì˜¤ì¸ì‹ ì˜ì‹¬
        """
        # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ê°ì§€
        page_numbers = self.PAGE_NUMBER_PATTERN.findall(content)
        
        if page_numbers:
            logger.warning(f"      ğŸš¨ í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ê°ì§€: {page_numbers}")
            
            # ì¡°ë¬¸ ë²ˆí˜¸ë„ ìˆëŠ”ì§€ í™•ì¸
            articles = self.ARTICLE_PATTERN.findall(content)
            
            if not articles:
                # ì¡°ë¬¸ ë²ˆí˜¸ëŠ” ì—†ê³  í˜ì´ì§€ ë²ˆí˜¸ë§Œ ìˆìŒ â†’ ì˜¤ì¸ì‹
                logger.error(f"      âŒ ì¡°ë¬¸ ë²ˆí˜¸ ì—†ìŒ - í˜ì´ì§€ ë²ˆí˜¸ ì˜¤ì¸ì‹")
                return False
            
            # ì¡°ë¬¸ ë²ˆí˜¸ì˜ ë²”ìœ„ ê²€ì¦
            for match in articles:
                article_no = int(match[0])
                if article_no < self.VALID_ARTICLE_RANGE[0] or article_no > self.VALID_ARTICLE_RANGE[1]:
                    logger.warning(f"      âš ï¸ ë¹„ì •ìƒ ì¡°ë¬¸ ë²ˆí˜¸: ì œ{article_no}ì¡°")
                    return False
        
        return True