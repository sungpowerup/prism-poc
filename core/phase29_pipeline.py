"""
core/phase29_pipeline.py
PRISM Phase 2.9 - êµ¬ì¡°í™”ëœ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

ê°œì„  ì‚¬í•­ (vs Phase 2.8):
1. âœ… êµ¬ì¡°í™”ëœ VLM í”„ë¡¬í”„íŠ¸ ì ìš©
2. âœ… í•œê¸€ ì¸ì½”ë”© ìë™ ìˆ˜ì •
3. âœ… ì„¹ì…˜ ê¸°ë°˜ ì§€ëŠ¥í˜• ì²­í‚¹
4. âœ… RAG ìµœì í™” ë©”íƒ€ë°ì´í„°
5. âœ… ì°¨íŠ¸ë³„ ë…ë¦½ ì²˜ë¦¬

Author: PRISM ê°œë°œíŒ€ ì „ì›
Date: 2025-10-21
"""

import os
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging

from .pdf_processor import PDFProcessor
from .element_classifier import ElementClassifier
from .vlm_service import VLMService
from .structured_prompts import StructuredPrompts
from .encoding_fixer import EncodingFixer, SmartEncodingFixer
from .structural_chunker import RAGOptimizedChunker

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/phase29.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class Phase29Pipeline:
    """
    Phase 2.9 ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    
    Processing Flow:
    1. PDF â†’ í˜ì´ì§€ ì´ë¯¸ì§€
    2. í˜ì´ì§€ ì´ë¯¸ì§€ â†’ VLM êµ¬ì¡°í™” ë¶„ì„
    3. ì¸ì½”ë”© ìˆ˜ì •
    4. êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹
    5. ë©”íƒ€ë°ì´í„° í’ë¶€í™”
    """
    
    def __init__(self, vlm_provider: str = 'azure_openai'):
        """
        Args:
            vlm_provider: 'azure_openai', 'claude', 'ollama'
        """
        logger.info("="*60)
        logger.info("PRISM Phase 2.9 Pipeline ì´ˆê¸°í™”")
        logger.info("="*60)
        
        # 1. VLM ì„œë¹„ìŠ¤
        self.vlm_service = VLMService(provider=vlm_provider)
        self.vlm_provider = vlm_provider
        logger.info(f"âœ… VLM ì„œë¹„ìŠ¤: {vlm_provider}")
        
        # 2. PDF í”„ë¡œì„¸ì„œ
        try:
            self.pdf_processor = PDFProcessor(vlm_service=self.vlm_service)
        except TypeError:
            self.pdf_processor = PDFProcessor()
        logger.info("âœ… PDF í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”")
        
        # 3. Element ë¶„ë¥˜ê¸°
        self.element_classifier = ElementClassifier(use_vlm=False)
        logger.info("âœ… Element ë¶„ë¥˜ê¸° ì´ˆê¸°í™”")
        
        # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
        self.prompt_builder = StructuredPrompts()
        logger.info("âœ… êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°")
        
        # 5. ì¸ì½”ë”© ìˆ˜ì •ê¸°
        self.encoding_fixer = SmartEncodingFixer()
        logger.info("âœ… ìŠ¤ë§ˆíŠ¸ ì¸ì½”ë”© ìˆ˜ì •ê¸°")
        
        # 6. ì²­í‚¹ ì—”ì§„
        self.chunker = RAGOptimizedChunker(
            min_chunk_size=100,
            max_chunk_size=800,
            overlap=50,
            preserve_structure=True
        )
        logger.info("âœ… RAG ìµœì í™” ì²­í‚¹ ì—”ì§„")
        
        logger.info("="*60)
    
    def process_pdf(
        self,
        pdf_path: str,
        output_dir: str = 'output',
        max_pages: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        PDF ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        logger.info("\n" + "="*60)
        logger.info(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {Path(pdf_path).name}")
        logger.info("="*60)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # Stage 1: PDF â†’ í˜ì´ì§€ ì´ë¯¸ì§€ â†’ VLM ë¶„ì„
        stage1_results = self._stage1_vlm_analysis(pdf_path, max_pages)
        
        # Stage 2: ì¸ì½”ë”© ìˆ˜ì •
        stage2_results = self._stage2_fix_encoding(stage1_results)
        
        # Stage 3: êµ¬ì¡°í™”ëœ ì²­í‚¹
        stage3_results = self._stage3_structural_chunking(stage2_results)
        
        # í†µê³„ ê³„ì‚°
        processing_time = time.time() - start_time
        stats = self._calculate_statistics(stage1_results, stage3_results, processing_time)
        
        # ê²°ê³¼ ì·¨í•©
        result = {
            'metadata': {
                'filename': Path(pdf_path).name,
                'total_pages': len(stage1_results),
                'total_chunks': len(stage3_results),
                'processing_time_sec': round(processing_time, 2),
                'vlm_provider': self.vlm_provider,
                'processed_at': datetime.now().isoformat(),
                'encoding_fixes': self.encoding_fixer.base_fixer.get_stats(),
                'phase': '2.9'
            },
            'stage1_vlm_analysis': [self._summarize_page(p) for p in stage1_results],
            'stage3_chunks': [chunk.to_dict() for chunk in stage3_results]
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(result, output_dir)
        
        logger.info("\n" + "="*60)
        logger.info("âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
        logger.info(f"   í˜ì´ì§€: {result['metadata']['total_pages']}ê°œ")
        logger.info(f"   ì²­í¬: {result['metadata']['total_chunks']}ê°œ")
        logger.info(f"   ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        logger.info(f"   ì¸ì½”ë”© ìˆ˜ì •: {stats['encoding_fixes']}ê±´")
        logger.info("="*60)
        
        return result
    
    def _stage1_vlm_analysis(
        self,
        pdf_path: str,
        max_pages: Optional[int]
    ) -> List[Dict[str, Any]]:
        """
        Stage 1: VLM êµ¬ì¡°í™” ë¶„ì„
        """
        logger.info("\n--- Stage 1: VLM êµ¬ì¡°í™” ë¶„ì„ ---")
        
        # PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
        page_images = self.pdf_processor.pdf_to_images(pdf_path)
        
        if max_pages:
            page_images = page_images[:max_pages]
        
        logger.info(f"í˜ì´ì§€ ìˆ˜: {len(page_images)}")
        
        results = []
        
        for page_num, page_image in enumerate(page_images, start=1):
            logger.info(f"\n[í˜ì´ì§€ {page_num}/{len(page_images)}]")
            
            page_start_time = time.time()
            
            # 1. Element ë¶„ë¥˜ (ì „ì²´ í˜ì´ì§€)
            classification = self.element_classifier.classify_image(page_image)
            element_type = classification['type']
            confidence = classification['confidence']
            
            logger.info(f"  íƒ€ì…: {element_type} (ì‹ ë¢°ë„: {confidence:.2f})")
            
            # 2. êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_builder.build_prompt_with_context(
                element_type=element_type,
                page_number=page_num,
                total_pages=len(page_images),
                detected_regions=1
            )
            
            # 3. VLM ë¶„ì„
            logger.info(f"  VLM ë¶„ì„ ì¤‘... (í”„ë¡¬í”„íŠ¸: {len(prompt)}ì)")
            
            vlm_result = self.vlm_service.analyze_image(
                image=page_image,
                prompt=prompt
            )
            
            caption = vlm_result.get('text', '')
            tokens = vlm_result.get('tokens_used', 0)
            
            page_time = time.time() - page_start_time
            
            logger.info(f"  ì™„ë£Œ: {len(caption)}ì (í† í°: {tokens}, {page_time:.2f}ì´ˆ)")
            
            results.append({
                'page_number': page_num,
                'element_type': element_type,
                'confidence': confidence,
                'caption': caption,
                'tokens_used': tokens,
                'processing_time_sec': page_time
            })
        
        logger.info(f"\nStage 1 ì™„ë£Œ: {len(results)}ê°œ í˜ì´ì§€ ë¶„ì„")
        
        return results
    
    def _stage2_fix_encoding(
        self,
        stage1_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Stage 2: ì¸ì½”ë”© ìˆ˜ì •
        """
        logger.info("\n--- Stage 2: ì¸ì½”ë”© ìˆ˜ì • ---")
        
        fixed_results = []
        fix_count = 0
        
        for page_data in stage1_results:
            caption = page_data['caption']
            
            # ì¸ì½”ë”© ìˆ˜ì •
            fixed_caption, confidence = self.encoding_fixer.fix_with_confidence(caption)
            
            if fixed_caption != caption:
                fix_count += 1
                logger.info(f"í˜ì´ì§€ {page_data['page_number']}: ì¸ì½”ë”© ìˆ˜ì • (ì‹ ë¢°ë„: {confidence:.2%})")
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            fixed_data = page_data.copy()
            fixed_data['caption'] = fixed_caption
            fixed_data['encoding_confidence'] = confidence
            
            fixed_results.append(fixed_data)
        
        logger.info(f"Stage 2 ì™„ë£Œ: {fix_count}ê°œ í˜ì´ì§€ ì¸ì½”ë”© ìˆ˜ì •")
        
        return fixed_results
    
    def _stage3_structural_chunking(
        self,
        stage2_results: List[Dict[str, Any]]
    ) -> List:
        """
        Stage 3: êµ¬ì¡°í™”ëœ ì²­í‚¹
        """
        logger.info("\n--- Stage 3: êµ¬ì¡°í™”ëœ ì²­í‚¹ ---")
        
        all_chunks = []
        
        for page_data in stage2_results:
            page_num = page_data['page_number']
            caption = page_data['caption']
            element_type = page_data['element_type']
            
            # êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹
            chunks = self.chunker.chunk_document(
                content=caption,
                page_number=page_num,
                element_type=element_type
            )
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            for chunk in chunks:
                chunk.metadata['tokens_used'] = page_data['tokens_used']
                chunk.metadata['processing_time_sec'] = page_data['processing_time_sec']
                chunk.metadata['model_used'] = self.vlm_provider
            
            all_chunks.extend(chunks)
            
            logger.info(f"í˜ì´ì§€ {page_num}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        logger.info(f"Stage 3 ì™„ë£Œ: ì´ {len(all_chunks)}ê°œ ì²­í¬")
        
        return all_chunks
    
    def _calculate_statistics(
        self,
        stage1_results: List[Dict],
        chunks: List,
        processing_time: float
    ) -> Dict[str, Any]:
        """í†µê³„ ê³„ì‚°"""
        
        # ì²­í¬ íƒ€ì… ë¶„í¬
        chunk_types = {}
        for chunk in chunks:
            etype = chunk.metadata.get('element_type', 'unknown')
            chunk_types[etype] = chunk_types.get(etype, 0) + 1
        
        # ì¸ì½”ë”© ìˆ˜ì • í†µê³„
        encoding_fixes = self.encoding_fixer.base_fixer.get_stats()
        
        return {
            'total_pages': len(stage1_results),
            'total_chunks': len(chunks),
            'chunk_types': chunk_types,
            'avg_chunks_per_page': len(chunks) / len(stage1_results) if stage1_results else 0,
            'processing_time': processing_time,
            'avg_time_per_page': processing_time / len(stage1_results) if stage1_results else 0,
            'encoding_fixes': encoding_fixes['fixed']
        }
    
    def _summarize_page(self, page_data: Dict) -> Dict:
        """í˜ì´ì§€ ìš”ì•½"""
        return {
            'page_number': page_data['page_number'],
            'element_type': page_data['element_type'],
            'confidence': round(page_data['confidence'], 2),
            'caption_length': len(page_data['caption']),
            'tokens_used': page_data['tokens_used'],
            'processing_time_sec': round(page_data['processing_time_sec'], 2)
        }
    
    def _save_results(self, result: Dict, output_dir: str):
        """
        ê²°ê³¼ ì €ì¥ (JSON + Markdown)
        
        Features:
        - UTF-8 BOMìœ¼ë¡œ ì €ì¥
        - JSONê³¼ MD ë™ì‹œ ìƒì„±
        - íƒ€ì„ìŠ¤íƒ¬í”„ ìë™ ì¶”ê°€
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = Path(result['metadata']['filename']).stem
        
        # JSON ì €ì¥ (UTF-8 BOM)
        json_path = Path(output_dir) / f"{filename}_phase29_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8-sig') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ JSON ì €ì¥: {json_path}")
        
        # Markdown ì €ì¥
        md_path = Path(output_dir) / f"{filename}_phase29_{timestamp}.md"
        md_content = self._generate_markdown(result)
        
        with open(md_path, 'w', encoding='utf-8-sig') as f:
            f.write(md_content)
        
        logger.info(f"ğŸ’¾ Markdown ì €ì¥: {md_path}")
    
    def _generate_markdown(self, result: Dict) -> str:
        """
        Markdown í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
        
        Format:
        - ë¬¸ì„œ ì •ë³´
        - ì²­í¬ë³„ ì„¹ì…˜
        - êµ¬ì¡°í™”ëœ ë ˆì´ì•„ì›ƒ
        """
        lines = []
        
        # í—¤ë”
        lines.append("# PRISM Phase 2.9 - êµ¬ì¡°í™”ëœ ë¬¸ì„œ ì¶”ì¶œ ê²°ê³¼")
        lines.append("")
        lines.append(f"**ìƒì„±ì¼ì‹œ**: {result['metadata']['processed_at'][:19]}")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # ë¬¸ì„œ ì •ë³´
        lines.append("## ğŸ“„ ë¬¸ì„œ ì •ë³´")
        lines.append("")
        lines.append(f"- **íŒŒì¼ëª…**: {result['metadata']['filename']}")
        lines.append(f"- **ì´ í˜ì´ì§€**: {result['metadata']['total_pages']}ê°œ")
        lines.append(f"- **ì´ ì²­í¬**: {result['metadata']['total_chunks']}ê°œ")
        lines.append(f"- **ì²˜ë¦¬ ì‹œê°„**: {result['metadata']['processing_time_sec']:.2f}ì´ˆ")
        lines.append(f"- **VLM í”„ë¡œë°”ì´ë”**: {result['metadata']['vlm_provider']}")
        lines.append(f"- **ì¸ì½”ë”© ìˆ˜ì •**: {result['metadata']['encoding_fixes']['fixed']}ê±´")
        lines.append(f"- **Phase**: {result['metadata']['phase']}")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # ì²­í¬
        lines.append("## ğŸ§© êµ¬ì¡°í™”ëœ ì²­í¬")
        lines.append("")
        
        for i, chunk_dict in enumerate(result['stage3_chunks'], start=1):
            lines.append(f"### ì²­í¬ #{i}")
            lines.append("")
            
            metadata = chunk_dict['metadata']
            
            lines.append(f"**í˜ì´ì§€**: {metadata['page_number']}")
            lines.append(f"**íƒ€ì…**: {metadata['element_type']}")
            lines.append(f"**ëª¨ë¸**: {metadata.get('model_used', 'N/A')}")
            
            # ì„¹ì…˜ ì •ë³´
            if metadata.get('section_title'):
                lines.append(f"**ì„¹ì…˜**: {metadata['section_title']}")
            
            # ì°¨íŠ¸ ì •ë³´
            if metadata.get('chart_type'):
                lines.append(f"**ì°¨íŠ¸ íƒ€ì…**: {metadata['chart_type']}")
            
            # í‚¤ì›Œë“œ
            if metadata.get('keywords'):
                keywords = ', '.join(metadata['keywords'][:5])
                lines.append(f"**í‚¤ì›Œë“œ**: {keywords}")
            
            lines.append("")
            lines.append("**ë‚´ìš©**:")
            lines.append("")
            lines.append("```")
            lines.append(chunk_dict['content'])
            lines.append("```")
            lines.append("")
            lines.append("---")
            lines.append("")
        
        return '\n'.join(lines)


# CLI ì‹¤í–‰
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python phase29_pipeline.py <pdf_path> [output_dir] [vlm_provider]")
        print("ì˜ˆì‹œ: python phase29_pipeline.py input/test.pdf output azure_openai")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else 'output'
    vlm_provider = sys.argv[3] if len(sys.argv) > 3 else 'azure_openai'
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = Phase29Pipeline(vlm_provider=vlm_provider)
    result = pipeline.process_pdf(pdf_path, output_dir)
    
    print("\n" + "="*60)
    print("ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
    print("="*60)
    print(f"í˜ì´ì§€: {result['metadata']['total_pages']}")
    print(f"ì²­í¬: {result['metadata']['total_chunks']}")
    print(f"ì‹œê°„: {result['metadata']['processing_time_sec']:.2f}ì´ˆ")
    print("="*60)