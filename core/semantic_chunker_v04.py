"""
Semantic Chunker V0.4
Phase 0.4.0 "Quality Assurance Release"

Enhanced semantic chunking with mandatory non-article section handling
Creates independent chunks for critical sections (Í∏∞Î≥∏Ï†ïÏã†, Í∞úÏ†ïÏù¥Î†•)

Author: Î∞ïÏ§ÄÌò∏ (AI/ML Lead)
Date: 2025-11-09
"""

from typing import List, Dict, Any
import re
from datetime import datetime

# Version check
from .version import PRISM_VERSION, check_version
VERSION = "0.4.0"
check_version(__name__, VERSION)

class SemanticChunkerV04:
    """
    Enhanced semantic chunker for Phase 0.4
    Handles both article-based and non-article sections
    """
    
    # ============================================
    # Chunking Strategy
    # ============================================
    
    CHUNKING_RULES = """
üìã CHUNKING STRATEGY (Phase 0.4)

üéØ Priority Order:
1. Í∞úÏ†ïÏù¥Î†• (Revision History) ‚Üí Independent chunk
2. Í∏∞Î≥∏Ï†ïÏã† (Basic Principles) ‚Üí Independent chunk
3. Ï†ú1Ï°∞, Ï†ú2Ï°∞, ... (Articles) ‚Üí One chunk per article
4. Î∂ÄÏπô (Supplementary Provisions) ‚Üí Independent chunk

‚ö†Ô∏è CRITICAL: Non-article sections MUST be chunked separately
- Each section gets its own chunk with proper metadata
- Do NOT merge with articles
- Do NOT skip these sections
"""
    
    def __init__(
        self,
        min_chunk_size: int = 300,
        max_chunk_size: int = 2000,
        overlap: int = 100
    ):
        """
        Initialize semantic chunker
        
        Args:
            min_chunk_size: Minimum characters per chunk
            max_chunk_size: Maximum characters per chunk
            overlap: Character overlap between chunks
        """
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.overlap = overlap
        
        # Patterns
        self.article_pattern = re.compile(r'###\s*Ï†ú\s*(\d+)\s*Ï°∞')
        self.chapter_pattern = re.compile(r'##\s*Ï†ú\s*(\d+)\s*Ïû•')
    
    def chunk(self, markdown: str, doc_type: str = "regulation") -> List[Dict[str, Any]]:
        """
        Create semantic chunks from markdown
        
        Args:
            markdown: Preprocessed markdown content
            doc_type: Document type
        
        Returns:
            List of chunk dictionaries
        """
        chunks = []
        
        # 1. Extract revision history (highest priority)
        revision_chunk = self._extract_revision_history(markdown)
        if revision_chunk:
            chunks.append(revision_chunk)
        
        # 2. Extract basic principles
        principles_chunk = self._extract_basic_principles(markdown)
        if principles_chunk:
            chunks.append(principles_chunk)
        
        # 3. Extract articles
        article_chunks = self._extract_articles(markdown)
        chunks.extend(article_chunks)
        
        # 4. Extract supplementary provisions
        supplement_chunk = self._extract_supplementary(markdown)
        if supplement_chunk:
            chunks.append(supplement_chunk)
        
        # 5. Add metadata and finalize
        for i, chunk in enumerate(chunks, 1):
            chunk['id'] = f"chunk_{i:03d}"
            chunk['sequence'] = i
            chunk['total_chunks'] = len(chunks)
        
        return chunks
    
    def _extract_revision_history(self, markdown: str) -> Dict[str, Any] | None:
        """
        Extract revision history as independent chunk
        
        Returns:
            Chunk dict or None if not found
        """
        # Pattern: ## Í∞úÏ†ïÏù¥Î†• or | Í∞úÏ†ïÏùºÏûê |
        patterns = [
            r'##\s*Í∞úÏ†ï\s*Ïù¥Î†•.*?\n(.*?)(?=\n##|\Z)',
            r'\|\s*Í∞úÏ†ïÏùºÏûê\s*\|.*?\n((?:\|.*?\n)+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, markdown, re.DOTALL | re.IGNORECASE)
            if match:
                content = match.group(0).strip()
                
                # Parse table if present
                revision_count = len(re.findall(r'\|\s*\d{4}', content))
                
                return {
                    'content': content,
                    'metadata': {
                        'type': 'revision_history',
                        'section': 'Í∞úÏ†ïÏù¥Î†•',
                        'article_no': None,
                        'char_count': len(content),
                        'revision_count': revision_count
                    }
                }
        
        return None
    
    def _extract_basic_principles(self, markdown: str) -> Dict[str, Any] | None:
        """
        Extract basic principles as independent chunk
        
        Returns:
            Chunk dict or None if not found
        """
        # Pattern: ## Í∏∞Î≥∏Ï†ïÏã† or ## Í∏∞Î≥∏ Ï†ïÏã†
        pattern = r'##\s*Í∏∞Î≥∏\s*Ï†ïÏã†.*?\n(.*?)(?=\n##|\n###|\Z)'
        match = re.search(pattern, markdown, re.DOTALL | re.IGNORECASE)
        
        if match:
            content = match.group(0).strip()
            
            return {
                'content': content,
                'metadata': {
                    'type': 'basic_principles',
                    'section': 'Í∏∞Î≥∏Ï†ïÏã†',
                    'article_no': None,
                    'char_count': len(content)
                }
            }
        
        return None
    
    def _extract_articles(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract articles as individual chunks
        
        Returns:
            List of article chunks
        """
        chunks = []
        
        # Split by article headers
        article_splits = re.split(r'(###\s*Ï†ú\s*\d+\s*Ï°∞.*?\n)', markdown)
        
        current_article = None
        current_content = []
        
        for i, segment in enumerate(article_splits):
            # Check if this is an article header
            article_match = self.article_pattern.match(segment.strip())
            
            if article_match:
                # Save previous article if exists
                if current_article and current_content:
                    chunks.append(self._create_article_chunk(
                        current_article,
                        ''.join(current_content)
                    ))
                
                # Start new article
                current_article = segment.strip()
                current_content = [segment]
            else:
                # Add to current article content
                if current_article:
                    current_content.append(segment)
        
        # Save last article
        if current_article and current_content:
            chunks.append(self._create_article_chunk(
                current_article,
                ''.join(current_content)
            ))
        
        return chunks
    
    def _create_article_chunk(self, header: str, content: str) -> Dict[str, Any]:
        """
        Create chunk from article header and content
        
        Args:
            header: Article header (e.g., "### Ï†ú1Ï°∞(Î™©Ï†Å)")
            content: Full article content
        
        Returns:
            Chunk dictionary
        """
        # Extract article number
        article_match = re.search(r'Ï†ú\s*(\d+)\s*Ï°∞', header)
        article_no = article_match.group(1) if article_match else None
        
        # Extract article title
        title_match = re.search(r'Ï†ú\s*\d+\s*Ï°∞\s*\(([^)]+)\)', header)
        article_title = title_match.group(1) if title_match else None
        
        # Clean content
        content = content.strip()
        
        # Count sub-items
        item_count = len(re.findall(r'^\d+\.', content, re.MULTILINE))
        subitem_count = len(re.findall(r'^\s+[Í∞Ä-Ìû£]\.', content, re.MULTILINE))
        
        return {
            'content': content,
            'metadata': {
                'type': 'article',
                'section': f'Ï†ú{article_no}Ï°∞',
                'article_no': article_no,
                'article_title': article_title,
                'char_count': len(content),
                'item_count': item_count,
                'subitem_count': subitem_count
            }
        }
    
    def _extract_supplementary(self, markdown: str) -> Dict[str, Any] | None:
        """
        Extract supplementary provisions
        
        Returns:
            Chunk dict or None if not found
        """
        pattern = r'##\s*Î∂Ä\s*Ïπô.*?\n(.*?)(?=\n##|\Z)'
        match = re.search(pattern, markdown, re.DOTALL | re.IGNORECASE)
        
        if match:
            content = match.group(0).strip()
            
            return {
                'content': content,
                'metadata': {
                    'type': 'supplementary',
                    'section': 'Î∂ÄÏπô',
                    'article_no': None,
                    'char_count': len(content)
                }
            }
        
        return None
    
    def validate_chunks(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Validate chunking quality
        
        Args:
            chunks: List of chunks
        
        Returns:
            Validation report
        """
        issues = []
        
        # Check for critical chunks
        chunk_types = [c['metadata']['type'] for c in chunks]
        
        if 'basic_principles' not in chunk_types:
            issues.append({
                'severity': 'critical',
                'message': 'Missing "Í∏∞Î≥∏Ï†ïÏã†" chunk'
            })
        
        if 'revision_history' not in chunk_types:
            issues.append({
                'severity': 'major',
                'message': 'Missing "Í∞úÏ†ïÏù¥Î†•" chunk'
            })
        
        # Check chunk sizes
        for chunk in chunks:
            size = chunk['metadata']['char_count']
            if size < self.min_chunk_size:
                issues.append({
                    'severity': 'minor',
                    'message': f"Chunk {chunk.get('id', '?')} too small ({size} chars)"
                })
        
        # Calculate score
        critical_count = len([i for i in issues if i['severity'] == 'critical'])
        major_count = len([i for i in issues if i['severity'] == 'major'])
        
        score = 100 - (critical_count * 30) - (major_count * 10)
        
        return {
            'valid': len(issues) == 0,
            'score': max(0, score),
            'total_chunks': len(chunks),
            'chunk_types': chunk_types,
            'issues': issues
        }


# ============================================
# Usage Example
# ============================================

if __name__ == "__main__":
    sample_markdown = """
## Í∞úÏ†ïÏù¥Î†•

| Í∞úÏ†ïÏùºÏûê | Í∞úÏ†ïÏÇ¨Ïú† | ÎπÑÍ≥† |
|---------|---------|------|
| 2024.01.01 | ÏµúÏ¥à Ï†úÏ†ï | - |

## Í∏∞Î≥∏Ï†ïÏã†

Î™®Îì† ÏßÅÏõêÏùÄ ÌèâÎì±ÌïòÍ≤å ÎåÄÏö∞Î∞õÎäîÎã§.

### Ï†ú1Ï°∞(Î™©Ï†Å)
Ïù¥ Í∑úÏ†ïÏùÄ ÏßÅÏõêÏùò Ïù∏ÏÇ¨Í¥ÄÎ¶¨Ïóê Í¥ÄÌïú ÏÇ¨Ìï≠ÏùÑ Ï†ïÌï®ÏùÑ Î™©Ï†ÅÏúºÎ°ú ÌïúÎã§.

### Ï†ú2Ï°∞(Ï†ÅÏö©Î≤îÏúÑ)
Ïù¥ Í∑úÏ†ïÏùÄ Î™®Îì† ÏßÅÏõêÏóêÍ≤å Ï†ÅÏö©ÌïúÎã§.
"""
    
    chunker = SemanticChunkerV04()
    chunks = chunker.chunk(sample_markdown)
    
    print("=" * 60)
    print(f"Total Chunks: {len(chunks)}")
    print("=" * 60)
    
    for chunk in chunks:
        print(f"\n[{chunk['id']}] {chunk['metadata']['type']}")
        print(f"Section: {chunk['metadata']['section']}")
        print(f"Size: {chunk['metadata']['char_count']} chars")
        print(f"Content: {chunk['content'][:100]}...")
    
    print("\n" + "=" * 60)
    print("Validation Report:")
    print("=" * 60)
    validation = chunker.validate_chunks(chunks)
    print(f"Valid: {validation['valid']}")
    print(f"Score: {validation['score']}/100")
    print(f"Issues: {len(validation['issues'])}")
