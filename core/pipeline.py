"""
core/pipeline_v50.py
PRISM Phase 5.2.0 - Pipeline (ì™„ì „ ë²”ìš© ë¬¸ì„œ ì²˜ë¦¬)

âœ… Phase 5.2.0 í•µì‹¬:
1. ë¬¸ì„œ íƒ€ì… ìë™ ì¸ì‹ + íƒ€ì…ë³„ ì „ëµ ìë™ ì ìš©
2. 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¤€ìˆ˜
3. ì›ë³¸ ì¶©ì‹¤ë„ 95% + ì²­í‚¹ í’ˆì§ˆ 90% + RAG ìµœì í™” 95%

Author: ì´ì„œì˜ (Backend Lead)
Date: 2025-10-24
Version: 5.2.0
"""

import logging
from typing import List, Dict, Any, Optional
import time
import uuid
import re

# Phase 5.2.0: SemanticChunker import
try:
    from .semantic_chunker import SemanticChunker
except ImportError:
    try:
        from semantic_chunker import SemanticChunker
    except ImportError:
        import sys
        import importlib.util
        spec = importlib.util.spec_from_file_location("semantic_chunker", "core/semantic_chunker.py")
        if spec:
            semantic_chunker = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(semantic_chunker)
            SemanticChunker = semantic_chunker.SemanticChunker

logger = logging.getLogger(__name__)




def clean_markdown(markdown: str) -> str:
    """
    RAG ìµœì í™”: ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
    
    Phase 5.2.0 í•µì‹¬:
    - HTML ì£¼ì„ ì œê±°
    - ë©”íƒ€ ì„¤ëª… ì œê±°
    - ì¤‘ë³µ ì •ë³´ ì œê±°
    """
    # HTML ì£¼ì„ ì œê±°
    markdown = re.sub(r'<!--.*?-->', '', markdown, flags=re.DOTALL)
    
    # ë©”íƒ€ ì„¤ëª… ì œê±°
    meta_patterns = [
        r'^ì´ ì´ë¯¸ì§€ëŠ”.*?\.\s*',
        r'^ì•„ë˜ì™€ ê°™ì´.*?\.\s*',
        r'\n\*\*ìš”ì•½:\*\*\n.*?(?=\n#|\Z)',
        r'\n## ìš”ì•½\n.*?(?=\n#|\Z)',
    ]
    for pattern in meta_patterns:
        markdown = re.sub(pattern, '', markdown, flags=re.MULTILINE | re.DOTALL)
    
    # ì½”ë“œ ë¸”ë¡ ì¤‘ì²© ì œê±°
    markdown = re.sub(r'```markdown\s*\n(.*?)\n```', r'\1', markdown, flags=re.DOTALL)
    
    # JSON ì˜ˆì‹œ ì œê±°
    markdown = re.sub(r'```json\s*\n.*?\n```', '', markdown, flags=re.DOTALL)
    markdown = re.sub(r'####?\s*\*\*êµ¬ì¡° ì¶”ì¶œ ì˜ˆì‹œ.*?(?=\n##|\n---|\Z)', '', markdown, flags=re.DOTALL)
    
    # ë¹ˆ ì¤„ ì •ë¦¬
    markdown = re.sub(r'\n{4,}', '\n\n\n', markdown)
    
    return markdown.strip()


class Phase50Pipeline:
    """
    Phase 5.2.0 ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    
    íŠ¹ì§•:
    - ì™„ì „ ë²”ìš© ì„¤ê³„ (ëª¨ë“  ë¬¸ì„œ íƒ€ì… ì§€ì›)
    - 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¤€ìˆ˜
    - í•˜ë“œì½”ë”© ì œë¡œ
    """
    
    def __init__(self, pdf_processor, vlm_service, storage):
        """
        Args:
            pdf_processor: PDFProcessor ì¸ìŠ¤í„´ìŠ¤
            vlm_service: VLMServiceV50 ì¸ìŠ¤í„´ìŠ¤
            storage: Storage ì¸ìŠ¤í„´ìŠ¤
        """
        self.pdf_processor = pdf_processor
        self.vlm_service = vlm_service
        self.storage = storage
        self.semantic_chunker = SemanticChunker(
            min_chunk_size=600,
            max_chunk_size=1200,
            target_chunk_size=900
        )
        logger.info("âœ… Phase 5.2.0: SemanticChunker ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_pdf(
        self,
        pdf_path: str,
        max_pages: int = 20,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        PDF ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ (Phase 5.2.0)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        session_id = str(uuid.uuid4())[:8]
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ PRISM Phase 5.2.0 - ë²”ìš© ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"{'='*80}")
        logger.info(f"ğŸ“„ íŒŒì¼: {pdf_path}")
        logger.info(f"ğŸ†” Session ID: {session_id}")
        logger.info(f"ğŸ“Š ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        logger.info(f"{'='*80}\n")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 1: PDF â†’ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³€í™˜ (300 DPI)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if progress_callback:
            progress_callback("ğŸ“„ PDF ë³€í™˜ ì¤‘ (300 DPI ê³ í•´ìƒë„)...", 0)
        
        logger.info("[Stage 1] PDF â†’ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³€í™˜ (300 DPI)")
        images = self.pdf_processor.pdf_to_images(pdf_path, max_pages=max_pages, dpi=300)
        logger.info(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ\n")
        
        if not images:
            logger.error("âŒ PDF ë³€í™˜ ì‹¤íŒ¨")
            return {
                'status': 'error',
                'error': 'PDF ë³€í™˜ ì‹¤íŒ¨',
                'session_id': session_id
            }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 2: Phase 5.2.0 ë²”ìš© ë¶„ì„ (ë¬¸ì„œ íƒ€ì… ìë™ íŒë³„)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        logger.info("[Stage 2] Phase 5.2.0 ë²”ìš© ë¶„ì„ ì‹œì‘")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        results = []
        success_count = 0
        error_count = 0
        
        doc_type_counts = {}
        
        for page_num, img_data in enumerate(images):
            if progress_callback:
                progress = int(5 + (page_num / len(images)) * 85)
                progress_callback(
                    f"ğŸ¯ í˜ì´ì§€ {page_num + 1}/{len(images)} ë²”ìš© ë¶„ì„ ì¤‘...",
                    progress
                )
            
            logger.info(f"ğŸ“„ í˜ì´ì§€ {page_num + 1}/{len(images)} ì²˜ë¦¬ ì‹œì‘")
            
            try:
                # Phase 5.2.0: ë²”ìš© ë¶„ì„
                vlm_result = self.vlm_service.analyze_page_v50(
                    image_data=img_data,
                    page_num=page_num + 1
                )
                
                content = vlm_result.get('content', '')
                confidence = vlm_result.get('confidence', 0.0)
                doc_type = vlm_result.get('doc_type', 'mixed')
                subtype = vlm_result.get('subtype', 'unknown')
                quality_score = vlm_result.get('quality_score', 0.0)
                
                if not content or len(content) < 20:
                    logger.warning(f"   âš ï¸ VLM ê²°ê³¼ ë¶€ì¡±: {len(content)} ê¸€ì")
                    error_count += 1
                    continue
                
                # ì„±ê³µ!
                success_count += 1
                doc_type_counts[doc_type] = doc_type_counts.get(doc_type, 0) + 1
                
                logger.info(f"   âœ… ì„±ê³µ!")
                logger.info(f"      - íƒ€ì…: {doc_type} ({subtype})")
                logger.info(f"      - ì‹ ë¢°ë„: {confidence:.2f}")
                logger.info(f"      - í’ˆì§ˆ: {quality_score:.1f}/100")
                logger.info(f"      - ê¸€ì ìˆ˜: {len(content):,}")
                logger.info("")
                
                results.append({
                    'page_num': page_num + 1,
                    'content': content,
                    'confidence': confidence,
                    'doc_type': doc_type,
                    'subtype': subtype,
                    'strategy': vlm_result.get('strategy', 'unknown'),
                    'quality_score': quality_score,
                    'structure': vlm_result.get('structure', {})
                })
                
            except Exception as e:
                logger.error(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\n")
                error_count += 1
        
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info(f"âœ… Phase 5.2.0 ë¶„ì„ ì™„ë£Œ:")
        logger.info(f"   - ì„±ê³µ: {success_count}/{len(images)}ê°œ")
        logger.info(f"   - ì‹¤íŒ¨: {error_count}/{len(images)}ê°œ")
        logger.info(f"   - ë¬¸ì„œ íƒ€ì… ë¶„í¬: {doc_type_counts}")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        if success_count == 0:
            logger.error("âŒ ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨")
            return {
                'status': 'error',
                'error': 'ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨',
                'session_id': session_id
            }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 3: Markdown í†µí•© (ì§€ëŠ¥í˜• ì²­í‚¹)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if progress_callback:
            progress_callback("ğŸ“ Markdown í†µí•© ì¤‘...", 95)
        
        logger.info("[Stage 3] Markdown í†µí•© (ì§€ëŠ¥í˜• ì²­í‚¹)")
        
        full_markdown = self._generate_markdown_with_chunking(results)
        
        logger.info(f"âœ… Markdown ìƒì„± ì™„ë£Œ")
        logger.info(f"   - ì´ ê¸€ì ìˆ˜: {len(full_markdown):,}")
        logger.info(f"   - ì„¹ì…˜ ìˆ˜: {full_markdown.count('---')}")
        logger.info("")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 4: 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        logger.info("[Stage 4] 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„")
        quality_metrics = self._analyze_quality_checklist(results, full_markdown)
        
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("ğŸ“Š 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"   âœ… 1. ì›ë³¸ ì¶©ì‹¤ë„:  {quality_metrics['fidelity_score']:.1f}/100")
        logger.info(f"   âœ… 2. ì²­í‚¹ í’ˆì§ˆ:    {quality_metrics['chunking_score']:.1f}/100")
        logger.info(f"   âœ… 3. RAG ì í•©ë„:   {quality_metrics['rag_score']:.1f}/100")
        logger.info(f"   âœ… 4. ë²”ìš©ì„±:       {quality_metrics['universality_score']:.1f}/100")
        logger.info(f"   âœ… 5. ê²½ìŸì‚¬ ëŒ€ë¹„:  {quality_metrics['competitive_score']:.1f}/100")
        logger.info(f"   ğŸ¯ ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {quality_metrics['overall_score']:.1f}/100")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 5: ê²°ê³¼ ì €ì¥
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if progress_callback:
            progress_callback("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...", 98)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        result = {
            'status': 'success',
            'session_id': session_id,
            'version': '5.2.0',
            'processing_time': processing_time,
            'pages_total': len(images),
            'pages_success': success_count,
            'pages_error': error_count,
            'strategy': 'universal_v50',
            'doc_type_counts': doc_type_counts,
            'markdown': full_markdown,
            'page_results': results,
            **quality_metrics
        }
        
        # DB ì €ì¥
        try:
            if hasattr(self.storage, 'save_session'):
                self.storage.save_session(result)
                logger.info("âœ… DB ì €ì¥ ì™„ë£Œ\n")
        except Exception as e:
            logger.error(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨: {e}\n")
        
        if progress_callback:
            progress_callback("âœ… ì™„ë£Œ!", 100)
        
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("ğŸ‰ PRISM Phase 5.2.0 ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
        logger.info(f"   ğŸ“„ ì„±ê³µ: {success_count}/{len(images)}ê°œ")
        logger.info(f"   ğŸ¯ ì¢…í•© í’ˆì§ˆ: {quality_metrics['overall_score']:.1f}/100")
        logger.info(f"   ğŸ“Š ì´ ê¸€ì: {len(full_markdown):,}")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        return result
    
    def _generate_markdown_with_chunking(self, results: List[Dict[str, Any]]) -> str:
        """
        í˜ì´ì§€ë³„ ê²°ê³¼ë¥¼ ì§€ëŠ¥í˜• ì²­í‚¹ìœ¼ë¡œ í†µí•©
        
        âœ… Phase 5.2.0: ì§€ëŠ¥í˜• ì²­í‚¹
        1. í˜ì´ì§€ ê²½ê³„ ë¬´ì‹œí•˜ê³  ì „ì²´ Markdown ìƒì„±
        2. SemanticChunkerë¡œ ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹
        3. ì²­í¬ ë©”íƒ€ë°ì´í„° í¬í•¨
        """
        # Step 1: ì „ì²´ Markdown ìƒì„± (í˜ì´ì§€ êµ¬ë¶„ ì—†ì´)
        markdown_parts = []
        
        for i, result in enumerate(results):
            content = result['content']
            
            # ë‚´ìš© ì •ì œ
            content = clean_markdown(content)
            
            # ë‚´ìš© ì¶”ê°€ (í˜ì´ì§€ êµ¬ë¶„ ì—†ì´)
            if content.strip():
                markdown_parts.append(content)
        
        # Step 2: ì „ì²´ Markdown í†µí•©
        full_markdown = "\n\n".join(markdown_parts)
        full_markdown = clean_markdown(full_markdown)
        
        logger.info(f"\nğŸ“¦ Phase 5.2.0 ì§€ëŠ¥í˜• ì²­í‚¹ ì‹œì‘...")
        logger.info(f"  ğŸ“„ ì „ì²´ ë¬¸ì„œ: {len(full_markdown):,}ì")
        
        # Step 3: ì§€ëŠ¥í˜• ì²­í‚¹ (ì˜ë¯¸ ë‹¨ìœ„)
        chunked_data = self.semantic_chunker.chunk_markdown(full_markdown)
        
        # Step 4: ì²­í‚¹ í†µê³„
        stats = self.semantic_chunker.get_statistics(chunked_data)
        logger.info(f"  ğŸ“Š ì²­í¬ ìˆ˜: {stats.get('total_chunks', 0)}ê°œ")
        logger.info(f"  ğŸ“ í‰ê·  í¬ê¸°: {stats.get('avg_chunk_size', 0):.0f}ì")
        logger.info(f"  ğŸ¯ ëª©í‘œ ë‹¬ì„±ë¥ : {stats.get('target_achievement', 0):.1f}%")
        logger.info(f"âœ… ì§€ëŠ¥í˜• ì²­í‚¹ ì™„ë£Œ\n")
        
        # Step 5: ì²­í¬ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        output_parts = []
        
        for chunk_data in chunked_data:
            chunk_id = chunk_data['chunk_id']
            content = chunk_data['content']
            section = chunk_data['section']
            size = chunk_data['size']
            
            # ì²­í¬ í—¤ë” (ì£¼ì„ìœ¼ë¡œ)
            output_parts.append(f"<!-- Chunk {chunk_id}: {section} ({size}ì) -->\n")
            output_parts.append(content)
            output_parts.append("\n\n")
        
        return "".join(output_parts).strip()
    
    def _analyze_quality_checklist(self, results: List[Dict], markdown: str) -> Dict[str, float]:
        """
        5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
        
        1. ì›ë³¸ ì¶©ì‹¤ë„ 95% ëª©í‘œ
        2. ì²­í‚¹ í’ˆì§ˆ 90% ëª©í‘œ
        3. RAG ì í•©ë„ 95% ëª©í‘œ
        4. ë²”ìš©ì„± 100% ëª©í‘œ
        5. ê²½ìŸì‚¬ ëŒ€ë¹„ 95% ëª©í‘œ
        """
        
        if not results:
            return {
                'fidelity_score': 0.0,
                'chunking_score': 0.0,
                'rag_score': 0.0,
                'universality_score': 0.0,
                'competitive_score': 0.0,
                'overall_score': 0.0
            }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 1. ì›ë³¸ ì¶©ì‹¤ë„ (Fidelity Score)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        fidelity_score = self._calculate_fidelity_score(markdown, results)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 2. ì²­í‚¹ í’ˆì§ˆ (Chunking Quality)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        chunking_score = self._calculate_chunking_quality(markdown, results)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 3. RAG ì í•©ë„ (RAG Suitability)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        rag_score = self._calculate_rag_suitability(markdown)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 4. ë²”ìš©ì„± (Universality)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        universality_score = self._calculate_universality(results)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 5. ê²½ìŸì‚¬ ëŒ€ë¹„ (Competitive Score)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        competitive_score = self._calculate_competitive_score(
            fidelity_score, chunking_score, rag_score, universality_score
        )
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì¢…í•© ì ìˆ˜
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        overall_score = (
            fidelity_score * 0.25 +      # ì›ë³¸ ì¶©ì‹¤ë„ 25%
            chunking_score * 0.20 +      # ì²­í‚¹ í’ˆì§ˆ 20%
            rag_score * 0.20 +           # RAG ì í•©ë„ 20%
            universality_score * 0.20 +  # ë²”ìš©ì„± 20%
            competitive_score * 0.15     # ê²½ìŸì‚¬ ëŒ€ë¹„ 15%
        )
        
        return {
            'fidelity_score': fidelity_score,
            'chunking_score': chunking_score,
            'rag_score': rag_score,
            'universality_score': universality_score,
            'competitive_score': competitive_score,
            'overall_score': min(100.0, overall_score)
        }
    
    def _calculate_fidelity_score(self, markdown: str, results: List[Dict]) -> float:
        """
        âœ… 1. ì›ë³¸ ì¶©ì‹¤ë„ ê³„ì‚°
        
        í‰ê°€ ê¸°ì¤€:
        - ìµœì†Œ ê¸¸ì´ ì¶©ì¡±
        - êµ¬ì¡° í—¤ë” ì¡´ì¬
        - í˜ì´ì§€ë³„ ê· í˜•
        - ì‹ ë¢°ë„
        """
        score = 100.0
        
        # 1. ìµœì†Œ ê¸¸ì´ ì²´í¬
        if len(markdown) < 100:
            score -= 40
        elif len(markdown) < 500:
            score -= 20
        elif len(markdown) >= 1000:
            score += 5  # ë³´ë„ˆìŠ¤
        
        # 2. êµ¬ì¡° í—¤ë” ì¡´ì¬
        headers = re.findall(r'^#+\s+', markdown, re.MULTILINE)
        header_count = len(headers)
        
        if header_count == 0:
            score -= 25
        elif header_count >= 1 and header_count < 3:
            score -= 10
        elif header_count >= 5:
            score += 10  # ë³´ë„ˆìŠ¤
        
        # 3. í˜ì´ì§€ë³„ ë‚´ìš© ê· í˜•
        page_lengths = [len(r.get('content', '')) for r in results]
        if page_lengths and len(page_lengths) > 1:
            avg_len = sum(page_lengths) / len(page_lengths)
            if avg_len > 0:
                variance = sum((l - avg_len) ** 2 for l in page_lengths) / len(page_lengths)
                std_dev = variance ** 0.5
                cv = std_dev / avg_len  # ë³€ë™ê³„ìˆ˜
                
                if cv < 0.3:
                    score += 15  # ë§¤ìš° ê· í˜•ì¡í˜
                elif cv < 0.5:
                    score += 10
                elif cv > 1.0:
                    score -= 10  # ë¶ˆê· í˜•
        
        # 4. í‰ê·  ì‹ ë¢°ë„
        confidences = [r.get('confidence', 0.0) for r in results]
        if confidences:
            avg_confidence = sum(confidences) / len(confidences)
            score += avg_confidence * 10  # ìµœëŒ€ +10ì 
        
        return max(0.0, min(100.0, score))
    
    def _calculate_chunking_quality(self, markdown: str, results: List[Dict]) -> float:
        """
        âœ… 2. ì²­í‚¹ í’ˆì§ˆ ê³„ì‚° (Phase 5.2.0)
        
        í‰ê°€ ê¸°ì¤€:
        - ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´ (<!-- Chunk --> ì£¼ì„)
        - ì²­í¬ í¬ê¸° ê· ì¼ì„± (800~1,000ì)
        - ì„¹ì…˜ í—¤ë” ì¡´ì¬
        - í‘œ ì™„ê²°ì„±
        """
        score = 0.0
        
        # 1. ì²­í¬ ì£¼ì„ ì¡´ì¬ (ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹ í™•ì¸)
        chunk_comments = re.findall(r'<!-- Chunk (\d+):', markdown)
        chunk_count = len(chunk_comments)
        
        if chunk_count > 0:
            score += 30  # ì²­í‚¹ ì™„ë£Œ
        
        # 2. ì²­í¬ í¬ê¸° ë¶„ì„
        chunks = re.split(r'<!-- Chunk \d+:[^>]+-->', markdown)
        chunks = [c.strip() for c in chunks if c.strip()]
        
        if chunks:
            chunk_sizes = [len(c) for c in chunks]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            
            # í‰ê·  í¬ê¸°ê°€ 800~1,000ìì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            if 800 <= avg_size <= 1000:
                score += 30  # ìµœì  í¬ê¸°
            elif 600 <= avg_size <= 1200:
                score += 20  # ì–‘í˜¸
            elif avg_size < 600 or avg_size > 1200:
                score += 10  # ê°œì„  í•„ìš”
            
            # í¬ê¸° ê· ì¼ì„± (í‘œì¤€í¸ì°¨)
            if len(chunk_sizes) > 1 and avg_size > 0:
                variance = sum((s - avg_size) ** 2 for s in chunk_sizes) / len(chunk_sizes)
                std_dev = variance ** 0.5
                cv = std_dev / avg_size  # ë³€ë™ê³„ìˆ˜
                
                if cv < 0.2:
                    score += 20  # ë§¤ìš° ê· ì¼
                elif cv < 0.4:
                    score += 15  # ê· ì¼
                elif cv < 0.6:
                    score += 10  # ì–‘í˜¸
        
        # 3. ì„¹ì…˜ í—¤ë” ì¡´ì¬
        headers = re.findall(r'^##\s+', markdown, re.MULTILINE)
        if len(headers) >= chunk_count * 0.8:
            score += 20  # ëŒ€ë¶€ë¶„ ì²­í¬ê°€ í—¤ë” í¬í•¨
        elif len(headers) >= chunk_count * 0.5:
            score += 10
        
        return min(100.0, score)
    
    def _calculate_rag_suitability(self, markdown: str) -> float:
        """
        âœ… 3. RAG ì í•©ë„ ê³„ì‚°
        
        í‰ê°€ ê¸°ì¤€:
        - ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì—†ìŒ
        - ì¤‘ë³µ ì œê±°
        - êµ¬ì¡°í™”ëœ ë°ì´í„°
        """
        score = 100.0
        
        # 1. ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì²´í¬ (ê°ì )
        meta_keywords = [
            'ì´ ë¬¸ì„œëŠ”', 'ë‹¤ìŒê³¼ ê°™ì´', 'ì•„ë˜ì™€ ê°™ì´',
            'ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤', 'í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤',
            'ì´ í˜ì´ì§€', 'ë¬¸ì„œ ìƒë‹¨', 'ë¬¸ì„œ í•˜ë‹¨',
            'ìœ„ì—ì„œ ì–¸ê¸‰í•œ', 'ì•„ë˜ì—ì„œ ì„¤ëª…í• '
        ]
        
        meta_count = 0
        for keyword in meta_keywords:
            meta_count += markdown.count(keyword)
        
        score -= meta_count * 3  # ê°œë‹¹ -3ì 
        
        # 2. ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ì²´í¬ (ê°ì )
        lines = markdown.split('\n')
        line_counts = {}
        for line in lines:
            clean = line.strip()
            if len(clean) > 15:  # ì§§ì€ ì¤„ ì œì™¸
                line_counts[clean] = line_counts.get(clean, 0) + 1
        
        duplicates = sum(1 for count in line_counts.values() if count >= 3)
        score -= duplicates * 5  # ì¤‘ë³µ ì¤„ë‹¹ -5ì 
        
        # 3. êµ¬ì¡°í™”ëœ ë°ì´í„° (ê°€ì‚°ì )
        has_table = '|' in markdown and markdown.count('|') >= 6
        has_list = re.search(r'^\d+\.\s+', markdown, re.MULTILINE) is not None
        has_bullet = re.search(r'^[-*]\s+', markdown, re.MULTILINE) is not None
        
        if has_table:
            score += 10
        if has_list:
            score += 5
        if has_bullet:
            score += 5
        
        return max(0.0, min(100.0, score))
    
    def _calculate_universality(self, results: List[Dict]) -> float:
        """
        âœ… 4. ë²”ìš©ì„± ê³„ì‚°
        
        í‰ê°€ ê¸°ì¤€:
        - ë‹¤ì–‘í•œ ë¬¸ì„œ íƒ€ì… ì²˜ë¦¬
        - íƒ€ì…ë³„ ì „ëµ ì ìš©
        - í•˜ë“œì½”ë”© ì—†ìŒ
        """
        score = 100.0
        
        # 1. ë¬¸ì„œ íƒ€ì… ë‹¤ì–‘ì„±
        doc_types = set(r.get('doc_type', 'mixed') for r in results)
        type_diversity = len(doc_types)
        
        if type_diversity >= 3:
            score += 10  # ë§¤ìš° ë‹¤ì–‘
        elif type_diversity >= 2:
            score += 5
        
        # 2. íƒ€ì…ë³„ ì „ëµ ì ìš© í™•ì¸
        strategies_used = set(r.get('strategy', '') for r in results)
        if 'universal_v50' in ' '.join(strategies_used):
            score += 10  # Phase 5.2.0 ì „ëµ ì‚¬ìš©
        
        # 3. í•˜ë“œì½”ë”© ì²´í¬ (ë²„ìŠ¤ ì „ìš© í‚¤ì›Œë“œ)
        hardcoded_keywords = [
            'ì¼ë°˜ë²„ìŠ¤', 'ê´‘ì—­ë²„ìŠ¤', 'ë§ˆì„ë²„ìŠ¤',
            'ë°°ì°¨ê°„ê²©', 'ì²«ì°¨', 'ë§‰ì°¨'
        ]
        
        all_content = ' '.join(r.get('content', '') for r in results)
        
        # ë¬¸ì„œ íƒ€ì…ì´ 'diagram/transport_route'ì¼ ë•Œë§Œ í—ˆìš©
        is_transport = any(
            r.get('doc_type') == 'diagram' and 
            r.get('subtype') == 'transport_route'
            for r in results
        )
        
        if not is_transport:
            for keyword in hardcoded_keywords:
                if keyword in all_content:
                    score -= 15  # í•˜ë“œì½”ë”© ë°œê²¬! (í° ê°ì )
        
        return max(0.0, min(100.0, score))
    
    def _calculate_competitive_score(
        self,
        fidelity: float,
        chunking: float,
        rag: float,
        universality: float
    ) -> float:
        """
        âœ… 5. ê²½ìŸì‚¬ ëŒ€ë¹„ ì ìˆ˜ ê³„ì‚°
        
        ê²½ìŸì‚¬ ê¸°ì¤€:
        - ì›ë³¸ ì¶©ì‹¤ë„: 85ì 
        - ì²­í‚¹ í’ˆì§ˆ: 80ì 
        - RAG ì í•©ë„: 90ì 
        - ë²”ìš©ì„±: 70ì 
        """
        competitor_baseline = {
            'fidelity': 85.0,
            'chunking': 80.0,
            'rag': 90.0,
            'universality': 70.0
        }
        
        # ê° í•­ëª©ë³„ ê²½ìŸì‚¬ ëŒ€ë¹„ ì ìˆ˜
        fidelity_ratio = (fidelity / competitor_baseline['fidelity']) * 100
        chunking_ratio = (chunking / competitor_baseline['chunking']) * 100
        rag_ratio = (rag / competitor_baseline['rag']) * 100
        universality_ratio = (universality / competitor_baseline['universality']) * 100
        
        # í‰ê· 
        avg_ratio = (fidelity_ratio + chunking_ratio + rag_ratio + universality_ratio) / 4
        
        # 95% ì´ìƒì´ë©´ ë§Œì 
        if avg_ratio >= 95:
            return 100.0
        else:
            return min(100.0, avg_ratio)