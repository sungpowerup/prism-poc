"""
PRISM Phase 2.5 - Enhanced Pipeline (Phase 2.5 Extractor í˜¸í™˜)

ê°œì„  ì‚¬í•­:
- extract_full_page() â†’ extract() ë©”ì„œë“œëª… ë³€ê²½ ëŒ€ì‘
- Phase 2.5 ê°œì„  í”„ë¡¬í”„íŠ¸ ì ìš©

Author: ì´ì„œì˜ (Backend Lead) + ë°•ì¤€í˜¸ (AI/ML Lead)
Date: 2025-10-17
"""

import os
import time
from pathlib import Path
from typing import List, Dict, Optional
from PIL import Image
import fitz  # PyMuPDF

from models.layout_detector import LayoutDetector, DocumentElement, ElementType, BoundingBox
from core.claude_full_page_extractor import ClaudeFullPageExtractor, PageContent
from core.intelligent_chunker import IntelligentChunker
from core.document_analyzer import DocumentAnalyzer


class Phase2Pipeline:
    """
    PRISM Phase 2.5 íŒŒì´í”„ë¼ì¸ (Enhanced Chart Extraction)
    
    ì²˜ë¦¬ ë‹¨ê³„:
    1. â­ Claude Visionìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ ë¶„ì„ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)
    2. í…ìŠ¤íŠ¸, í‘œ, ì°¨íŠ¸, ê·¸ë˜í”„, ì´ë¯¸ì§€ ë™ì‹œ ì¶”ì¶œ
    3. Intelligent Chunking
    """
    
    def __init__(
        self,
        azure_endpoint: Optional[str] = None,
        azure_api_key: Optional[str] = None,
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        use_full_claude_vision: bool = True
    ):
        """
        Args:
            azure_endpoint: Azure OpenAI ì—”ë“œí¬ì¸íŠ¸ (ì„ íƒ)
            azure_api_key: Azure OpenAI API í‚¤ (ì„ íƒ)
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
            use_full_claude_vision: ì „ì²´ í˜ì´ì§€ Claude Vision ì‚¬ìš© ì—¬ë¶€
        """
        print("Initializing PRISM Phase 2.5 Pipeline (Enhanced Chart Extraction)...")
        
        # Azure OpenAI ì„¤ì • ì €ì¥
        self.azure_endpoint = azure_endpoint
        self.azure_api_key = azure_api_key
        
        # 1. Layout Detector (ì°¸ê³ ìš©)
        self.layout_detector = LayoutDetector()
        
        # 2. â­ Claude Full Page Extractor (Phase 2.5 ê°œì„ )
        self.use_full_claude_vision = use_full_claude_vision
        if use_full_claude_vision:
            self.claude_extractor = ClaudeFullPageExtractor(
                azure_endpoint=azure_endpoint,
                azure_api_key=azure_api_key
            )
            if self.claude_extractor.client:
                print("âœ… Phase 2.5 Enhanced Claude Vision enabled")
            else:
                print("âš ï¸  Claude Vision unavailable, falling back to OCR")
                self.use_full_claude_vision = False
        
        # 3. Intelligent Chunker
        self.chunker = IntelligentChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # 4. Document Analyzer
        self.analyzer = DocumentAnalyzer()
        
        print("âœ… Phase 2.5 Pipeline initialized")

    def _convert_page_to_chunks(
        self,
        page_content: PageContent
    ) -> List[Dict]:
        """
        PageContentë¥¼ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            page_content: Claudeê°€ ì¶”ì¶œí•œ í˜ì´ì§€ ë‚´ìš©
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        chunk_counter = 1
        
        page_num = page_content.page_num
        
        # 1. í…ìŠ¤íŠ¸ ì²­í¬
        for text_block in page_content.text_blocks:
            chunk_id = f"chunk_{page_num:03d}_{chunk_counter:03d}"
            chunks.append({
                "chunk_id": chunk_id,
                "type": "text",
                "content": text_block.text,
                "page_num": page_num,
                "metadata": {
                    "section_title": "",
                    "section_type": "text",
                    "confidence": text_block.confidence
                }
            })
            chunk_counter += 1
        
        # 2. í‘œ ì²­í¬
        for table in page_content.tables:
            chunk_id = f"table_{page_num:03d}_{chunk_counter:03d}"
            chunks.append({
                "chunk_id": chunk_id,
                "type": "table",
                "content": table.markdown,
                "page_num": page_num,
                "metadata": {
                    "caption": table.caption,
                    "confidence": table.confidence
                }
            })
            chunk_counter += 1
        
        # 3. â­ ì°¨íŠ¸ ì²­í¬ (Phase 2.5 - ë°ì´í„° í¬ì¸íŠ¸ í¬í•¨)
        for chart in page_content.charts:
            chunk_id = f"chart_{page_num:03d}_{chunk_counter:03d}"
            
            # ì°¨íŠ¸ ë‚´ìš© í¬ë§·íŒ…
            content_lines = [
                f"[ì°¨íŠ¸: {chart.title}]",
                f"íƒ€ì…: {chart.type}",
                f"ì„¤ëª…: {chart.description}",
                "ë°ì´í„°:"
            ]
            
            # â­ ë°ì´í„° í¬ì¸íŠ¸ ì¶”ê°€
            if chart.data_points:
                # ê·¸ë£¹ ë°ì´í„° ì—¬ë¶€ í™•ì¸
                if chart.data_points and isinstance(chart.data_points[0], dict):
                    first_point = chart.data_points[0]
                    if 'category' in first_point and 'values' in first_point:
                        # ê·¸ë£¹ ë°ì´í„° (ì˜ˆ: ì…ì¥ë£Œ - ì „ì²´/íŒ¬/ì¼ë°˜)
                        content_lines.append("")
                        for group in chart.data_points:
                            content_lines.append(f"[{group['category']}]")
                            for value in group['values']:
                                unit = value.get('unit', '')
                                content_lines.append(f"  - {value['label']}: {value['value']}{unit}")
                            content_lines.append("")
                    else:
                        # ë‹¨ìˆœ ë°ì´í„° (ì˜ˆ: ë‚¨ì„± 45.2%, ì—¬ì„± 54.8%)
                        for point in chart.data_points:
                            label = point.get('label', '')
                            value = point.get('value', '')
                            unit = point.get('unit', '')
                            content_lines.append(f"  - {label}: {value}{unit}")
            else:
                content_lines.append("  - (ë°ì´í„° ì—†ìŒ)")
            
            content = "\n".join(content_lines)
            
            chunks.append({
                "chunk_id": chunk_id,
                "type": "chart",
                "content": content,
                "page_num": page_num,
                "metadata": {
                    "chart_type": chart.type,
                    "title": chart.title,
                    "description": chart.description,
                    "data_points": chart.data_points,
                    "confidence": chart.confidence
                }
            })
            chunk_counter += 1
        
        # 4. ì´ë¯¸ì§€/ë‹¤ì´ì–´ê·¸ë¨ ì²­í¬
        for figure in page_content.figures:
            chunk_id = f"figure_{page_num:03d}_{chunk_counter:03d}"
            chunks.append({
                "chunk_id": chunk_id,
                "type": "figure",
                "content": f"[ì´ë¯¸ì§€: {figure.type}]\n{figure.description}",
                "page_num": page_num,
                "metadata": {
                    "figure_type": figure.type,
                    "description": figure.description,
                    "confidence": figure.confidence
                }
            })
            chunk_counter += 1
        
        return chunks

    def process(
        self,
        pdf_path: str,
        max_pages: Optional[int] = None
    ) -> Dict:
        """
        PDF ë¬¸ì„œ ì²˜ë¦¬ (Phase 2.5)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "="*60)
        print("PRISM Phase 2.5 - Document Processing")
        print("="*60)
        
        start_time = time.time()
        
        # 1. PDF ì—´ê¸°
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        if max_pages:
            total_pages = min(total_pages, max_pages)
        
        print(f"\nğŸ“„ ë¬¸ì„œ: {Path(pdf_path).name}")
        print(f"ğŸ“Š ì´ í˜ì´ì§€: {total_pages}")
        
        all_chunks = []
        stats = {
            'total_pages': total_pages,
            'text_chunks': 0,
            'table_chunks': 0,
            'chart_chunks': 0,
            'figure_chunks': 0
        }
        
        # 2. í˜ì´ì§€ë³„ ì²˜ë¦¬
        for page_num in range(total_pages):
            print(f"\n{'='*60}")
            print(f"ğŸ“„ Processing Page {page_num + 1}/{total_pages}")
            print(f"{'='*60}")
            
            # 2.1 í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            page = doc[page_num]
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # 2.2 â­ Claude Visionìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ ì¶”ì¶œ (Phase 2.5 ê°œì„ )
            if self.use_full_claude_vision:
                # âœ… ìˆ˜ì •: extract_full_page() â†’ extract()
                page_content = self.claude_extractor.extract(img, page_num + 1)
                
                if page_content:
                    # PageContent â†’ ì²­í¬ ë³€í™˜
                    page_chunks = self._convert_page_to_chunks(page_content)
                    all_chunks.extend(page_chunks)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    for chunk in page_chunks:
                        chunk_type = chunk['type']
                        if chunk_type == 'text':
                            stats['text_chunks'] += 1
                        elif chunk_type == 'table':
                            stats['table_chunks'] += 1
                        elif chunk_type == 'chart':
                            stats['chart_chunks'] += 1
                        elif chunk_type == 'figure':
                            stats['figure_chunks'] += 1
                else:
                    print(f"âš ï¸  Page {page_num + 1} extraction failed")
        
        doc.close()
        
        # 3. í†µê³„ ê³„ì‚°
        end_time = time.time()
        stats['processing_time'] = end_time - start_time
        stats['total_chunks'] = len(all_chunks)
        
        # 4. ê²°ê³¼ ë°˜í™˜
        result = {
            'chunks': all_chunks,
            'statistics': stats
        }
        
        print("\n" + "="*60)
        print("âœ… Processing Complete")
        print("="*60)
        print(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {stats['processing_time']:.1f}ì´ˆ")
        print(f"ğŸ“Š ì´ ì²­í¬: {stats['total_chunks']}ê°œ")
        print(f"   - í…ìŠ¤íŠ¸: {stats['text_chunks']}ê°œ")
        print(f"   - í‘œ: {stats['table_chunks']}ê°œ")
        print(f"   - ì°¨íŠ¸: {stats['chart_chunks']}ê°œ")
        print(f"   - ì´ë¯¸ì§€: {stats['figure_chunks']}ê°œ")
        
        return result


# ============================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("PRISM Phase 2.5 - Pipeline Test")
    print("="*60 + "\n")
    
    pipeline = Phase2Pipeline()
    
    # í…ŒìŠ¤íŠ¸ PDF ê²½ë¡œ
    test_pdf = "input/test_parser_02.pdf"
    
    if Path(test_pdf).exists():
        print(f"âœ… Test PDF found: {test_pdf}")
        result = pipeline.process(test_pdf, max_pages=3)
        
        # ê²°ê³¼ ì €ì¥
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        
        import json
        with open(output_dir / "test_phase25_result.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Results saved to: output/test_phase25_result.json")
    else:
        print(f"âŒ Test PDF not found: {test_pdf}")