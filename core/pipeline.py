"""
core/pipeline.py
PRISM Phase 5.7.7.2 - Pipeline (ì½”ë“œíœìŠ¤ ì œê±°)

âœ… Phase 5.7.7.2 ê¸´ê¸‰ ìˆ˜ì •:
1. Markdown í†µí•© í›„ ì½”ë“œíœìŠ¤ ì œê±° (ë¯¸ì†¡ ì œì•ˆ)
2. SemanticChunker ì „ë‹¬ ì „ ì •ì œ
3. í—¤ë” ì¸ì‹ë¥  100% ë³µêµ¬

(Phase 5.7.6.1 ê¸°ëŠ¥ ìœ ì§€)

Author: ì´ì„œì˜ (Backend Lead) + ë¯¸ì†¡ ì§„ë‹¨
Date: 2025-11-03
Version: 5.7.7.2 Hotfix
"""

import logging
import time
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)


class PipelineV576:
    """
    Phase 5.7.7.2 í†µí•© íŒŒì´í”„ë¼ì¸ (ì½”ë“œíœìŠ¤ ì œê±°)
    
    âœ… Phase 5.7.7.2 ê°œì„ :
    - Markdown í†µí•© í›„ ì½”ë“œíœìŠ¤ ì œê±°
    - í—¤ë” ì¸ì‹ë¥  ë³µêµ¬ (### ì œnì¡°)
    - ì²­í‚¹ ì •ìƒí™” (1ê°œ â†’ 3~4ê°œ)
    
    í”Œë¡œìš°:
    1. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    2. HybridExtractor: í˜ì´ì§€ë³„ ì¶”ì¶œ
    3. âœ… Markdown í†µí•© + ì½”ë“œíœìŠ¤ ì œê±° (Phase 5.7.7.2)
    4. SemanticChunker: ì¡°ë¬¸ ê²½ê³„ ê¸°ë°˜ ì²­í‚¹
    5. ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€
    """
    
    def __init__(
        self,
        pdf_processor,
        classifier,
        vlm_service,
        max_pages: int = 20,
        session_id: str = None
    ):
        """ì´ˆê¸°í™”"""
        self.pdf_processor = pdf_processor
        self.classifier = classifier
        self.vlm_service = vlm_service
        self.max_pages = max_pages
        self.session_id = session_id or self._generate_session_id()
        
        # Phase 5.7.4.1 components
        from core.semantic_chunker import SemanticChunker
        self.chunker = SemanticChunker(
            min_chunk_size=600,
            max_chunk_size=1200,
            target_chunk_size=900
        )
        
        logger.info("âœ… Phase 5.7.7.2 Pipeline ì´ˆê¸°í™” ì™„ë£Œ (ì½”ë“œíœìŠ¤ ì œê±°)")
        logger.info("   - HybridExtractor v5.7.7.1: pypdf Fallback + í˜ì´ì§€ ë§ˆì»¤ ì œê±°")
        logger.info("   - SemanticChunker v5.7.7.1: ì¡°ë¬¸ ê²½ê³„ ê¸°ë°˜ ì²­í‚¹")
        logger.info("   - Markdown ì •ì œ: ì½”ë“œíœìŠ¤ ì œê±° (Phase 5.7.7.2)")
    
    def process(self, pdf_path: str) -> Dict[str, Any]:
        """
        Phase 5.7.7.2 ì „ì²´ ì²˜ë¦¬ (ì½”ë“œíœìŠ¤ ì œê±°)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        logger.info(f"ğŸ¯ Phase 5.7.7.2 ì²˜ë¦¬ ì‹œì‘ (ì½”ë“œíœìŠ¤ ì œê±°)")
        logger.info(f"   íŒŒì¼: {pdf_path}")
        logger.info(f"   ì„¸ì…˜: {self.session_id}")
        logger.info(f"   ìµœëŒ€ í˜ì´ì§€: {self.max_pages}")
        
        start_time = time.time()
        
        # Step 1: PDF â†’ ì´ë¯¸ì§€
        logger.info("ğŸ“„ Step 1: PDF â†’ ì´ë¯¸ì§€ ë³€í™˜")
        # pdf_to_imagesëŠ” [(base64_image, page_num), ...] í˜•ì‹ ë°˜í™˜
        image_tuples = self.pdf_processor.pdf_to_images(pdf_path, self.max_pages)
        logger.info(f"   âœ… {len(image_tuples)}í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")
        
        # Step 2: í˜ì´ì§€ë³„ ì¶”ì¶œ (HybridExtractor)
        from core.hybrid_extractor import HybridExtractor
        extractor = HybridExtractor(
            vlm_service=self.vlm_service,
            pdf_path=pdf_path
        )
        
        pages_data = []
        
        for image_data, page_num in image_tuples:
            logger.info(f"ğŸ“„ í˜ì´ì§€ {page_num}/{len(image_tuples)} ì²˜ë¦¬ ì‹œì‘")
            
            page_result = extractor.extract(image_data, page_num)
            
            if not page_result.get('is_empty', False):
                pages_data.append(page_result)
                logger.info(f"   âœ… í˜ì´ì§€ {page_num} ì™„ë£Œ: í’ˆì§ˆ {page_result['quality_score']}/100 (ì¶œì²˜: {page_result['source']})")
            else:
                logger.warning(f"   âš ï¸ í˜ì´ì§€ {page_num} ë¹ˆ í˜ì´ì§€ ì œì™¸")
        
        # Fallback í†µê³„
        fallback_stats = extractor.get_fallback_stats()
        logger.info(f"ğŸ“Š ìœ íš¨ í˜ì´ì§€: {len(pages_data)}/{len(image_tuples)} (ë¹ˆ í˜ì´ì§€ {len(image_tuples) - len(pages_data)}ê°œ ì œì™¸)")
        logger.info(f"ğŸ“Š Fallback í†µê³„:")
        logger.info(f"   - VLM ì„±ê³µ: {fallback_stats['vlm_success_count']}í˜ì´ì§€")
        logger.info(f"   - Fallback ì‚¬ìš©: {fallback_stats['fallback_count']}í˜ì´ì§€")
        logger.info(f"   - Fallback ë¹„ìœ¨: {fallback_stats['fallback_rate']*100:.1f}%")
        
        # Step 3: Markdown í†µí•© + ì½”ë“œíœìŠ¤ ì œê±°
        logger.info("ğŸ“ Step 3: Markdown í†µí•© + ì½”ë“œíœìŠ¤ ì œê±°")
        
        markdown_parts = []
        for page_data in pages_data:
            content = page_data.get('content', '')
            if content:
                markdown_parts.append(content)
        
        full_markdown = '\n\n'.join(markdown_parts)
        
        # âœ… Phase 5.7.7.2: ì½”ë“œíœìŠ¤ ì œê±° (ë¯¸ì†¡ ì œì•ˆ)
        full_markdown = self._remove_code_fences(full_markdown)
        
        logger.info(f"   âœ… Markdown í†µí•© ì™„ë£Œ: {len(full_markdown)} ê¸€ì")
        
        # Step 4: SemanticChunking
        logger.info("âœ‚ï¸ Step 4: SemanticChunking v5.7.7.1 (ì¡°ë¬¸ ê²½ê³„)")
        chunks = self.chunker.chunk(full_markdown)
        logger.info(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # Step 5: ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€
        logger.info("ğŸ“Š Step 5: ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€")
        checklist = self._evaluate_checklist(pages_data, chunks, full_markdown)
        
        for key, score in checklist.items():
            logger.info(f"   âœ… {key}: {score}/100")
        
        overall_score = checklist['overall_score']
        logger.info(f"   ğŸ¯ ì¢…í•©: {overall_score}/100")
        
        # ìµœì¢… ê²°ê³¼
        elapsed = time.time() - start_time
        
        logger.info("âœ… Phase 5.7.7.2 ì²˜ë¦¬ ì™„ë£Œ")
        logger.info(f"   - ìœ íš¨ í˜ì´ì§€: {len(pages_data)}/{len(image_tuples)}")
        logger.info(f"   - ë¹ˆ í˜ì´ì§€: {len(image_tuples) - len(pages_data)}")
        logger.info(f"   - Fallback ì‚¬ìš©: {fallback_stats['fallback_count']}")
        logger.info(f"   - ì‹œê°„: {elapsed:.1f}ì´ˆ")
        logger.info(f"   - ì¢…í•©: {overall_score}/100")
        
        return {
            'session_id': self.session_id,
            'markdown': full_markdown,
            'chunks': chunks,
            'pages_processed': len(pages_data),
            'total_pages': len(image_tuples),
            'empty_pages': len(image_tuples) - len(pages_data),
            'fallback_count': fallback_stats['fallback_count'],
            'fallback_rate': fallback_stats['fallback_rate'],
            'processing_time': elapsed,
            'checklist': checklist,
            'overall_score': overall_score
        }
    
    def _remove_code_fences(self, content: str) -> str:
        """
        âœ… Phase 5.7.7.2: ì½”ë“œíœìŠ¤ ì œê±° (ë¯¸ì†¡ ì œì•ˆ)
        
        ë¬¸ì œ:
        - VLMì´ Markdownì„ ì½”ë“œë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ë©´ í—¤ë” ì¸ì‹ ì‹¤íŒ¨
        - ```\n### ì œ1ì¡°...\n``` â†’ í—¤ë”ê°€ ì½”ë“œë¡œ ì·¨ê¸‰
        
        í•´ê²°:
        - ì•ë’¤ ì½”ë“œíœìŠ¤ ì œê±°
        - ì¤‘ê°„ ì½”ë“œíœìŠ¤ëŠ” ë³´ì¡´ (ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œì¼ ìˆ˜ ìˆìŒ)
        
        Args:
            content: ì›ë³¸ Markdown
        
        Returns:
            ì½”ë“œíœìŠ¤ ì œê±°ëœ Markdown
        """
        # 1) ì•ìª½ ì½”ë“œíœìŠ¤ ì œê±°
        content = re.sub(r'^```[a-z]*\s*\n', '', content, flags=re.MULTILINE)
        
        # 2) ë’¤ìª½ ì½”ë“œíœìŠ¤ ì œê±°
        content = re.sub(r'\n```\s*$', '', content, flags=re.MULTILINE)
        
        # 3) ì•ë’¤ ê³µë°± ì •ë¦¬
        content = content.strip()
        
        logger.debug(f"      ì½”ë“œíœìŠ¤ ì œê±° ì™„ë£Œ: {len(content)} ê¸€ì")
        return content
    
    def _evaluate_checklist(
        self,
        pages_data: List[Dict[str, Any]],
        chunks: List[Dict[str, Any]],
        full_markdown: str
    ) -> Dict[str, int]:
        """
        Phase 5.7.4.1 ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€ (ë³´ìˆ˜ì )
        
        5ê°€ì§€ ê¸°ì¤€:
        1. ì›ë³¸ ì¶©ì‹¤ë„ (90~100)
        2. ì²­í‚¹ í’ˆì§ˆ (60~100)
        3. RAG ì í•©ë„ (60~100)
        4. ë²”ìš©ì„± (90~100)
        5. ê²½ìŸë ¥ (60~80)
        
        Args:
            pages_data: í˜ì´ì§€ë³„ ì¶”ì¶œ ê²°ê³¼
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            full_markdown: ì „ì²´ Markdown
        
        Returns:
            ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìˆ˜
        """
        # 1. ì›ë³¸ ì¶©ì‹¤ë„
        avg_quality = sum(p['quality_score'] for p in pages_data) / max(1, len(pages_data))
        fidelity_score = int(avg_quality * 0.9)  # ë³´ìˆ˜ì 
        
        # 2. ì²­í‚¹ í’ˆì§ˆ
        avg_chunk_size = sum(len(c['content']) for c in chunks) / max(1, len(chunks))
        
        if 600 <= avg_chunk_size <= 1200:
            chunking_score = 100
        elif 400 <= avg_chunk_size < 600:
            chunking_score = 80
        elif 1200 < avg_chunk_size <= 1800:
            chunking_score = 80
        else:
            chunking_score = 70
        
        # 3. RAG ì í•©ë„
        if len(chunks) >= 3:
            rag_score = min(100, 70 + len(chunks) * 5)
        elif len(chunks) == 2:
            rag_score = 60
        else:
            rag_score = 33  # 1ê°œ ì²­í¬ëŠ” RAGì— ë¶€ì í•©
        
        # 4. ë²”ìš©ì„±
        versatility_score = 95  # HybridExtractor + pypdf Fallback
        
        # 5. ê²½ìŸë ¥
        if fidelity_score >= 90 and chunking_score >= 80:
            competitiveness_score = 80
        elif fidelity_score >= 80:
            competitiveness_score = 70
        else:
            competitiveness_score = 64
        
        # ì¢…í•© ì ìˆ˜
        overall_score = int(
            fidelity_score * 0.25 +
            chunking_score * 0.20 +
            rag_score * 0.25 +
            versatility_score * 0.15 +
            competitiveness_score * 0.15
        )
        
        return {
            'ì›ë³¸ ì¶©ì‹¤ë„': fidelity_score,
            'ì²­í‚¹ í’ˆì§ˆ': chunking_score,
            'RAG ì í•©ë„': rag_score,
            'ë²”ìš©ì„±': versatility_score,
            'ê²½ìŸë ¥': competitiveness_score,
            'overall_score': overall_score
        }
    
    def _generate_session_id(self) -> str:
        """ì„¸ì…˜ ID ìƒì„±"""
        import hashlib
        import time
        
        raw = f"{time.time()}"
        return hashlib.md5(raw.encode()).hexdigest()[:8]


# âœ… í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ í´ë˜ìŠ¤ëª… ì§€ì›
class Pipeline(PipelineV576):
    """v5.7.6 í˜¸í™˜ì„± ë˜í¼"""
    pass


class Phase53Pipeline:
    """
    v5.3 í˜¸í™˜ì„± ë˜í¼ (app.pyìš©)
    
    ê¸°ì¡´ API:
        pipeline = Phase53Pipeline(pdf_processor, vlm_service)
        result = pipeline.process_pdf(pdf_path)
    
    ìƒˆ API:
        pipeline = PipelineV576(pdf_processor, classifier, vlm_service)
        result = pipeline.process(pdf_path)
    """
    
    def __init__(self, pdf_processor, vlm_service, max_pages: int = 20):
        """
        ê¸°ì¡´ API í˜¸í™˜ì„± ì´ˆê¸°í™”
        
        Args:
            pdf_processor: PDFProcessor ì¸ìŠ¤í„´ìŠ¤
            vlm_service: VLMServiceV50 ì¸ìŠ¤í„´ìŠ¤
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        """
        # ClassifierëŠ” ë‚´ë¶€ì—ì„œ ìë™ ìƒì„±
        from core.document_classifier import DocumentClassifierV50
        classifier = DocumentClassifierV50()
        
        # ìƒˆ Pipeline ì´ˆê¸°í™”
        self._pipeline = PipelineV576(
            pdf_processor=pdf_processor,
            classifier=classifier,
            vlm_service=vlm_service,
            max_pages=max_pages
        )
    
    def process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """
        ê¸°ì¡´ API í˜¸í™˜ì„± ë©”ì„œë“œ
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        return self._pipeline.process(pdf_path)