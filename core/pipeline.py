"""
core/pipeline.py
PRISM Phase 0.3.3 Final - Pipeline with Safe Normalizers Only

âœ… Phase 0.3.3 Final ìˆ˜ì •:
1. Safe íŒŒì¼ ì „ìš© (Fallback ì œê±°)
2. ì›ë³¸ ì¶©ì‹¤ë„ ìš°ì„ 
3. ê³¨ë“  diff ê¸°ë°˜ ì •ê·œí™”

Author: ì´ì„œì˜ (Backend Lead)
Date: 2025-11-08
Version: Phase 0.3.3 Final
"""

import logging
import time
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
import string
import random

logger = logging.getLogger(__name__)

# âœ… Phase 0.3.3: Safe ëª¨ë“ˆë§Œ ì‚¬ìš© (Fallback ì œê±°)
from core.typo_normalizer_safe import TypoNormalizer
from core.post_merge_normalizer_safe import PostMergeNormalizer
from core.semantic_chunker import SemanticChunker
from core.document_classifier import DocumentClassifierV50
from core.hybrid_extractor import HybridExtractor

logger.info("âœ… Safe Normalizers ë¡œë“œ ì™„ë£Œ (Phase 0.3.3)")


class ProcessingPipeline:
    """
    Phase 0.3.3 ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Safe Only)
    
    âœ… Phase 0.3.3 ê°œì„ :
    - Safe íŒŒì¼ ì „ìš©
    - ë ˆì´ì–´ ë¶„ë¦¬ ì •ê·œí™”
    - ê³¨ë“  diff ê¸°ë°˜
    - ì›ë³¸ ì¶©ì‹¤ë„ ìµœìš°ì„ 
    """
    
    VERSION = "Phase 0.3.3"
    
    def __init__(
        self,
        pdf_path: str,
        vlm_service,
        session_id: str,
        max_pages: int = 20
    ):
        """ì´ˆê¸°í™”"""
        self.pdf_path = pdf_path
        self.vlm_service = vlm_service
        self.session_id = session_id
        self.max_pages = max_pages
        
        # âš ï¸ Phase 0.3.3: DocumentClassifier ë¹„í™œì„±í™”
        # ì´ìœ : VLM client ì†ì„± ë¬¸ì œë¡œ ì¸í•œ AttributeError
        # í˜„ì¬ ì „ëµ: statute ê³ ì • (ì¸ì‚¬ê·œì • ë¬¸ì„œ íŠ¹í™”)
        if hasattr(vlm_service, 'classifier'):
            self.classifier = vlm_service.classifier
            logger.info("âœ… VLM Serviceì˜ classifier ì‚¬ìš©")
        else:
            logger.warning("âš ï¸ VLM Serviceì— classifier ì—†ìŒ")
            self.classifier = DocumentClassifierV50(vlm_service)
        
        # ì²­í‚¹ ì—”ì§„
        self.chunker = SemanticChunker()
        
        # âœ… Phase 0.3.3: ì •ê·œí™” ì—”ì§„ (Safe ë²„ì „)
        self.post_normalizer = PostMergeNormalizer()
        self.typo_normalizer = TypoNormalizer()
        
        # HybridExtractorëŠ” ë‚˜ì¤‘ì— ì´ˆê¸°í™” (pdf_path í•„ìš”)
        self.extractor = None
        
        logger.info(f"âœ… {self.VERSION} Pipeline ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - Safe Mode: í™œì„±í™”")
        logger.info(f"   - SemanticChunker: ë¬¸ì¥ ê²½ê³„ ë³´ì¡´")
        logger.info(f"   - HybridExtractor: íƒ€ì… ì•ˆì „ ì²˜ë¦¬")
    
    def process(self) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        
        logger.info(f"ğŸ¯ {self.VERSION} ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"   íŒŒì¼: {self.pdf_path}")
        logger.info(f"   ì„¸ì…˜: {self.session_id}")
        logger.info(f"   ìµœëŒ€ í˜ì´ì§€: {self.max_pages}")
        
        try:
            # Step 1: PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
            logger.info("ğŸ“„ Step 1: PDF â†’ ì´ë¯¸ì§€ ë³€í™˜")
            
            # PDFProcessor ê°€ì ¸ì˜¤ê¸°
            from core.pdf_processor import PDFProcessor
            pdf_processor = PDFProcessor()
            
            images = pdf_processor.pdf_to_images(
                self.pdf_path,
                max_pages=self.max_pages
            )
            logger.info(f"   âœ… {len(images)}í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")
            
            # Step 2: ë¬¸ì„œ ë¶„ë¥˜
            logger.info("ğŸ“Š Step 2: ë¬¸ì„œ ë¶„ë¥˜")
            
            # ì²« í˜ì´ì§€ ì´ë¯¸ì§€ ì¶”ì¶œ
            first_image = images[0][0] if images else None
            
            # âš ï¸ Phase 0.3.3: statute ê³ ì • (Classifier ë¹„í™œì„±í™”)
            doc_type = 'statute'
            logger.info(f"   âœ… ì „ì—­ doc_type: {doc_type} (Classifier ë¹„í™œì„±í™”)")
            
            # Step 3: HybridExtractor ì´ˆê¸°í™”
            logger.info("ğŸ“ Step 3: HybridExtractor ì´ˆê¸°í™”")
            
            allow_tables = (doc_type == 'statute')
            
            self.extractor = HybridExtractor(
                vlm_service=self.vlm_service,
                pdf_path=self.pdf_path,
                allow_tables=allow_tables
            )
            
            logger.info(f"   âœ… HybridExtractor ì´ˆê¸°í™” ì™„ë£Œ (allow_tables={allow_tables})")
            
            # Step 4: í˜ì´ì§€ë³„ ì¶”ì¶œ
            logger.info("ğŸ“ Step 4: í˜ì´ì§€ë³„ ì¶”ì¶œ")
            pages_data = []
            
            for i, (image_data, page_num) in enumerate(images, 1):
                result = self.extractor.extract(image_data, page_num)
                
                # ìœ íš¨ í˜ì´ì§€ë§Œ ì¶”ê°€
                if result.get('quality_score', 0) >= 50:
                    # Markdown ì¶”ì¶œ
                    markdown = result.get('content', '')
                    
                    pages_data.append({
                        'markdown': markdown,
                        'page_num': page_num,
                        'source': result.get('source', 'unknown'),
                        'quality_score': result.get('quality_score', 0)
                    })
            
            # ìœ íš¨ í˜ì´ì§€ í†µê³„
            valid_count = len(pages_data)
            empty_count = len(images) - valid_count
            
            logger.info(f"ğŸ“Š ìœ íš¨ í˜ì´ì§€: {valid_count}/{len(images)} (ë¹ˆ í˜ì´ì§€ {empty_count}ê°œ ì œì™¸)")
            
            # Fallback í†µê³„
            fallback_count = sum(1 for p in pages_data if p.get('source') == 'fallback')
            fallback_ratio = fallback_count / valid_count if valid_count > 0 else 0
            
            logger.info(f"ğŸ“Š Fallback í†µê³„:")
            logger.info(f"   - VLM ì„±ê³µ: {valid_count - fallback_count}í˜ì´ì§€")
            logger.info(f"   - Fallback ì‚¬ìš©: {fallback_count}í˜ì´ì§€")
            logger.info(f"   - Fallback ë¹„ìœ¨: {fallback_ratio:.1%}")
            
            # Step 5: Markdown í†µí•©
            logger.info("ğŸ“ Step 5: Markdown í†µí•© + ì½”ë“œíœìŠ¤ ì œê±°")
            full_markdown = self._merge_markdown(pages_data)
            logger.info(f"   âœ… Markdown í†µí•© ì™„ë£Œ: {len(full_markdown)} ê¸€ì")
            
            # Step 6: í›„ì²˜ë¦¬ (Safe Mode)
            logger.info(f"ğŸ”§ Step 6: í›„ì²˜ë¦¬ (Safe Mode, doc_type={doc_type})")
            full_markdown = self.post_normalizer.normalize(full_markdown, doc_type)
            full_markdown = self.typo_normalizer.normalize(full_markdown, doc_type)
            
            # Step 7: SemanticChunking
            logger.info("âœ‚ï¸ Step 7: SemanticChunking Phase 0.3.3 (ë¬¸ì¥ ê²½ê³„ ë³´ì¡´)")
            chunks = self.chunker.chunk(full_markdown)
            logger.info(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            # Step 8: ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€
            logger.info("ğŸ“Š Step 8: ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€")
            checklist = self._evaluate_checklist(
                pages_data=pages_data,
                markdown=full_markdown,
                chunks=chunks,
                doc_type=doc_type
            )
            
            # ì²˜ë¦¬ ì‹œê°„
            elapsed = time.time() - start_time
            
            logger.info(f"âœ… {self.VERSION} ì²˜ë¦¬ ì™„ë£Œ")
            logger.info(f"   - ìœ íš¨ í˜ì´ì§€: {valid_count}/{len(images)}")
            logger.info(f"   - ë¹ˆ í˜ì´ì§€: {empty_count}")
            logger.info(f"   - Fallback ì‚¬ìš©: {fallback_count}")
            logger.info(f"   - ì‹œê°„: {elapsed:.1f}ì´ˆ")
            logger.info(f"   - ì¢…í•©: {checklist['overall']}/100")
            
            return {
                'success': True,
                'markdown': full_markdown,
                'chunks': chunks,
                'pages_count': valid_count,
                'doc_type': doc_type,
                'checklist': checklist,
                'elapsed_time': elapsed,
                'fallback_count': fallback_count,
                'fallback_ratio': fallback_ratio
            }
            
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e)
            }
    
    def _generate_session_id(self) -> str:
        """ì„¸ì…˜ ID ìƒì„±"""
        return ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))
    
    def _merge_markdown(self, pages_data: List[Dict]) -> str:
        """Markdown í†µí•©"""
        parts = []
        
        for page in pages_data:
            md = page['markdown']
            
            # ì½”ë“œíœìŠ¤ ì œê±°
            md = md.replace('```markdown\n', '').replace('\n```', '')
            
            parts.append(md)
        
        return '\n\n'.join(parts)
    
    def _evaluate_checklist(
        self,
        pages_data: List[Dict],
        markdown: str,
        chunks: List[Dict],
        doc_type: str
    ) -> Dict[str, int]:
        """
        ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€
        
        âš ï¸ ë‚´ë¶€ íœ´ë¦¬ìŠ¤í‹± ì§„ë‹¨ìš©
        """
        
        # 1. ì›ë³¸ ì¶©ì‹¤ë„
        fidelity = self._check_fidelity(pages_data, markdown)
        logger.info(f"   âœ… ì›ë³¸ ì¶©ì‹¤ë„: {fidelity}/100")
        
        # 2. ì²­í‚¹ í’ˆì§ˆ
        chunking = self._check_chunking(chunks, doc_type)
        logger.info(f"   âœ… ì²­í‚¹ í’ˆì§ˆ: {chunking}/100")
        
        # 3. RAG ì í•©ë„
        rag = self._check_rag_readiness(chunks)
        logger.info(f"   âœ… RAG ì í•©ë„: {rag}/100")
        
        # 4. ë²”ìš©ì„±
        generality = self._check_generality(markdown, doc_type)
        logger.info(f"   âœ… ë²”ìš©ì„±: {generality}/100")
        
        # 5. ê²½ìŸë ¥
        competitive = self._check_competitive_edge(markdown, chunks)
        logger.info(f"   âœ… ê²½ìŸë ¥: {competitive}/100")
        
        # ì¢…í•© (ê°€ì¤‘ í‰ê· )
        overall = int(
            fidelity * 0.3 +
            chunking * 0.2 +
            rag * 0.2 +
            generality * 0.15 +
            competitive * 0.15
        )
        
        logger.info(f"   ğŸ¯ ì¢…í•©: {overall}/100 (ë‚´ë¶€ ì§„ë‹¨ìš©)")
        
        return {
            'fidelity': fidelity,
            'chunking': chunking,
            'rag_readiness': rag,
            'generality': generality,
            'competitive_edge': competitive,
            'overall': overall
        }
    
    def _check_fidelity(self, pages_data: List[Dict], markdown: str) -> int:
        """ì›ë³¸ ì¶©ì‹¤ë„ ê²€ì‚¬"""
        score = 100
        
        # í˜ì´ì§€ ë§ˆì»¤ ë‚¨ì•„ìˆìœ¼ë©´ ê°ì 
        marker_patterns = [
            r'_\d{3,4}-\d{1,2}_',
            r'\*\d{3,4}-\d{1,2}\*',
            r'^\d{3,4}-\d{1,2}$',
        ]
        
        for pattern in marker_patterns:
            if re.search(pattern, markdown, re.MULTILINE):
                score -= 20
                break
        
        # ì¡°ë¬¸ í—¤ë” í™•ì¸
        if 'ì œ1ì¡°' in markdown or 'ì œ2ì¡°' in markdown:
            score += 0
        else:
            score -= 10
        
        # ê°œì • ì´ë ¥ í™•ì¸
        if 'ê°œì •' in markdown and re.search(r'\d{4}\.\s*\d{1,2}\.\s*\d{1,2}', markdown):
            score += 0
        else:
            score -= 5
        
        return max(0, min(100, score))
    
    def _check_chunking(self, chunks: List[Dict], doc_type: str) -> int:
        """ì²­í‚¹ í’ˆì§ˆ ê²€ì‚¬"""
        if not chunks:
            return 0
        
        score = 100
        
        # ì²­í¬ í¬ê¸° ë¶„í¬
        sizes = [c['metadata']['char_count'] for c in chunks]
        avg_size = sum(sizes) / len(sizes)
        
        # ëª©í‘œ: 600-1200ì
        if 600 <= avg_size <= 1200:
            score += 0
        else:
            score -= 20
        
        # ì¡°ë¬¸ ë‹¨ìœ„ ì²­í‚¹
        if doc_type == 'statute':
            article_count = sum(1 for c in chunks if 'ì œ' in c['content'] and 'ì¡°' in c['content'])
            if article_count >= len(chunks) * 0.7:
                score += 0
            else:
                score -= 10
        
        return max(0, min(100, score))
    
    def _check_rag_readiness(self, chunks: List[Dict]) -> int:
        """RAG ì í•©ë„ ê²€ì‚¬"""
        score = 100
        
        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        for chunk in chunks:
            if not chunk.get('metadata', {}).get('article_no'):
                score -= 5
                break
        
        # ì²­í¬ ë…ë¦½ì„±
        avg_len = sum(len(c['content']) for c in chunks) / len(chunks) if chunks else 0
        if avg_len > 500:
            score += 0
        else:
            score -= 10
        
        return max(0, min(100, score))
    
    def _check_generality(self, markdown: str, doc_type: str) -> int:
        """ë²”ìš©ì„± ê²€ì‚¬"""
        score = 100
        return score
    
    def _check_competitive_edge(self, markdown: str, chunks: List[Dict]) -> int:
        """ê²½ìŸë ¥ ê²€ì‚¬"""
        score = 80
        
        # êµ¬ì¡° ë³´ì¡´
        if '###' in markdown:
            score += 10
        
        # ë©”íƒ€ë°ì´í„° í’ë¶€ì„±
        if chunks and chunks[0].get('metadata', {}).get('article_title'):
            score += 10
        
        return min(100, score)


# âœ… app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
class Phase53Pipeline(ProcessingPipeline):
    """Phase 5.3 Pipeline í˜¸í™˜ í´ë˜ìŠ¤"""
    
    def __init__(self, pdf_processor, vlm_service, max_pages: int = 20):
        """
        app.py í˜¸í™˜ì„± ì´ˆê¸°í™”
        
        Args:
            pdf_processor: PDFProcessor ì¸ìŠ¤í„´ìŠ¤ (ì‚¬ìš© ì•ˆ í•¨)
            vlm_service: VLMServiceV50 ì¸ìŠ¤í„´ìŠ¤
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        """
        # ë”ë¯¸ ê²½ë¡œ (ì‹¤ì œ ì²˜ë¦¬ ì‹œ êµì²´ë¨)
        super().__init__(
            pdf_path="",
            vlm_service=vlm_service,
            session_id=self._generate_session_id(),
            max_pages=max_pages
        )
        # pdf_processorëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (PDFProcessorë¥¼ ë‚´ë¶€ì—ì„œ ìƒì„±)
    
    def process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """
        app.py í˜¸í™˜ì„± ë©”ì„œë“œ
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        # ê²½ë¡œ ì—…ë°ì´íŠ¸
        self.pdf_path = pdf_path
        
        return self.process()