"""
core/pipeline.py
PRISM Phase 5.7.6.1 - Pipeline (ê¸´ê¸‰ íŒ¨ì¹˜)

âœ… Phase 5.7.6.1 ê¸´ê¸‰ ìˆ˜ì •:
1. ì´ë¯¸ì§€ ë°ì´í„° íŠœí”Œ ì–¸íŒ¨í‚¹ ìˆ˜ì •
2. ë¹ˆ í˜ì´ì§€ ì²˜ë¦¬ ì•ˆì •í™”

(Phase 5.7.4 ê¸°ëŠ¥ ìœ ì§€)

Author: ì´ì„œì˜ (Backend Lead) + ë§ˆì°½ìˆ˜ì‚° íŒ€
Date: 2025-11-02
Version: 5.7.6.1 Hotfix
"""

import logging
from typing import List, Dict, Any, Optional
import time
import uuid
import json
from pathlib import Path
import statistics

# Phase 5.7.4: HybridExtractor v5.7.4
try:
    from .hybrid_extractor import HybridExtractor
    from .semantic_chunker import SemanticChunker
except ImportError:
    import sys
    sys.path.insert(0, str(Path(__file__).parent))
    from hybrid_extractor import HybridExtractor
    from semantic_chunker import SemanticChunker

logger = logging.getLogger(__name__)


class Phase53Pipeline:
    """
    Phase 5.7.6.1 ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ê¸´ê¸‰ íŒ¨ì¹˜)
    
    íŠ¹ì§•:
    - âœ… ì´ë¯¸ì§€ ë°ì´í„° íŠœí”Œ ì–¸íŒ¨í‚¹ ìˆ˜ì •
    - HybridExtractor v5.7.6 í†µí•© (pypdf Fallback)
    - ë¹ˆ í˜ì´ì§€ ìë™ Skip (DoD æ¯ìˆ˜ ì œì™¸)
    - SemanticChunker v5.7.4.1 (ì¡°ë¬¸ ê²½ê³„ ê¸°ë°˜)
    - CV íŒíŠ¸ ê¸°ë°˜ ì§€ëŠ¥í˜• ì¶”ì¶œ
    - KVS ì •ê·œí™” + ë³„ë„ ì €ì¥
    - ê´€ì¸¡ì„± ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    """
    
    def __init__(self, pdf_processor, vlm_service, storage=None):
        """
        Args:
            pdf_processor: PDFProcessor ì¸ìŠ¤í„´ìŠ¤
            vlm_service: VLMServiceV50 ì¸ìŠ¤í„´ìŠ¤
            storage: Storage ì¸ìŠ¤í„´ìŠ¤ (Optional)
        """
        self.pdf_processor = pdf_processor
        self.vlm_service = vlm_service
        self.storage = storage
        
        # âœ… Phase 5.7.4: HybridExtractorëŠ” process_pdfì—ì„œ ì´ˆê¸°í™” (PDF ê²½ë¡œ í•„ìš”)
        self.extractor = None
        
        # Phase 5.7.4: SemanticChunker v5.7.4.1
        self.chunker = SemanticChunker(
            min_chunk_size=600,
            max_chunk_size=1200,
            target_chunk_size=900
        )
        
        logger.info("âœ… Phase 5.7.6.1 Pipeline ì´ˆê¸°í™” ì™„ë£Œ (ê¸´ê¸‰ íŒ¨ì¹˜)")
        logger.info("   - HybridExtractor v5.7.6: pypdf Fallback ì§€ì›")
        logger.info("   - SemanticChunker v5.7.4.1: ì¡°ë¬¸ ê²½ê³„ ê¸°ë°˜ ì²­í‚¹")
    
    def process_pdf(
        self,
        pdf_path: str,
        max_pages: int = 20,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        PDF ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ (Phase 5.7.6.1 ê¸´ê¸‰ íŒ¨ì¹˜)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°±
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ (Fallback í†µê³„ í¬í•¨)
        """
        start_time = time.time()
        session_id = str(uuid.uuid4())[:8]
        
        logger.info(f"ğŸ¯ Phase 5.7.6.1 ì²˜ë¦¬ ì‹œì‘ (ê¸´ê¸‰ íŒ¨ì¹˜)")
        logger.info(f"   íŒŒì¼: {pdf_path}")
        logger.info(f"   ì„¸ì…˜: {session_id}")
        logger.info(f"   ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        
        try:
            # âœ… Phase 5.7.4: HybridExtractor ì´ˆê¸°í™” (PDF ê²½ë¡œ ì „ë‹¬)
            self.extractor = HybridExtractor(
                vlm_service=self.vlm_service,
                pdf_path=pdf_path  # Fallbackìš©
            )
            
            # Step 1: PDF â†’ Images
            if progress_callback:
                progress_callback("PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘...", 0.1)
            
            logger.info("ğŸ“„ Step 1: PDF â†’ ì´ë¯¸ì§€ ë³€í™˜")
            images = self.pdf_processor.pdf_to_images(
                pdf_path=pdf_path,
                max_pages=max_pages,
                dpi=300
            )
            
            total_pages = len(images)
            logger.info(f"   âœ… {total_pages}í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")
            
            # Step 2: í˜ì´ì§€ë³„ HybridExtractor ì²˜ë¦¬
            page_results = []
            kvs_files = []
            metrics_list = []
            empty_page_count = 0
            
            for i, image_tuple in enumerate(images):
                # âœ… Phase 5.7.6.1: íŠœí”Œ ì–¸íŒ¨í‚¹ (base64_str, page_num)
                image_data, page_num = image_tuple
                
                progress = 0.1 + (0.7 * (i / max(1, total_pages)))
                
                if progress_callback:
                    progress_callback(
                        f"í˜ì´ì§€ {page_num}/{total_pages} ì²˜ë¦¬ ì¤‘...",
                        progress
                    )
                
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page_num}/{total_pages} ì²˜ë¦¬ ì‹œì‘")
                
                # âœ… Phase 5.7.6: HybridExtractor v5.7.6 í˜¸ì¶œ (pypdf Fallback)
                result = self.extractor.extract(image_data, page_num=page_num)
                
                # âœ… ë¹ˆ í˜ì´ì§€ ê°ì§€
                if result.get('is_empty', False):
                    empty_page_count += 1
                    logger.info(f"   â„¹ï¸ í˜ì´ì§€ {page_num}: ë¹ˆ í˜ì´ì§€ Skip")
                    continue  # DoD æ¯ìˆ˜ì—ì„œ ì œì™¸
                
                # í˜ì´ì§€ ê²°ê³¼ ìˆ˜ì§‘
                page_results.append({
                    'page_num': page_num,
                    'content': result['content'],
                    'doc_type': result.get('doc_type', 'unknown'),
                    'confidence': result.get('confidence', 0.0),
                    'quality_score': result.get('quality_score', 0.0),
                    'source': result.get('source', 'vlm')  # âœ… ì¶œì²˜ ì¶”ì 
                })
                
                # KVS ì €ì¥
                if result.get('kvs'):
                    kvs_file = f"kvs_page_{page_num}.json"
                    kvs_files.append(kvs_file)
                    # ì‹¤ì œ ì €ì¥ì€ storageê°€ ìˆì„ ë•Œë§Œ
                    if self.storage:
                        self.storage.save_json(kvs_file, result['kvs'])
                
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics_list.append(result['metrics'])
                
                logger.info(f"   âœ… í˜ì´ì§€ {page_num} ì™„ë£Œ: í’ˆì§ˆ {result['quality_score']:.0f}/100 (ì¶œì²˜: {result['source']})")
            
            valid_pages = len(page_results)
            
            logger.info(f"ğŸ“Š ìœ íš¨ í˜ì´ì§€: {valid_pages}/{total_pages} (ë¹ˆ í˜ì´ì§€ {empty_page_count}ê°œ ì œì™¸)")
            
            # âœ… Phase 5.7.6: Fallback í†µê³„ ìˆ˜ì§‘
            fallback_stats = self.extractor.get_fallback_stats()
            logger.info(f"ğŸ“Š Fallback í†µê³„:")
            logger.info(f"   - VLM ì„±ê³µ: {fallback_stats['vlm_success_count']}í˜ì´ì§€")
            logger.info(f"   - Fallback ì‚¬ìš©: {fallback_stats['fallback_count']}í˜ì´ì§€")
            logger.info(f"   - Fallback ë¹„ìœ¨: {fallback_stats['fallback_rate']:.1%}")
            
            # Step 3: Markdown í†µí•©
            if progress_callback:
                progress_callback("Markdown í†µí•© ì¤‘...", 0.8)
            
            logger.info("ğŸ“ Step 3: Markdown í†µí•©")
            markdown_pages = [p['content'] for p in page_results]
            markdown = "\n\n".join(markdown_pages)
            
            logger.info(f"   âœ… Markdown í†µí•© ì™„ë£Œ: {len(markdown)} ê¸€ì")
            
            # Step 4: SemanticChunking v5.7.4.1
            if progress_callback:
                progress_callback("ì¡°ë¬¸ ê²½ê³„ ê¸°ë°˜ ì²­í‚¹ ì¤‘...", 0.9)
            
            logger.info("âœ‚ï¸ Step 4: SemanticChunking v5.7.4.1 (ì¡°ë¬¸ ê²½ê³„)")
            chunks = self.chunker.chunk(markdown)
            
            logger.info(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            # Step 5: 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€
            if progress_callback:
                progress_callback("í’ˆì§ˆ í‰ê°€ ì¤‘...", 0.95)
            
            logger.info("ğŸ“Š Step 5: ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€")
            
            # 1. ì›ë³¸ ì¶©ì‹¤ë„
            avg_confidence = statistics.mean([p['confidence'] for p in page_results]) if page_results else 0.0
            fidelity_score = avg_confidence * 100
            
            # 2. ì²­í‚¹ í’ˆì§ˆ
            avg_chunk_size = statistics.mean([len(c['content']) for c in chunks]) if chunks else 0
            # ëª©í‘œ: 600~1200ì, ìµœì : 900ì
            if 600 <= avg_chunk_size <= 1200:
                chunking_score = 100.0
            elif 400 <= avg_chunk_size < 600:
                chunking_score = 70.0
            elif avg_chunk_size < 400:
                chunking_score = max(30.0, avg_chunk_size / 400 * 70)
            else:
                chunking_score = max(70.0, 100 - (avg_chunk_size - 1200) / 20)
            
            # 3. RAG ì í•©ë„
            rag_score = min(len(chunks) / max(1, valid_pages) * 100, 100)
            
            # 4. ë²”ìš©ì„±
            universality_score = 95.0
            
            # 5. ê²½ìŸë ¥
            competitive_score = (fidelity_score + chunking_score + rag_score) / 3
            
            # ì¢…í•©
            overall_score = (
                fidelity_score * 0.3 +
                chunking_score * 0.2 +
                rag_score * 0.2 +
                universality_score * 0.15 +
                competitive_score * 0.15
            )
            
            logger.info(f"   âœ… ì›ë³¸ ì¶©ì‹¤ë„: {fidelity_score:.0f}/100")
            logger.info(f"   âœ… ì²­í‚¹ í’ˆì§ˆ: {chunking_score:.0f}/100 (í‰ê· : {avg_chunk_size:.0f}ì)")
            logger.info(f"   âœ… RAG ì í•©ë„: {rag_score:.0f}/100")
            logger.info(f"   âœ… ë²”ìš©ì„±: {universality_score:.0f}/100")
            logger.info(f"   âœ… ê²½ìŸë ¥: {competitive_score:.0f}/100")
            logger.info(f"   ğŸ¯ ì¢…í•©: {overall_score:.0f}/100")
            
            # ì™„ë£Œ
            processing_time = time.time() - start_time
            
            result = {
                'status': 'success',
                'version': '5.7.6.1',  # âœ… Phase 5.7.6.1
                'session_id': session_id,
                'pages_total': total_pages,
                'pages_success': valid_pages,
                'empty_page_count': empty_page_count,
                'processing_time': processing_time,
                'markdown': markdown,
                'chunks': chunks,
                'kvs_payloads': kvs_files,
                'metrics': metrics_list,
                'fallback_stats': fallback_stats,  # âœ… Fallback í†µê³„
                'fidelity_score': fidelity_score,
                'chunking_score': chunking_score,
                'rag_score': rag_score,
                'universality_score': universality_score,
                'competitive_score': competitive_score,
                'overall_score': overall_score
            }
            
            logger.info(f"âœ… Phase 5.7.6.1 ì²˜ë¦¬ ì™„ë£Œ")
            logger.info(f"   - ìœ íš¨ í˜ì´ì§€: {valid_pages}/{total_pages}")
            logger.info(f"   - ë¹ˆ í˜ì´ì§€: {empty_page_count}")
            logger.info(f"   - Fallback ì‚¬ìš©: {fallback_stats['fallback_count']}")
            logger.info(f"   - ì‹œê°„: {processing_time:.1f}ì´ˆ")
            logger.info(f"   - ì¢…í•©: {overall_score:.0f}/100")
            
            return result
        
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            return {
                'status': 'error',
                'version': '5.7.6.1',
                'session_id': session_id,
                'error': str(e),
                'pages_total': 0,
                'pages_success': 0,
                'empty_page_count': 0,
                'fallback_stats': {
                    'vlm_success_count': 0,
                    'fallback_count': 0,
                    'total_pages': 0,
                    'fallback_rate': 0.0
                },
                'processing_time': time.time() - start_time
            }