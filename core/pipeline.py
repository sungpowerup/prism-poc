"""
core/pipeline_v50.py
PRISM Phase 5.0 - Pipeline (ì™„ì „ ë²”ìš© ë¬¸ì„œ ì²˜ë¦¬)

âœ… Phase 5.0 í•µì‹¬:
1. ë¬¸ì„œ íƒ€ì… ìë™ ì¸ì‹ + íƒ€ì…ë³„ ì „ëµ ìë™ ì ìš©
2. 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¤€ìˆ˜
3. ì›ë³¸ ì¶©ì‹¤ë„ 95% + ì²­í‚¹ í’ˆì§ˆ 90% + RAG ìµœì í™” 95%

Author: ì´ì„œì˜ (Backend Lead)
Date: 2025-10-24
Version: 5.0
"""

import logging
from typing import List, Dict, Any, Optional
import time
import uuid
import re

logger = logging.getLogger(__name__)


class Phase50Pipeline:
    """
    Phase 5.0 ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    
    íŠ¹ì§•:
    - ì™„ì „ ë²”ìš© ì„¤ê³„ (ëª¨ë“  ë¬¸ì„œ íƒ€ì… ì§€ì›)
    - 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¤€ìˆ˜
    - í•˜ë“œì½”ë”© ì œë¡œ
    """
    
    def __init__(self, pdf_processor, vlm_service, storage):
        """
        Args:
            pdf_processor: PDFProcessor ì¸ìŠ¤í„´ìŠ¤
            vlm_service: VLMServiceV50 ì¸ìŠ¤í„´ìŠ¤
            storage: Storage ì¸ìŠ¤í„´ìŠ¤
        """
        self.pdf_processor = pdf_processor
        self.vlm_service = vlm_service
        self.storage = storage
    
    def process_pdf(
        self,
        pdf_path: str,
        max_pages: int = 20,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        PDF ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ (Phase 5.0)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        session_id = str(uuid.uuid4())[:8]
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ PRISM Phase 5.0 - ë²”ìš© ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"{'='*80}")
        logger.info(f"ğŸ“„ íŒŒì¼: {pdf_path}")
        logger.info(f"ğŸ†” Session ID: {session_id}")
        logger.info(f"ğŸ“Š ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        logger.info(f"{'='*80}\n")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 1: PDF â†’ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³€í™˜ (300 DPI)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if progress_callback:
            progress_callback("ğŸ“„ PDF ë³€í™˜ ì¤‘ (300 DPI ê³ í•´ìƒë„)...", 0)
        
        logger.info("[Stage 1] PDF â†’ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³€í™˜ (300 DPI)")
        images = self.pdf_processor.pdf_to_images(pdf_path, max_pages=max_pages, dpi=300)
        logger.info(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ\n")
        
        if not images:
            logger.error("âŒ PDF ë³€í™˜ ì‹¤íŒ¨")
            return {
                'status': 'error',
                'error': 'PDF ë³€í™˜ ì‹¤íŒ¨',
                'session_id': session_id
            }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 2: Phase 5.0 ë²”ìš© ë¶„ì„ (ë¬¸ì„œ íƒ€ì… ìë™ íŒë³„)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        logger.info("[Stage 2] Phase 5.0 ë²”ìš© ë¶„ì„ ì‹œì‘")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        results = []
        success_count = 0
        error_count = 0
        
        doc_type_counts = {}
        
        for page_num, img_data in enumerate(images):
            if progress_callback:
                progress = int(5 + (page_num / len(images)) * 85)
                progress_callback(
                    f"ğŸ¯ í˜ì´ì§€ {page_num + 1}/{len(images)} ë²”ìš© ë¶„ì„ ì¤‘...",
                    progress
                )
            
            logger.info(f"ğŸ“„ í˜ì´ì§€ {page_num + 1}/{len(images)} ì²˜ë¦¬ ì‹œì‘")
            
            try:
                # Phase 5.0: ë²”ìš© ë¶„ì„
                vlm_result = self.vlm_service.analyze_page_v50(
                    image_data=img_data,
                    page_num=page_num + 1
                )
                
                content = vlm_result.get('content', '')
                confidence = vlm_result.get('confidence', 0.0)
                doc_type = vlm_result.get('doc_type', 'mixed')
                subtype = vlm_result.get('subtype', 'unknown')
                quality_score = vlm_result.get('quality_score', 0.0)
                
                if not content or len(content) < 20:
                    logger.warning(f"   âš ï¸ VLM ê²°ê³¼ ë¶€ì¡±: {len(content)} ê¸€ì")
                    error_count += 1
                    continue
                
                # ì„±ê³µ!
                success_count += 1
                doc_type_counts[doc_type] = doc_type_counts.get(doc_type, 0) + 1
                
                logger.info(f"   âœ… ì„±ê³µ!")
                logger.info(f"      - íƒ€ì…: {doc_type} ({subtype})")
                logger.info(f"      - ì‹ ë¢°ë„: {confidence:.2f}")
                logger.info(f"      - í’ˆì§ˆ: {quality_score:.1f}/100")
                logger.info(f"      - ê¸€ì ìˆ˜: {len(content):,}")
                logger.info("")
                
                results.append({
                    'page_num': page_num + 1,
                    'content': content,
                    'confidence': confidence,
                    'doc_type': doc_type,
                    'subtype': subtype,
                    'strategy': vlm_result.get('strategy', 'unknown'),
                    'quality_score': quality_score,
                    'structure': vlm_result.get('structure', {})
                })
                
            except Exception as e:
                logger.error(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\n")
                error_count += 1
        
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info(f"âœ… Phase 5.0 ë¶„ì„ ì™„ë£Œ:")
        logger.info(f"   - ì„±ê³µ: {success_count}/{len(images)}ê°œ")
        logger.info(f"   - ì‹¤íŒ¨: {error_count}/{len(images)}ê°œ")
        logger.info(f"   - ë¬¸ì„œ íƒ€ì… ë¶„í¬: {doc_type_counts}")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        if success_count == 0:
            logger.error("âŒ ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨")
            return {
                'status': 'error',
                'error': 'ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨',
                'session_id': session_id
            }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 3: Markdown í†µí•© (ì§€ëŠ¥í˜• ì²­í‚¹)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if progress_callback:
            progress_callback("ğŸ“ Markdown í†µí•© ì¤‘...", 95)
        
        logger.info("[Stage 3] Markdown í†µí•© (ì§€ëŠ¥í˜• ì²­í‚¹)")
        
        full_markdown = self._generate_markdown_with_chunking(results)
        
        logger.info(f"âœ… Markdown ìƒì„± ì™„ë£Œ")
        logger.info(f"   - ì´ ê¸€ì ìˆ˜: {len(full_markdown):,}")
        logger.info(f"   - ì„¹ì…˜ ìˆ˜: {full_markdown.count('---')}")
        logger.info("")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 4: 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        logger.info("[Stage 4] 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„")
        quality_metrics = self._analyze_quality_checklist(results, full_markdown)
        
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("ğŸ“Š 5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"   âœ… 1. ì›ë³¸ ì¶©ì‹¤ë„:  {quality_metrics['fidelity_score']:.1f}/100")
        logger.info(f"   âœ… 2. ì²­í‚¹ í’ˆì§ˆ:    {quality_metrics['chunking_score']:.1f}/100")
        logger.info(f"   âœ… 3. RAG ì í•©ë„:   {quality_metrics['rag_score']:.1f}/100")
        logger.info(f"   âœ… 4. ë²”ìš©ì„±:       {quality_metrics['universality_score']:.1f}/100")
        logger.info(f"   âœ… 5. ê²½ìŸì‚¬ ëŒ€ë¹„:  {quality_metrics['competitive_score']:.1f}/100")
        logger.info(f"   ğŸ¯ ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {quality_metrics['overall_score']:.1f}/100")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Stage 5: ê²°ê³¼ ì €ì¥
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if progress_callback:
            progress_callback("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...", 98)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        result = {
            'status': 'success',
            'session_id': session_id,
            'version': '5.0',
            'processing_time': processing_time,
            'pages_total': len(images),
            'pages_success': success_count,
            'pages_error': error_count,
            'strategy': 'universal_v50',
            'doc_type_counts': doc_type_counts,
            'markdown': full_markdown,
            'page_results': results,
            **quality_metrics
        }
        
        # DB ì €ì¥
        try:
            if hasattr(self.storage, 'save_session'):
                self.storage.save_session(result)
                logger.info("âœ… DB ì €ì¥ ì™„ë£Œ\n")
        except Exception as e:
            logger.error(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨: {e}\n")
        
        if progress_callback:
            progress_callback("âœ… ì™„ë£Œ!", 100)
        
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("ğŸ‰ PRISM Phase 5.0 ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
        logger.info(f"   ğŸ“„ ì„±ê³µ: {success_count}/{len(images)}ê°œ")
        logger.info(f"   ğŸ¯ ì¢…í•© í’ˆì§ˆ: {quality_metrics['overall_score']:.1f}/100")
        logger.info(f"   ğŸ“Š ì´ ê¸€ì: {len(full_markdown):,}")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        return result
    
    def _generate_markdown_with_chunking(self, results: List[Dict[str, Any]]) -> str:
        """
        í˜ì´ì§€ë³„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ Markdownìœ¼ë¡œ í†µí•© (ì§€ëŠ¥í˜• ì²­í‚¹)
        
        âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸ 2ë²ˆ: ì§€ëŠ¥í˜• ì²­í‚¹
        - í˜ì´ì§€ êµ¬ë¶„: `---`
        - ì„¹ì…˜ í—¤ë”: `##`
        """
        markdown_parts = []
        
        for i, result in enumerate(results):
            content = result['content']
            page_num = result['page_num']
            doc_type = result.get('doc_type', 'mixed')
            
            # í˜ì´ì§€ í—¤ë” (ë©”íƒ€ ì •ë³´ ìµœì†Œí™”)
            markdown_parts.append(f"<!-- í˜ì´ì§€ {page_num} ({doc_type}) -->\n\n")
            
            # ë‚´ìš©
            markdown_parts.append(content)
            
            # í˜ì´ì§€ êµ¬ë¶„ (ë§ˆì§€ë§‰ í˜ì´ì§€ ì œì™¸)
            if i < len(results) - 1:
                markdown_parts.append("\n\n---\n\n")
        
        return "".join(markdown_parts).strip()
    
    def _analyze_quality_checklist(self, results: List[Dict], markdown: str) -> Dict[str, float]:
        """
        5ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
        
        1. ì›ë³¸ ì¶©ì‹¤ë„ 95% ëª©í‘œ
        2. ì²­í‚¹ í’ˆì§ˆ 90% ëª©í‘œ
        3. RAG ì í•©ë„ 95% ëª©í‘œ
        4. ë²”ìš©ì„± 100% ëª©í‘œ
        5. ê²½ìŸì‚¬ ëŒ€ë¹„ 95% ëª©í‘œ
        """
        
        if not results:
            return {
                'fidelity_score': 0.0,
                'chunking_score': 0.0,
                'rag_score': 0.0,
                'universality_score': 0.0,
                'competitive_score': 0.0,
                'overall_score': 0.0
            }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 1. ì›ë³¸ ì¶©ì‹¤ë„ (Fidelity Score)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        fidelity_score = self._calculate_fidelity_score(markdown, results)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 2. ì²­í‚¹ í’ˆì§ˆ (Chunking Quality)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        chunking_score = self._calculate_chunking_quality(markdown, results)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 3. RAG ì í•©ë„ (RAG Suitability)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        rag_score = self._calculate_rag_suitability(markdown)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 4. ë²”ìš©ì„± (Universality)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        universality_score = self._calculate_universality(results)
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # âœ… 5. ê²½ìŸì‚¬ ëŒ€ë¹„ (Competitive Score)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        competitive_score = self._calculate_competitive_score(
            fidelity_score, chunking_score, rag_score, universality_score
        )
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì¢…í•© ì ìˆ˜
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        overall_score = (
            fidelity_score * 0.25 +      # ì›ë³¸ ì¶©ì‹¤ë„ 25%
            chunking_score * 0.20 +      # ì²­í‚¹ í’ˆì§ˆ 20%
            rag_score * 0.20 +           # RAG ì í•©ë„ 20%
            universality_score * 0.20 +  # ë²”ìš©ì„± 20%
            competitive_score * 0.15     # ê²½ìŸì‚¬ ëŒ€ë¹„ 15%
        )
        
        return {
            'fidelity_score': fidelity_score,
            'chunking_score': chunking_score,
            'rag_score': rag_score,
            'universality_score': universality_score,
            'competitive_score': competitive_score,
            'overall_score': min(100.0, overall_score)
        }
    
    def _calculate_fidelity_score(self, markdown: str, results: List[Dict]) -> float:
        """
        âœ… 1. ì›ë³¸ ì¶©ì‹¤ë„ ê³„ì‚°
        
        í‰ê°€ ê¸°ì¤€:
        - ìµœì†Œ ê¸¸ì´ ì¶©ì¡±
        - êµ¬ì¡° í—¤ë” ì¡´ì¬
        - í˜ì´ì§€ë³„ ê· í˜•
        - ì‹ ë¢°ë„
        """
        score = 100.0
        
        # 1. ìµœì†Œ ê¸¸ì´ ì²´í¬
        if len(markdown) < 100:
            score -= 40
        elif len(markdown) < 500:
            score -= 20
        elif len(markdown) >= 1000:
            score += 5  # ë³´ë„ˆìŠ¤
        
        # 2. êµ¬ì¡° í—¤ë” ì¡´ì¬
        headers = re.findall(r'^#+\s+', markdown, re.MULTILINE)
        header_count = len(headers)
        
        if header_count == 0:
            score -= 25
        elif header_count >= 1 and header_count < 3:
            score -= 10
        elif header_count >= 5:
            score += 10  # ë³´ë„ˆìŠ¤
        
        # 3. í˜ì´ì§€ë³„ ë‚´ìš© ê· í˜•
        page_lengths = [len(r.get('content', '')) for r in results]
        if page_lengths and len(page_lengths) > 1:
            avg_len = sum(page_lengths) / len(page_lengths)
            if avg_len > 0:
                variance = sum((l - avg_len) ** 2 for l in page_lengths) / len(page_lengths)
                std_dev = variance ** 0.5
                cv = std_dev / avg_len  # ë³€ë™ê³„ìˆ˜
                
                if cv < 0.3:
                    score += 15  # ë§¤ìš° ê· í˜•ì¡í˜
                elif cv < 0.5:
                    score += 10
                elif cv > 1.0:
                    score -= 10  # ë¶ˆê· í˜•
        
        # 4. í‰ê·  ì‹ ë¢°ë„
        confidences = [r.get('confidence', 0.0) for r in results]
        if confidences:
            avg_confidence = sum(confidences) / len(confidences)
            score += avg_confidence * 10  # ìµœëŒ€ +10ì 
        
        return max(0.0, min(100.0, score))
    
    def _calculate_chunking_quality(self, markdown: str, results: List[Dict]) -> float:
        """
        âœ… 2. ì²­í‚¹ í’ˆì§ˆ ê³„ì‚°
        
        í‰ê°€ ê¸°ì¤€:
        - í˜ì´ì§€ êµ¬ë¶„ (`---`)
        - ì„¹ì…˜ í—¤ë” (`##`)
        - ê· í˜•ì¡íŒ ì„¹ì…˜ í¬ê¸°
        """
        score = 0.0
        
        # 1. í˜ì´ì§€ êµ¬ë¶„ (`---`)
        page_separators = markdown.count('---')
        expected_separators = len(results) - 1  # ë§ˆì§€ë§‰ í˜ì´ì§€ ì œì™¸
        
        if page_separators >= expected_separators:
            score += 35  # ì™„ë²½í•œ í˜ì´ì§€ êµ¬ë¶„
        elif page_separators >= expected_separators * 0.8:
            score += 25
        elif page_separators >= expected_separators * 0.5:
            score += 15
        
        # 2. ì„¹ì…˜ í—¤ë” (`##`)
        headers = re.findall(r'^##\s+(.+)$', markdown, re.MULTILINE)
        header_count = len(headers)
        
        if header_count >= len(results) * 2:
            score += 35  # í˜ì´ì§€ë‹¹ 2ê°œ ì´ìƒ
        elif header_count >= len(results):
            score += 25  # í˜ì´ì§€ë‹¹ 1ê°œ ì´ìƒ
        elif header_count >= len(results) * 0.5:
            score += 15
        
        # 3. ê· í˜•ì¡íŒ ì„¹ì…˜ í¬ê¸°
        sections = markdown.split('---')
        if len(sections) > 1:
            section_lengths = [len(s.strip()) for s in sections if s.strip()]
            if section_lengths:
                avg_len = sum(section_lengths) / len(section_lengths)
                if avg_len > 0:
                    variance = sum((l - avg_len) ** 2 for l in section_lengths) / len(section_lengths)
                    std_dev = variance ** 0.5
                    cv = std_dev / avg_len
                    
                    if cv < 0.3:
                        score += 30  # ë§¤ìš° ê· í˜•ì¡í˜
                    elif cv < 0.5:
                        score += 20
                    elif cv < 0.8:
                        score += 10
        
        return min(100.0, score)
    
    def _calculate_rag_suitability(self, markdown: str) -> float:
        """
        âœ… 3. RAG ì í•©ë„ ê³„ì‚°
        
        í‰ê°€ ê¸°ì¤€:
        - ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì—†ìŒ
        - ì¤‘ë³µ ì œê±°
        - êµ¬ì¡°í™”ëœ ë°ì´í„°
        """
        score = 100.0
        
        # 1. ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì²´í¬ (ê°ì )
        meta_keywords = [
            'ì´ ë¬¸ì„œëŠ”', 'ë‹¤ìŒê³¼ ê°™ì´', 'ì•„ë˜ì™€ ê°™ì´',
            'ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤', 'í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤',
            'ì´ í˜ì´ì§€', 'ë¬¸ì„œ ìƒë‹¨', 'ë¬¸ì„œ í•˜ë‹¨',
            'ìœ„ì—ì„œ ì–¸ê¸‰í•œ', 'ì•„ë˜ì—ì„œ ì„¤ëª…í• '
        ]
        
        meta_count = 0
        for keyword in meta_keywords:
            meta_count += markdown.count(keyword)
        
        score -= meta_count * 3  # ê°œë‹¹ -3ì 
        
        # 2. ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ì²´í¬ (ê°ì )
        lines = markdown.split('\n')
        line_counts = {}
        for line in lines:
            clean = line.strip()
            if len(clean) > 15:  # ì§§ì€ ì¤„ ì œì™¸
                line_counts[clean] = line_counts.get(clean, 0) + 1
        
        duplicates = sum(1 for count in line_counts.values() if count >= 3)
        score -= duplicates * 5  # ì¤‘ë³µ ì¤„ë‹¹ -5ì 
        
        # 3. êµ¬ì¡°í™”ëœ ë°ì´í„° (ê°€ì‚°ì )
        has_table = '|' in markdown and markdown.count('|') >= 6
        has_list = re.search(r'^\d+\.\s+', markdown, re.MULTILINE) is not None
        has_bullet = re.search(r'^[-*]\s+', markdown, re.MULTILINE) is not None
        
        if has_table:
            score += 10
        if has_list:
            score += 5
        if has_bullet:
            score += 5
        
        return max(0.0, min(100.0, score))
    
    def _calculate_universality(self, results: List[Dict]) -> float:
        """
        âœ… 4. ë²”ìš©ì„± ê³„ì‚°
        
        í‰ê°€ ê¸°ì¤€:
        - ë‹¤ì–‘í•œ ë¬¸ì„œ íƒ€ì… ì²˜ë¦¬
        - íƒ€ì…ë³„ ì „ëµ ì ìš©
        - í•˜ë“œì½”ë”© ì—†ìŒ
        """
        score = 100.0
        
        # 1. ë¬¸ì„œ íƒ€ì… ë‹¤ì–‘ì„±
        doc_types = set(r.get('doc_type', 'mixed') for r in results)
        type_diversity = len(doc_types)
        
        if type_diversity >= 3:
            score += 10  # ë§¤ìš° ë‹¤ì–‘
        elif type_diversity >= 2:
            score += 5
        
        # 2. íƒ€ì…ë³„ ì „ëµ ì ìš© í™•ì¸
        strategies_used = set(r.get('strategy', '') for r in results)
        if 'universal_v50' in ' '.join(strategies_used):
            score += 10  # Phase 5.0 ì „ëµ ì‚¬ìš©
        
        # 3. í•˜ë“œì½”ë”© ì²´í¬ (ë²„ìŠ¤ ì „ìš© í‚¤ì›Œë“œ)
        hardcoded_keywords = [
            'ì¼ë°˜ë²„ìŠ¤', 'ê´‘ì—­ë²„ìŠ¤', 'ë§ˆì„ë²„ìŠ¤',
            'ë°°ì°¨ê°„ê²©', 'ì²«ì°¨', 'ë§‰ì°¨'
        ]
        
        all_content = ' '.join(r.get('content', '') for r in results)
        
        # ë¬¸ì„œ íƒ€ì…ì´ 'diagram/transport_route'ì¼ ë•Œë§Œ í—ˆìš©
        is_transport = any(
            r.get('doc_type') == 'diagram' and 
            r.get('subtype') == 'transport_route'
            for r in results
        )
        
        if not is_transport:
            for keyword in hardcoded_keywords:
                if keyword in all_content:
                    score -= 15  # í•˜ë“œì½”ë”© ë°œê²¬! (í° ê°ì )
        
        return max(0.0, min(100.0, score))
    
    def _calculate_competitive_score(
        self,
        fidelity: float,
        chunking: float,
        rag: float,
        universality: float
    ) -> float:
        """
        âœ… 5. ê²½ìŸì‚¬ ëŒ€ë¹„ ì ìˆ˜ ê³„ì‚°
        
        ê²½ìŸì‚¬ ê¸°ì¤€:
        - ì›ë³¸ ì¶©ì‹¤ë„: 85ì 
        - ì²­í‚¹ í’ˆì§ˆ: 80ì 
        - RAG ì í•©ë„: 90ì 
        - ë²”ìš©ì„±: 70ì 
        """
        competitor_baseline = {
            'fidelity': 85.0,
            'chunking': 80.0,
            'rag': 90.0,
            'universality': 70.0
        }
        
        # ê° í•­ëª©ë³„ ê²½ìŸì‚¬ ëŒ€ë¹„ ì ìˆ˜
        fidelity_ratio = (fidelity / competitor_baseline['fidelity']) * 100
        chunking_ratio = (chunking / competitor_baseline['chunking']) * 100
        rag_ratio = (rag / competitor_baseline['rag']) * 100
        universality_ratio = (universality / competitor_baseline['universality']) * 100
        
        # í‰ê· 
        avg_ratio = (fidelity_ratio + chunking_ratio + rag_ratio + universality_ratio) / 4
        
        # 95% ì´ìƒì´ë©´ ë§Œì 
        if avg_ratio >= 95:
            return 100.0
        else:
            return min(100.0, avg_ratio)