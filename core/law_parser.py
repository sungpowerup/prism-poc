# law_parser.py - Phase 0.9.2
# 
# Phase 0.9.2 ìˆ˜ì •ì‚¬í•­:
# 1. âœ… Chapter ìœ„ì¹˜ ì¬ë°°ì¹˜ (Critical Fix)
#    - Chapter ì²­í¬ë¥¼ í•´ë‹¹ ì¥ì˜ ì²« article ì•ì— ë°°ì¹˜
#    - engine.md, chunks.json, review.md êµ¬ì¡° ì¼ì¹˜
# 2. âœ… ì œ5ì¡° 1í•­ ë²ˆí˜¸ ë³µêµ¬ (Data Loss Fix)
#    - "."ì§ìœ„"ë€1ëª… â†’ 1. "ì§ìœ„"ë€ 1ëª…
# 3. âœ… Article ë³¸ë¬¸ ì •ë¦¬ (Phase 0.8.7 ìœ ì§€)

"""
law_parser.py - PRISM LawParser

ë²•ë¥ /ê·œì • ë¬¸ì„œ íŒŒì‹±

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€  
Date: 2025-11-20
Version: Phase 0.9.2
"""

import re
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# Annex SubChunker Import (Phase 0.8)
try:
    from core.annex_subchunker import AnnexSubChunker, validate_subchunks
    ANNEX_SUBCHUNKING_AVAILABLE = True
    logger.info("âœ… AnnexSubChunker ë¡œë“œ ì„±ê³µ (Phase 0.8)")
except ImportError:
    ANNEX_SUBCHUNKING_AVAILABLE = False
    logger.warning("âš ï¸ AnnexSubChunker ë¯¸ì„¤ì¹˜ - Annex ë‹¨ì¼ ì²­í¬ ëª¨ë“œ")


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: str
    body: str
    chapter_number: str
    section_order: int


@dataclass
class Chapter:
    """ì¥ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: str
    section_order: int


class LawParser:
    """
    ë²•ë¥ /ê·œì • ë¬¸ì„œ íŒŒì„œ
    
    Phase 0.9.2:
    - âœ… Chapter ìœ„ì¹˜ ì¬ë°°ì¹˜ (GPT Critical Fix)
    - âœ… ì œ5ì¡° 1í•­ ë²ˆí˜¸ ë³µêµ¬
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… LawParser v0.9.2 ì´ˆê¸°í™” (Chapter Position Fix)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = True
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ íŒŒì‹±
        """
        logger.info(f"ğŸ“œ LawParser íŒŒì‹± ì‹œì‘: {document_title}")
        
        # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        cleaned_text = pdf_text
        
        if clean_artifacts:
            cleaned_text = self._clean_page_artifacts(cleaned_text)
            logger.info("   âœ… í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° ì™„ë£Œ")
        
        if normalize_linebreaks:
            cleaned_text = self._normalize_linebreaks(cleaned_text)
            logger.info("   âœ… ê°œí–‰ ì •ê·œí™” ì™„ë£Œ")
        
        # 2. ë¬¸ì„œ íƒ€ì… íŒë³„
        has_articles = bool(re.search(r'ì œ\d+ì¡°', cleaned_text))
        has_annex_only = bool(re.search(r'\[ë³„í‘œ', cleaned_text))
        
        if has_articles:
            logger.info("   ğŸ“‹ ë¬¸ì„œ íƒ€ì…: ë²•ë ¹/ê·œì • (ì¡°ë¬¸ í¬í•¨)")
            return self._parse_legal_document(cleaned_text, document_title)
        elif has_annex_only:
            logger.info("   ğŸ“‹ ë¬¸ì„œ íƒ€ì…: Annex ì „ìš©")
            return self._parse_annex_only_document(cleaned_text, document_title)
        else:
            logger.warning("   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ íƒ€ì… - ê¸°ë³¸ ì²˜ë¦¬")
            return self._create_empty_result(document_title, cleaned_text)
    
    def _clean_page_artifacts(self, text: str) -> str:
        """í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° (Phase 0.8.6)"""
        
        # íŒ¨í„´: "ì¸ì‚¬ê·œì • 402-2" ìŠ¤íƒ€ì¼
        artifact_pattern = r'^[ê°€-í£]{2,10}\s*\d{1,5}-\d{1,3}\s*$'
        
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line_stripped = line.strip()
            
            if re.match(artifact_pattern, line_stripped):
                logger.debug(f"      ì œê±°: {line_stripped}")
                continue
            
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _normalize_linebreaks(self, text: str) -> str:
        """ê°œí–‰ ì •ê·œí™”"""
        
        # 3ì¤„ ì´ìƒ ì—°ì† ê°œí–‰ â†’ 2ì¤„ë¡œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ë¬¸ì¥ ë + ë‹¨ì¼ ê°œí–‰ â†’ ìœ ì§€
        # (ì¡°ë¬¸ ë³¸ë¬¸ ë‚´ ìì—°ìŠ¤ëŸ¬ìš´ ê°œí–‰ ë³´ì¡´)
        
        return text
    
    def _parse_legal_document(self, cleaned_text: str, document_title: str) -> Dict[str, Any]:
        """ë²•ë ¹/ê·œì • ë¬¸ì„œ íŒŒì‹±"""
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ (Phase 0.8.6)
        amendment_history = self._extract_amendment_history(cleaned_text)
        logger.info(f"   âœ… ê°œì •ì´ë ¥: {len(amendment_history)}ê±´")
        
        # TreeBuilder ê²°ê³¼ë¥¼ ë³€í™˜
        from core.tree_builder import TreeBuilder
        
        tree_builder = TreeBuilder()
        tree_doc = tree_builder.build(
            markdown=cleaned_text,
            document_title=document_title
        )
        
        # Tree â†’ Article ë³€í™˜
        chapters = []
        articles = []
        
        tree_nodes = tree_doc['document']['tree']
        
        # Chapter ì¶”ì¶œ
        seen_chapters = {}
        for node in tree_nodes:
            if node.get('level') == 'article':
                chapter_info = node.get('chapter', '')
                if chapter_info and chapter_info not in seen_chapters:
                    chapter_match = re.match(r'(ì œ\d+ì¥)\s*(.+)?', chapter_info)
                    if chapter_match:
                        chapter_num = chapter_match.group(1)
                        chapter_title = chapter_match.group(2) or ""
                        
                        chapters.append(Chapter(
                            number=chapter_num,
                            title=chapter_title.strip(),
                            section_order=len(chapters)
                        ))
                        
                        seen_chapters[chapter_info] = chapter_num
        
        # Article ì¶”ì¶œ
        for node in tree_nodes:
            if node.get('level') == 'article':
                article_no = node.get('article_no', '')
                article_title = node.get('article_title', '')
                article_body = node.get('content', '')
                chapter_info = node.get('chapter', '')
                
                # Chapter number ë§¤í•‘
                chapter_number = seen_chapters.get(chapter_info, '')
                
                articles.append(Article(
                    number=article_no,
                    title=article_title,
                    body=article_body,
                    chapter_number=chapter_number,
                    section_order=len(articles)
                ))
        
        logger.info(f"   âœ… ì¡°ë¬¸ íŒŒì‹±: {len(articles)}ê°œ")
        logger.info(f"   âœ… ì¥ íŒŒì‹±: {len(chapters)}ê°œ")
        
        # Annex ì¶”ì¶œ
        parsed_result = {
            'document_title': document_title,
            'chapters': chapters,
            'articles': articles,
            'basic_spirit': '',
            'amendment_history': amendment_history,
            'annex_content': None,
            'annex_no': None,
            'annex_title': None,
            'related_article': None,
            'annex_tables': [],
            'total_chapters': len(chapters),
            'total_articles': len(articles)
        }
        
        self._apply_annex_extraction(cleaned_text, parsed_result)
        
        return parsed_result
    
    def _extract_amendment_history(self, text: str) -> List[str]:
        """ê°œì •ì´ë ¥ ì¶”ì¶œ (Phase 0.8.6)"""
        
        history = []
        
        # íŒ¨í„´: [ì „ë¶€ê°œì • 2017.7.14.], [ì¼ë¶€ê°œì • 2025.2.1.] ë“±
        pattern = r'\[(ì „ë¶€ê°œì •|ì¼ë¶€ê°œì •|ì œì •|ê°œì •)\s+(\d{4}\.\d{1,2}\.\d{1,2}\.?)\]'
        
        for match in re.finditer(pattern, text):
            amendment_type = match.group(1)
            amendment_date = match.group(2)
            
            # ë‚ ì§œ ì •ê·œí™” (ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œ ì œê±°)
            amendment_date = amendment_date.rstrip('.')
            
            history_item = f"{amendment_type} {amendment_date}"
            
            if history_item not in history:
                history.append(history_item)
        
        return sorted(history, reverse=True)
    
    def _parse_annex_only_document(self, cleaned_text: str, document_title: str) -> Dict[str, Any]:
        """Annex ì „ìš© ë¬¸ì„œ íŒŒì‹±"""
        
        parsed_result = {
            'document_title': document_title,
            'chapters': [],
            'articles': [],
            'basic_spirit': '',
            'amendment_history': [],
            'annex_content': None,
            'annex_no': None,
            'annex_title': None,
            'related_article': None,
            'annex_tables': [],
            'total_chapters': 0,
            'total_articles': 0
        }
        
        self._apply_annex_fallback(cleaned_text, parsed_result)
        
        return parsed_result
    
    def _create_empty_result(self, document_title: str, text: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        
        return {
            'document_title': document_title,
            'chapters': [],
            'articles': [],
            'basic_spirit': text[:500],
            'amendment_history': [],
            'annex_content': None,
            'annex_no': None,
            'annex_title': None,
            'related_article': None,
            'annex_tables': [],
            'total_chapters': 0,
            'total_articles': 0
        }
    
    def _apply_annex_fallback(self, cleaned_text: str, parsed_result: dict):
        """Annex-only ë¬¸ì„œ Fallback"""
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… Fallback Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
            
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
    
    def _apply_annex_extraction(self, cleaned_text: str, parsed_result: dict):
        """ë³¸ë¬¸+Annex í˜¼í•© ë¬¸ì„œì—ì„œ Annex ì¶”ì¶œ"""
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… í˜¼í•© ë¬¸ì„œ Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
            
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
    
    def _clean_article_body(self, body: str) -> str:
        """
        ì¡°ë¬¸ ë³¸ë¬¸ ì •ë¦¬ (Phase 0.8.7 + 0.9.2)
        
        Phase 0.9.2 ì¶”ê°€:
        - âœ… ì œ5ì¡° 1í•­ ë²ˆí˜¸ ë³µêµ¬: "."ì§ìœ„"ë€ â†’ 1. "ì§ìœ„"ë€
        """
        
        # 1. ì¥ ê¼¬ë¦¬ ì œê±° (Phase 0.8.7)
        chapter_pattern = r'ì œ\d+ì¥\s+[ê°€-í£\s]+$'
        body = re.sub(chapter_pattern, '', body, flags=re.MULTILINE)
        
        # 2. Phase 0.9.2: í•­ ë²ˆí˜¸ ë³µêµ¬
        # íŒ¨í„´: ."ë‹¨ì–´"ë€ â†’ 1. "ë‹¨ì–´"ë€
        # ë§¤ì¹­: ì¤„ ì‹œì‘ + . + " + í•œê¸€ + "
        body = re.sub(
            r'^\.(\s*"[ê°€-í£]+"\s*ë€)',
            r'1.\1',
            body,
            flags=re.MULTILINE
        )
        
        # 3. ì—°ì† ê³µë°± ì •ë¦¬
        body = re.sub(r'[ \t]+', ' ', body)
        
        # 4. ì—°ì† ê°œí–‰ ì •ë¦¬ (3ì¤„ ì´ìƒ â†’ 2ì¤„)
        body = re.sub(r'\n{3,}', '\n\n', body)
        
        return body.strip()
    
    def _clean_annex_text(self, text: str) -> str:
        """
        Annex í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° (Phase 0.8.7)
        """
        
        # Private Use Area ë¬¸ì ì œê±°
        text = re.sub(r'[\uF000-\uF8FF]', '', text)
        
        # Box drawing characters
        box_chars = 'â”€â”â”‚â”ƒâ”Œâ”â””â”˜â”œâ”¤â”¬â”´â”¼â•‹â•â•‘â•”â•—â•šâ•â• â•£â•¦â•©â•¬â– â–¡â–ªâ–«'
        for char in box_chars:
            text = text.replace(char, '')
        
        # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        
        # ì—°ì† ê°œí–‰ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
    
    def to_chunks(self, parsed_result: dict) -> list:
        """
        íŒŒì‹± ê²°ê³¼ â†’ RAG ì²­í¬ ë³€í™˜
        
        âœ… Phase 0.9.2: Chapter ìœ„ì¹˜ ì¬ë°°ì¹˜ (GPT Critical Fix)
        - Chapter ì²­í¬ë¥¼ í•´ë‹¹ ì¥ì˜ ì²« article ì•ì— insert
        - engine/chunks/review êµ¬ì¡° ì¼ì¹˜
        """
        
        chunks = []
        
        # ë¬¸ì„œ ì œëª©
        if parsed_result.get('document_title'):
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # ê°œì •ì´ë ¥ ì²­í¬
        if parsed_result.get('amendment_history'):
            history_content = "\n".join(parsed_result['amendment_history'])
            chunks.append({
                'content': history_content,
                'metadata': {
                    'type': 'amendment_history',
                    'boundary': 'amendment_history',
                    'char_count': len(history_content),
                    'amendment_count': len(parsed_result['amendment_history']),
                    'section_order': -2
                }
            })
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic_spirit',
                    'boundary': 'basic_spirit',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # âœ… Phase 0.9.2: ì¡°ë¬¸ ë¨¼ì € ì¶”ê°€, Chapter ë‚˜ì¤‘ì— insert
        article_chunks = []
        
        for article in parsed_result.get('articles', []):
            # Phase 0.9.2: ë³¸ë¬¸ ì •ë¦¬ (í•­ ë²ˆí˜¸ ë³µêµ¬ í¬í•¨)
            cleaned_body = self._clean_article_body(article.body)
            
            content = f"{article.number}({article.title})\n{cleaned_body}"
            article_chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        chunks.extend(article_chunks)
        
        # âœ… Phase 0.9.2 Critical Fix: Chapter ìœ„ì¹˜ ì¬ë°°ì¹˜
        # Strategy: ê° chapterì˜ ì²« article indexë¥¼ ì°¾ì•„ì„œ ê·¸ ì•ì— insert
        
        if parsed_result.get('chapters'):
            chapter_positions = {}  # {chapter_number: first_article_index}
            
            # 1. ê° chapterì˜ ì²« article index ì°¾ê¸°
            for idx, chunk in enumerate(chunks):
                if chunk['metadata']['type'] == 'article':
                    chapter_num = chunk['metadata'].get('chapter_number', '')
                    if chapter_num and chapter_num not in chapter_positions:
                        # title/amendment_history/basic_spirit ì´í›„ì˜ ì ˆëŒ€ index
                        chapter_positions[chapter_num] = idx
            
            # 2. Chapter ì²­í¬ë¥¼ ì—­ìˆœìœ¼ë¡œ insert (index ë³€í™” ë°©ì§€)
            chapters_sorted = sorted(
                parsed_result['chapters'],
                key=lambda ch: chapter_positions.get(ch.number, 999),
                reverse=True
            )
            
            for chapter in chapters_sorted:
                chapter_num = chapter.number
                insert_idx = chapter_positions.get(chapter_num)
                
                if insert_idx is not None:
                    content = f"{chapter_num} {chapter.title}".strip()
                    chapter_chunk = {
                        'content': content,
                        'metadata': {
                            'type': 'chapter',
                            'boundary': 'chapter',
                            'chapter_number': chapter_num,
                            'chapter_title': chapter.title,
                            'char_count': len(content),
                            'section_order': chapter.section_order
                        }
                    }
                    
                    # Insert at correct position
                    chunks.insert(insert_idx, chapter_chunk)
                    logger.debug(f"      Chapter '{chapter_num}' inserted at index {insert_idx}")
        
        # Annex ì„œë¸Œì²­í‚¹
        if parsed_result.get('annex_content') and ANNEX_SUBCHUNKING_AVAILABLE:
            logger.info("âœ… Phase 0.8: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘")
            
            subchunker = AnnexSubChunker()
            # Phase 0.8.7: Annex ë…¸ì´ì¦ˆ ì œê±°
            annex_text = self._clean_annex_text(parsed_result['annex_content'])
            
            try:
                sub_chunks = subchunker.chunk(annex_text)
                validation = validate_subchunks(sub_chunks, len(annex_text))
                
                if validation['is_valid']:
                    logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì„±ê³µ: {validation['chunk_count']}ê°œ")
                    
                    for sub in sub_chunks:
                        chunks.append({
                            'content': sub.content,
                            'metadata': {
                                'type': f"annex_{sub.section_type}",
                                'boundary': 'annex',
                                'section_id': sub.section_id,
                                'section_type': sub.section_type,
                                'char_count': sub.char_count,
                                'section_order': sub.order,
                                **sub.metadata
                            }
                        })
                else:
                    raise ValueError("ê²€ì¦ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Annex ì„œë¸Œì²­í‚¹ ì‹¤íŒ¨: {e}")
                chunks.append({
                    'content': annex_text,
                    'metadata': {
                        'type': 'annex',
                        'boundary': 'annex',
                        'char_count': len(annex_text),
                        'section_order': 1000
                    }
                })
        
        elif parsed_result.get('annex_content'):
            # Phase 0.8.7: Annex ë…¸ì´ì¦ˆ ì œê±°
            cleaned_annex = self._clean_annex_text(parsed_result['annex_content'])
            chunks.append({
                'content': cleaned_annex,
                'metadata': {
                    'type': 'annex',
                    'boundary': 'annex',
                    'char_count': len(cleaned_annex),
                    'section_order': 1000
                }
            })
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for chunk in chunks:
            ctype = chunk['metadata']['type']
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")
        
        return chunks
    
    def to_markdown(self, parsed_result: dict) -> str:
        """
        íŒŒì‹± ê²°ê³¼ â†’ Markdown ë³€í™˜
        
        Phase 0.9.2: ë³¸ë¬¸/Annex ì •ë¦¬ ì ìš©
        """
        
        lines = []
        
        # ì œëª©
        if parsed_result.get('document_title'):
            lines.append(f"# {parsed_result['document_title']}")
            lines.append("")
        
        # ê°œì •ì´ë ¥
        if parsed_result.get('amendment_history'):
            lines.append("## ê°œì • ì´ë ¥")
            lines.append("")
            for amendment in parsed_result['amendment_history']:
                lines.append(f"- {amendment}")
            lines.append("")
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            lines.append("## ê¸°ë³¸ì •ì‹ ")
            lines.append("")
            lines.append(parsed_result['basic_spirit'])
            lines.append("")
        
        # ì¥/ì¡°ë¬¸ (ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì„œ íë¦„)
        current_chapter = ""
        
        for article in parsed_result.get('articles', []):
            # ì¥ ë³€ê²½ ì‹œ ì¥ í—¤ë” ì¶œë ¥
            if article.chapter_number and article.chapter_number != current_chapter:
                current_chapter = article.chapter_number
                
                # Chapter ê°ì²´ ì°¾ê¸°
                for ch in parsed_result.get('chapters', []):
                    if ch.number == current_chapter:
                        lines.append(f"## {ch.number} {ch.title}")
                        lines.append("")
                        break
            
            # ì¡°ë¬¸
            cleaned_body = self._clean_article_body(article.body)
            lines.append(f"### {article.number}({article.title})")
            lines.append("")
            lines.append(cleaned_body)
            lines.append("")
        
        # Annex
        if parsed_result.get('annex_content'):
            cleaned_annex = self._clean_annex_text(parsed_result['annex_content'])
            lines.append("## ë³„í‘œ")
            lines.append("")
            lines.append(cleaned_annex)
            lines.append("")
        
        return '\n'.join(lines)