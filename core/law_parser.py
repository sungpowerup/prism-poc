"""
core/law_parser.py - PRISM Phase 0.8.7 Polishing
ì¶œë ¥ë¬¼ ì½ê¸° í’ˆì§ˆ ê°œì„  (íŒŒì‹± ë¡œì§ ìœ ì§€)

Phase 0.8.7 í•µì‹¬ ìˆ˜ì •:
- âœ… Article ë³¸ë¬¸ ëì˜ ì¥ í—¤ë” ì œê±° (ì œ2ì¥ì±„ìš© ì¤‘ë³µ ì œê±°)
- âœ… Annex í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ë¬¸ì ì œê±° (â–¡â–  ë“±)
- âœ… QA Summary í¬ë§· ê°œì„  (ì‚¬ëŒ ê°€ë…ì„±)

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-20
Version: Phase 0.8.7 Polishing
"""

import re
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# Annex ì„œë¸Œì²­í‚¹
try:
    from core.annex_subchunker import AnnexSubChunker, validate_subchunks
    ANNEX_SUBCHUNKING_AVAILABLE = True
except ImportError:
    ANNEX_SUBCHUNKING_AVAILABLE = False
    logger.warning("âš ï¸ AnnexSubChunker ë¯¸ì„¤ì¹˜ - Annex ì„œë¸Œì²­í‚¹ ë¹„í™œì„±í™”")

# TreeBuilder Import
try:
    from core.tree_builder import TreeBuilder
    TREE_BUILDER_AVAILABLE = True
except ImportError:
    TREE_BUILDER_AVAILABLE = False
    logger.error("âŒ TreeBuilder í•„ìˆ˜ - import ì‹¤íŒ¨")


@dataclass
class Chapter:
    """ì¥ ë°ì´í„°"""
    number: str
    title: str
    section_order: int = 0


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„°"""
    number: str
    title: str
    body: str
    chapter_number: str = ""
    section_order: int = 0


class LawParser:
    """
    Phase 0.8.7 LawParser Polishing
    
    í•µì‹¬ ìˆ˜ì •:
    - íŒŒì‹± ë¡œì§ ìœ ì§€
    - ì¶œë ¥ë¬¼ ì½ê¸° í’ˆì§ˆ ê°œì„ 
    """
    
    # âœ… Phase 0.8.6: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ íŒ¨í„´
    PAGE_ARTIFACT_PATTERNS = [
        re.compile(r'\nì¸ì‚¬ê·œì •\n\d{3}-\d{1,2}', re.MULTILINE),
        re.compile(r'ì¸ì‚¬ê·œì •\s*\d{3}-\d{1,2}', re.IGNORECASE),
        re.compile(r'\b\d{3}-\d{1,2}\b'),
    ]
    
    # âœ… Phase 0.8.7: Article ë³¸ë¬¸ ëì˜ ì¥ í—¤ë” íŒ¨í„´
    CHAPTER_TAIL_PATTERN = re.compile(
        r'\nì œ(?P<num>\d+)ì¥\s*(?P<title>[^\n]*)\s*$'
    )
    
    # âœ… Phase 0.8.7: Annex ë…¸ì´ì¦ˆ ë¬¸ì (PDFì—ì„œ ì˜¨ ë°•ìŠ¤/ë¼ì¸ ì•„í‹°íŒ©íŠ¸)
    ANNEX_NOISE_CHARS = ''.join([
        'â–¡', 'â– ', 'â—‹', 'â—', 'â—‡', 'â—†',  # ë°•ìŠ¤/ë„í˜•
        'â”', 'â”€', 'â•', 'â”ƒ', 'â”‚',  # ë¼ì¸
        '', '', '',  # Private Use Area
    ])
    ANNEX_NOISE_PATTERN = re.compile(f'[{re.escape(ANNEX_NOISE_CHARS)}]')
    
    # ê°œì •ì´ë ¥ íŒ¨í„´
    AMENDMENT_PATTERN = re.compile(
        r'(ì œ\d+ì°¨\s*ê°œì •\s*\d{4}\.\d{1,2}\.\d{1,2})',
        re.MULTILINE
    )
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        if not TREE_BUILDER_AVAILABLE:
            raise ImportError("TreeBuilder is required but not available")
        
        self.tree_builder = TreeBuilder()
        logger.info("âœ… LawParser ì´ˆê¸°í™” ì™„ë£Œ (Phase 0.8.7 Polishing)")
    
    def _clean_page_artifacts(self, text: str) -> str:
        """
        í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì™„ì „ ì œê±°
        """
        if not text:
            return text
        
        result = text
        removed_count = 0
        
        for pattern in self.PAGE_ARTIFACT_PATTERNS:
            matches = pattern.findall(result)
            if matches:
                removed_count += len(matches)
                result = pattern.sub('', result)
        
        result = re.sub(r' {2,}', ' ', result)
        result = re.sub(r'\n{3,}', '\n\n', result)
        
        if removed_count > 0:
            logger.debug(f"   ğŸ§¹ í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ {removed_count}ê°œ ì œê±°")
        
        return result.strip()
    
    def _clean_article_body(self, body: str) -> str:
        """
        âœ… Phase 0.8.7: Article ë³¸ë¬¸ ì •ë¦¬
        
        1. ë³¸ë¬¸ ëì— ë¶™ì€ ì¥ í—¤ë” ì œê±° (ì œ2ì¥ì±„ìš© ë“±)
        2. í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        """
        if not body:
            return body
        
        # 1. ë³¸ë¬¸ ëì˜ ì¥ í—¤ë” ì œê±°
        body = self.CHAPTER_TAIL_PATTERN.sub('', body)
        
        # 2. í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        body = self._clean_page_artifacts(body)
        
        return body.strip()
    
    def _clean_annex_text(self, text: str) -> str:
        """
        âœ… Phase 0.8.7: Annex í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°
        
        1. ë°•ìŠ¤/ë¼ì¸ ë¬¸ì ì œê±° (â–¡â– â”â”€ ë“±)
        2. ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬
        3. í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        """
        if not text:
            return text
        
        # 1. ë…¸ì´ì¦ˆ ë¬¸ì ì œê±°
        result = self.ANNEX_NOISE_PATTERN.sub('', text)
        
        # 2. ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬
        result = re.sub(r'\n{3,}', '\n\n', result)
        
        # 3. ì—°ì† ê³µë°± ì •ë¦¬
        result = re.sub(r' {2,}', ' ', result)
        
        # 4. í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        result = self._clean_page_artifacts(result)
        
        return result.strip()
    
    def _extract_amendment_history(self, text: str) -> List[str]:
        """ê°œì •ì´ë ¥ ì¶”ì¶œ"""
        amendments = []
        header_text = text[:2000] if len(text) > 2000 else text
        
        matches = self.AMENDMENT_PATTERN.findall(header_text)
        if matches:
            amendments = list(set(matches))
            amendments.sort(reverse=True)
        
        return amendments
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = True
    ) -> Dict[str, Any]:
        """PDF í…ìŠ¤íŠ¸ íŒŒì‹± (ë©”ì¸ ë©”ì„œë“œ)"""
        logger.info(f"ğŸ“œ LawParser.parse() ì‹œì‘: {document_title}")
        
        cleaned_text = pdf_text
        if normalize_linebreaks:
            cleaned_text = cleaned_text.replace('\r\n', '\n')
        
        amendment_history = self._extract_amendment_history(cleaned_text)
        
        tree_result = self.tree_builder.build(
            markdown=cleaned_text,
            document_title=document_title,
            enacted_date=None
        )
        
        parsed_result = self._convert_tree_to_result(tree_result, document_title)
        
        if amendment_history:
            parsed_result['amendment_history'] = amendment_history
            logger.info(f"   ğŸ“‹ ê°œì •ì´ë ¥: {len(amendment_history)}ê±´ ì¶”ì¶œ")
        
        has_articles = parsed_result.get('total_articles', 0) > 0
        has_annex_pattern = '[ë³„í‘œ' in cleaned_text
        
        if has_articles:
            logger.info(f"   ğŸ“‹ ë³¸ë¬¸ ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
            
            if has_annex_pattern and not parsed_result.get('annex_content'):
                logger.info("   ğŸ“‹ í˜¼í•© ë¬¸ì„œ - ë³¸ë¬¸ + Annex ë™ì‹œ ì²˜ë¦¬")
                self._apply_annex_extraction(cleaned_text, parsed_result)
        else:
            if len(cleaned_text) > 500:
                logger.warning("ğŸ”„ Annex-only ë¬¸ì„œ ê°ì§€ - Fallback Annex íŒŒì„œ ê°€ë™")
                self._apply_annex_fallback(cleaned_text, parsed_result)
        
        logger.info(f"âœ… LawParser.parse() ì™„ë£Œ:")
        logger.info(f"   - ì¥: {parsed_result['total_chapters']}ê°œ")
        logger.info(f"   - ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
        if parsed_result.get('amendment_history'):
            logger.info(f"   - ê°œì •ì´ë ¥: {len(parsed_result['amendment_history'])}ê±´")
        if parsed_result.get('annex_content'):
            logger.info(f"   - Annex: {len(parsed_result['annex_content'])}ì")
        
        return parsed_result
    
    def _convert_tree_to_result(
        self, 
        tree_result: Dict[str, Any],
        document_title: str
    ) -> Dict[str, Any]:
        """TreeBuilder ê²°ê³¼ â†’ LawParser í‘œì¤€ í¬ë§· ë³€í™˜"""
        
        tree = tree_result.get('document', {}).get('tree', [])
        
        logger.info(f"   ğŸŒ² TreeBuilder ë…¸ë“œ ìˆ˜: {len(tree)}ê°œ")
        
        chapters = []
        articles = []
        amendment_history = []
        basic_spirit = ""
        annex_content = ""
        
        current_chapter = ""
        section_order = 0
        
        for node in tree:
            node_type = node.get('level', node.get('type', ''))
            
            if node_type == 'chapter':
                chapter = Chapter(
                    number=node.get('chapter_number', node.get('chapter', node.get('number', ''))),
                    title=node.get('chapter_title', node.get('title', '')),
                    section_order=section_order
                )
                chapters.append(chapter)
                current_chapter = f"{chapter.number} {chapter.title}".strip()
                section_order += 1
                logger.debug(f"      ì¥ ë³€í™˜: {chapter.number} {chapter.title}")
            
            elif node_type == 'article':
                article_no = node.get('article_no', node.get('article_number', node.get('number', '')))
                article_title = node.get('article_title', node.get('title', ''))
                article_body = node.get('content', node.get('body', ''))
                
                chapter_info = node.get('chapter', current_chapter)
                if not chapter_info and current_chapter:
                    chapter_info = current_chapter
                
                if 'children' in node:
                    for child in node.get('children', []):
                        child_content = child.get('content', '')
                        child_no = child.get('clause_no', child.get('item_no', ''))
                        if child_content:
                            article_body += f"\n{child_no} {child_content}"
                
                article = Article(
                    number=article_no,
                    title=article_title,
                    body=article_body.strip(),
                    chapter_number=chapter_info,
                    section_order=section_order
                )
                articles.append(article)
                section_order += 1
                logger.debug(f"      ì¡°ë¬¸ ë³€í™˜: {article_no}({article_title})")
            
            elif node_type == 'amendment_history':
                amendment_history.append(node.get('content', ''))
            
            elif node_type == 'basic_spirit':
                basic_spirit = node.get('content', '')
            
            elif node_type == 'annex':
                annex_content = node.get('content', '')
        
        logger.info(f"   ğŸ“Š ë³€í™˜ ê²°ê³¼: ì¥ {len(chapters)}ê°œ, ì¡°ë¬¸ {len(articles)}ê°œ")
        
        return {
            'document_title': document_title,
            'chapters': chapters,
            'articles': articles,
            'amendment_history': amendment_history,
            'basic_spirit': basic_spirit,
            'annex_content': annex_content,
            'annex_title': '',
            'annex_no': None,
            'related_article': None,
            'annex_tables': [],
            'total_chapters': len(chapters),
            'total_articles': len(articles)
        }
    
    def _apply_annex_fallback(self, cleaned_text: str, parsed_result: dict):
        """Annex-only ë¬¸ì„œ Fallback"""
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… Fallback Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
            
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
    
    def _apply_annex_extraction(self, cleaned_text: str, parsed_result: dict):
        """ë³¸ë¬¸+Annex í˜¼í•© ë¬¸ì„œì—ì„œ Annex ì¶”ì¶œ"""
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… í˜¼í•© ë¬¸ì„œ Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
            
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
    
    def to_chunks(self, parsed_result: dict) -> list:
        """
        íŒŒì‹± ê²°ê³¼ â†’ RAG ì²­í¬ ë³€í™˜
        
        âœ… Phase 0.8.7: Article ë³¸ë¬¸ ì •ë¦¬ ì ìš©
        """
        
        chunks = []
        
        # ë¬¸ì„œ ì œëª©
        if parsed_result.get('document_title'):
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # ê°œì •ì´ë ¥ ì²­í¬
        if parsed_result.get('amendment_history'):
            history_content = "\n".join(parsed_result['amendment_history'])
            chunks.append({
                'content': history_content,
                'metadata': {
                    'type': 'amendment_history',
                    'boundary': 'amendment_history',
                    'char_count': len(history_content),
                    'amendment_count': len(parsed_result['amendment_history']),
                    'section_order': -2
                }
            })
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic_spirit',
                    'boundary': 'basic_spirit',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # ì¥(Chapter) ì²­í¬ synthesize
        seen_chapters = set()
        chapter_order = 0
        
        for article in parsed_result.get('articles', []):
            chapter_info = article.chapter_number
            if chapter_info and chapter_info not in seen_chapters:
                seen_chapters.add(chapter_info)
                
                chapter_match = re.match(r'(ì œ\d+ì¥)\s*(.+)?', chapter_info)
                if chapter_match:
                    chapter_num = chapter_match.group(1)
                    chapter_title = chapter_match.group(2) or ""
                else:
                    chapter_num = chapter_info
                    chapter_title = ""
                
                content = f"{chapter_num} {chapter_title}".strip()
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'chapter',
                        'boundary': 'chapter',
                        'chapter_number': chapter_num,
                        'chapter_title': chapter_title,
                        'char_count': len(content),
                        'section_order': chapter_order
                    }
                })
                chapter_order += 1
        
        # ì¡°ë¬¸ (Phase 0.8.7: ë³¸ë¬¸ ì •ë¦¬ ì ìš©)
        for article in parsed_result.get('articles', []):
            # âœ… Phase 0.8.7: ë³¸ë¬¸ ì •ë¦¬ (ì¥ ê¼¬ë¦¬ + í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸)
            cleaned_body = self._clean_article_body(article.body)
            
            content = f"{article.number}({article.title})\n{cleaned_body}"
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        # Annex ì„œë¸Œì²­í‚¹
        if parsed_result.get('annex_content') and ANNEX_SUBCHUNKING_AVAILABLE:
            logger.info("âœ… Phase 0.8: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘")
            
            subchunker = AnnexSubChunker()
            # âœ… Phase 0.8.7: Annex ë…¸ì´ì¦ˆ ì œê±°
            annex_text = self._clean_annex_text(parsed_result['annex_content'])
            
            try:
                sub_chunks = subchunker.chunk(annex_text)
                validation = validate_subchunks(sub_chunks, len(annex_text))
                
                if validation['is_valid']:
                    logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì„±ê³µ: {validation['chunk_count']}ê°œ")
                    
                    for sub in sub_chunks:
                        chunks.append({
                            'content': sub.content,
                            'metadata': {
                                'type': f"annex_{sub.section_type}",
                                'boundary': 'annex',
                                'section_id': sub.section_id,
                                'section_type': sub.section_type,
                                'char_count': sub.char_count,
                                'section_order': sub.order,
                                **sub.metadata
                            }
                        })
                else:
                    raise ValueError("ê²€ì¦ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Annex ì„œë¸Œì²­í‚¹ ì‹¤íŒ¨: {e}")
                chunks.append({
                    'content': annex_text,
                    'metadata': {
                        'type': 'annex',
                        'boundary': 'annex',
                        'char_count': len(annex_text),
                        'section_order': 1000
                    }
                })
        
        elif parsed_result.get('annex_content'):
            # âœ… Phase 0.8.7: Annex ë…¸ì´ì¦ˆ ì œê±°
            cleaned_annex = self._clean_annex_text(parsed_result['annex_content'])
            chunks.append({
                'content': cleaned_annex,
                'metadata': {
                    'type': 'annex',
                    'boundary': 'annex',
                    'char_count': len(cleaned_annex),
                    'section_order': 1000
                }
            })
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for chunk in chunks:
            ctype = chunk['metadata']['type']
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")
        
        return chunks
    
    def to_markdown(self, parsed_result: dict) -> str:
        """
        íŒŒì‹± ê²°ê³¼ â†’ Markdown ë³€í™˜
        
        âœ… Phase 0.8.7: ë³¸ë¬¸/Annex ì •ë¦¬ ì ìš©
        """
        
        lines = []
        
        # ì œëª©
        if parsed_result.get('document_title'):
            lines.append(f"# {parsed_result['document_title']}")
            lines.append("")
        
        # ê°œì •ì´ë ¥
        if parsed_result.get('amendment_history'):
            lines.append("## ê°œì • ì´ë ¥")
            lines.append("")
            for amendment in parsed_result['amendment_history']:
                lines.append(f"- {amendment}")
            lines.append("")
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            lines.append("## ê¸°ë³¸ì •ì‹ ")
            lines.append("")
            lines.append(parsed_result['basic_spirit'])
            lines.append("")
        
        # ì¥ê³¼ ì¡°ë¬¸
        current_chapter = None
        seen_chapters = set()
        
        for article in parsed_result.get('articles', []):
            # ì¥ì´ ë°”ë€Œë©´ ì¥ í—¤ë” ì¶”ê°€
            if article.chapter_number and article.chapter_number != current_chapter:
                current_chapter = article.chapter_number
                
                if current_chapter not in seen_chapters:
                    seen_chapters.add(current_chapter)
                    lines.append(f"## {current_chapter}")
                    lines.append("")
            
            # âœ… Phase 0.8.7: ì¡°ë¬¸ ë³¸ë¬¸ ì •ë¦¬
            cleaned_body = self._clean_article_body(article.body)
            
            lines.append(f"### {article.number}({article.title})")
            lines.append("")
            lines.append(cleaned_body)
            lines.append("")
        
        # Annex
        if parsed_result.get('annex_content'):
            annex_no = parsed_result.get('annex_no', '')
            annex_title = parsed_result.get('annex_title', '')
            
            if annex_no and annex_title:
                lines.append(f"## [ë³„í‘œ{annex_no}] {annex_title}")
            else:
                lines.append("## ë³„í‘œ")
            lines.append("")
            lines.append("---")
            
            # âœ… Phase 0.8.7: Annex ë…¸ì´ì¦ˆ ì œê±°
            cleaned_annex = self._clean_annex_text(parsed_result['annex_content'])
            lines.append(cleaned_annex)
            lines.append("---")
        
        return "\n".join(lines)


# í•˜ìœ„ í˜¸í™˜ì„±
parse_pdf_text = LawParser.parse