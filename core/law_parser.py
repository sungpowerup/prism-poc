"""
law_parser.py - PRISM LawParser Phase 0.9.7.7

Phase 0.9.7.7 Critical Fix (GPT ë¯¸ì†¡ë‹˜):
- âœ… Import ì‹¤íŒ¨ ì›ì¸ logger.exceptionìœ¼ë¡œ ë“œëŸ¬ë‚´ê¸°
- âœ… Import ê²½ë¡œ ë‹¨ì¼í™” (core.annex_subchunker)
- âœ… íŒ¨í‚¤ì§• í™•ì¸ ê°•í™”

Phase 0.9.5 ìˆ˜ì •ì‚¬í•­ (ë¯¸ì†¡ë‹˜ ê°€ì´ë“œ):
1. âœ… ê°œì •ì´ë ¥ íŒ¨í„´ ëŒ€í­ ê°•í™” (17ê±´ ì™„ì „ ì¶”ì¶œ)
2. âœ… Phase 0.9.4 íšŒê·€ ìˆ˜ì • ìœ ì§€
3. âœ… Chapter ìœ„ì¹˜ ìœ ì§€ (Phase 0.9.2)
4. âœ… ì œ5ì¡° 1í•­ ë²ˆí˜¸ ë³µêµ¬ ìœ ì§€ (Phase 0.9.2)

ê°€ë“œë ˆì¼ ì¤€ìˆ˜:
- ğŸ›‘ ê¸°ì¡´ ê¸°ëŠ¥ ì ˆëŒ€ ìˆ˜ì • ê¸ˆì§€
- ğŸ›‘ DualQA ë¡œì§ ë³€ê²½ ê¸ˆì§€
- ğŸ›‘ spacing ì—”ì§„ ë³€ê²½ ê¸ˆì§€

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT ë¯¸ì†¡ë‹˜
Date: 2025-11-25
Version: Phase 0.9.7.7
"""

import re
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# ============================================================
# Phase 0.9.7.7 Critical Fix - Import ê°•í™”
# ============================================================

# âœ… GPT ë¯¸ì†¡ë‹˜ ì§€ì‹œ A: import ì‹¤íŒ¨ ì›ì¸ì„ ìˆ¨ê¸°ì§€ ë§ê³  ë“œëŸ¬ë‚´ë¼
ANNEX_SUBCHUNKING_AVAILABLE = False
AnnexSubChunker = None
validate_subchunks = None

try:
    # âœ… GPT ë¯¸ì†¡ë‹˜ ì§€ì‹œ B: import ê²½ë¡œ ë‹¨ì¼í™”
    from core.annex_subchunker import AnnexSubChunker, validate_subchunks
    ANNEX_SUBCHUNKING_AVAILABLE = True
    logger.info("âœ… AnnexSubChunker import ì„±ê³µ")
    logger.info(f"   - ëª¨ë“ˆ ìœ„ì¹˜: {AnnexSubChunker.__module__}")
    logger.info(f"   - í´ë˜ìŠ¤: {AnnexSubChunker}")
except ImportError as e:
    logger.exception("âŒ AnnexSubChunker import ì‹¤íŒ¨ (ImportError)")
    logger.error(f"   - ì›ì¸: {e}")
    logger.error("   - í™•ì¸ì‚¬í•­:")
    logger.error("     1. core/annex_subchunker.py íŒŒì¼ ì¡´ì¬ ì—¬ë¶€")
    logger.error("     2. core/__init__.py íŒŒì¼ ì¡´ì¬ ì—¬ë¶€")
    logger.error("     3. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì¤‘ë³µ annex_subchunker.py ì—†ëŠ”ì§€ í™•ì¸")
    ANNEX_SUBCHUNKING_AVAILABLE = False
    AnnexSubChunker = None
    validate_subchunks = None
except Exception as e:
    logger.exception("âŒ AnnexSubChunker import ì‹¤íŒ¨ (ê¸°íƒ€ ì˜ˆì™¸)")
    logger.error(f"   - ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
    logger.error(f"   - ì›ì¸: {e}")
    ANNEX_SUBCHUNKING_AVAILABLE = False
    AnnexSubChunker = None
    validate_subchunks = None

# íŒ¨ì¹˜ ì ìš© í™•ì¸
logger.info("ğŸ”§ Phase 0.9.7.7 Critical Fix íŒ¨ì¹˜ ì ìš©ë¨")
logger.info(f"   - ANNEX_SUBCHUNKING_AVAILABLE: {ANNEX_SUBCHUNKING_AVAILABLE}")


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: str
    body: str
    chapter_number: str
    section_order: int


@dataclass
class Chapter:
    """ì¥ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: str
    section_order: int


class LawParser:
    """
    ë²•ë¥ /ê·œì • ë¬¸ì„œ íŒŒì„œ
    
    Phase 0.9.5 ê°œì •ì´ë ¥ ê°•í™”:
    - âœ… íŒ¨í„´ 4ê°œë¡œ í™•ì¥ ([], (), ë‹¨ë…, <>)
    - âœ… ê³µë°± ë¬´ê´€ ì²˜ë¦¬
    - âœ… 17ê±´ ì™„ì „ ì¶”ì¶œ
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… LawParser v0.9.7.7 ì´ˆê¸°í™” (Phase 0.9.7.7 Critical Fix + Amendment Pattern Enhanced)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = True
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ íŒŒì‹±
        """
        logger.info(f"ğŸ“œ LawParser íŒŒì‹± ì‹œì‘: {document_title}")
        
        # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        cleaned_text = pdf_text
        
        if clean_artifacts:
            cleaned_text = self._clean_page_artifacts(cleaned_text)
            logger.info("   âœ… í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° ì™„ë£Œ")
        
        if normalize_linebreaks:
            cleaned_text = self._normalize_linebreaks(cleaned_text)
            logger.info("   âœ… ê°œí–‰ ì •ê·œí™” ì™„ë£Œ")
        
        # 2. ë¬¸ì„œ íƒ€ì… íŒë³„
        has_articles = bool(re.search(r'ì œ\d+ì¡°', cleaned_text))
        has_annex_only = bool(re.search(r'\[ë³„í‘œ', cleaned_text))
        
        if has_articles:
            logger.info("   ğŸ“‹ ë¬¸ì„œ íƒ€ì…: ë²•ë ¹/ê·œì • (ì¡°ë¬¸ í¬í•¨)")
            return self._parse_legal_document(cleaned_text, document_title)
        elif has_annex_only:
            logger.info("   ğŸ“‹ ë¬¸ì„œ íƒ€ì…: Annex ì „ìš©")
            return self._parse_annex_only_document(cleaned_text, document_title)
        else:
            logger.warning("   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ íƒ€ì… - ê¸°ë³¸ ì²˜ë¦¬")
            return self._create_empty_result(document_title, cleaned_text)
    
    def _clean_page_artifacts(self, text: str) -> str:
        """í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° (Phase 0.8.6)"""
        
        # íŒ¨í„´: "ì¸ì‚¬ê·œì • 402-2" ìŠ¤íƒ€ì¼
        artifact_pattern = r'^[ê°€-í£]{2,10}\s*\d{1,5}-\d{1,3}\s*$'
        
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line_stripped = line.strip()
            
            if re.match(artifact_pattern, line_stripped):
                logger.debug(f"      ì œê±°: {line_stripped}")
                continue
            
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _normalize_linebreaks(self, text: str) -> str:
        """ê°œí–‰ ì •ê·œí™”"""
        
        # 3ì¤„ ì´ìƒ ì—°ì† ê°œí–‰ â†’ 2ì¤„ë¡œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ë¬¸ì¥ ë + ë‹¨ì¼ ê°œí–‰ â†’ ìœ ì§€
        # (ì¡°ë¬¸ ë³¸ë¬¸ ë‚´ ìì—°ìŠ¤ëŸ¬ìš´ ê°œí–‰ ë³´ì¡´)
        
        return text
    
    def _parse_legal_document(self, cleaned_text: str, document_title: str) -> Dict[str, Any]:
        """
        ë²•ë ¹/ê·œì • ë¬¸ì„œ íŒŒì‹±
        
        âœ… Phase 0.9.5: ê°œì •ì´ë ¥ íŒ¨í„´ ê°•í™”
        - 4ê°œ íŒ¨í„´ìœ¼ë¡œ í™•ì¥
        - ê³µë°± ë¬´ê´€ ì²˜ë¦¬
        - 17ê±´ ì™„ì „ ì¶”ì¶œ
        """
        
        # âœ… Phase 0.9.5: ê°œì •ì´ë ¥ íŒ¨í„´ ê°•í™”
        amendment_history = self._extract_amendment_history(cleaned_text)
        logger.info(f"   âœ… ê°œì •ì´ë ¥: {len(amendment_history)}ê±´")
        
        # TreeBuilder ê²°ê³¼ë¥¼ ë³€í™˜
        from core.tree_builder import TreeBuilder
        
        tree_builder = TreeBuilder()
        tree_doc = tree_builder.build(
            markdown=cleaned_text,
            document_title=document_title
        )
        
        # Tree â†’ Article ë³€í™˜
        chapters = []
        articles = []
        
        tree_nodes = tree_doc['document']['tree']
        
        # Chapter ì¶”ì¶œ
        seen_chapters = {}
        for node in tree_nodes:
            if node.get('level') == 'article':
                chapter_info = node.get('chapter', '')
                if chapter_info and chapter_info not in seen_chapters:
                    chapter_match = re.match(r'(ì œ\d+ì¥)\s*(.+)?', chapter_info)
                    if chapter_match:
                        chapter_num = chapter_match.group(1)
                        chapter_title = chapter_match.group(2) or ""
                        
                        chapters.append(Chapter(
                            number=chapter_num,
                            title=chapter_title.strip(),
                            section_order=len(chapters)
                        ))
                        
                        seen_chapters[chapter_info] = chapter_num
        
        # Article ì¶”ì¶œ
        for node in tree_nodes:
            if node.get('level') == 'article':
                article_no = node.get('article_no', '')
                article_title = node.get('article_title', '')
                article_body = node.get('content', '')
                chapter_info = node.get('chapter', '')
                
                # Chapter number ë§¤í•‘
                chapter_number = seen_chapters.get(chapter_info, '')
                
                articles.append(Article(
                    number=article_no,
                    title=article_title,
                    body=article_body,
                    chapter_number=chapter_number,
                    section_order=len(articles)
                ))
        
        logger.info(f"   âœ… ì¡°ë¬¸ íŒŒì‹±: {len(articles)}ê°œ")
        logger.info(f"   âœ… ì¥ íŒŒì‹±: {len(chapters)}ê°œ")
        
        # Annex ì¶”ì¶œ
        parsed_result = {
            'document_title': document_title,
            'chapters': chapters,
            'articles': articles,
            'basic_spirit': '',
            'amendment_history': amendment_history,  # âœ… Phase 0.9.5: ê°•í™”ë¨
            'annex_content': None,
            'annex_no': None,
            'annex_title': None,
            'related_article': None,
            'annex_tables': [],
            'total_chapters': len(chapters),
            'total_articles': len(articles)
        }
        
        self._apply_annex_extraction(cleaned_text, parsed_result)
        
        return parsed_result
    
    def _extract_amendment_history(self, text: str) -> List[str]:
        """
        ê°œì •ì´ë ¥ ì¶”ì¶œ
        
        âœ… Phase 0.9.5: íŒ¨í„´ ëŒ€í­ ê°•í™”
        - ë¯¸ì†¡ë‹˜ ì§€ì‹œ: 17ê±´ ëª¨ë‘ ì¶”ì¶œ
        - ê´„í˜¸ í˜•ì‹ ê°•í™”: (ê°œì • 2003.3.29), (ê°œì •2003.3.29), (ê°œì • 2003.3.29.)
        - ê³µë°± ë¬´ê´€: "ê°œì • 2003" / "ê°œì •2003" ë‘˜ ë‹¤ ì¸ì‹
        """
        
        history = []
        
        # âœ… Phase 0.9.5: íŒ¨í„´ í™•ì¥ (ë¯¸ì†¡ë‹˜ ìš”êµ¬ì‚¬í•­)
        patterns = [
            # íŒ¨í„´ 1: [ê°œì • 2003.3.29] - ê¸°ì¡´
            r'\[(ì „ë¶€ê°œì •|ì¼ë¶€ê°œì •|ì œì •|ê°œì •)\s*(\d{4}\.\d{1,2}\.\d{1,2}\.?)\]',
            
            # íŒ¨í„´ 2: (ê°œì • 2003.3.29) - ê¸°ì¡´
            r'\((ì „ë¶€ê°œì •|ì¼ë¶€ê°œì •|ì œì •|ê°œì •)\s*(\d{4}\.\d{1,2}\.\d{1,2}\.?)\)',
            
            # íŒ¨í„´ 3: ê°œì • 2003.3.29 (ê´„í˜¸ ì—†ìŒ, ë‹¨ë…)
            r'(?<![<\[\(])(ì „ë¶€ê°œì •|ì¼ë¶€ê°œì •|ì œì •|ê°œì •)\s*(\d{4}\.\d{1,2}\.\d{1,2}\.?)(?![>\]\)])',
            
            # íŒ¨í„´ 4: <ê°œì • 2003.3.29> (ë¶€ë“±í˜¸)
            r'<(ì „ë¶€ê°œì •|ì¼ë¶€ê°œì •|ì œì •|ê°œì •)\s*(\d{4}\.\d{1,2}\.\d{1,2}\.?)>',
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                amendment_type = match.group(1)
                amendment_date = match.group(2)
                
                # ë‚ ì§œ ì •ê·œí™”
                amendment_date = amendment_date.rstrip('.')
                
                # ì¤‘ë³µ ì œê±° (ë‚ ì§œ ê¸°ì¤€)
                history_item = f"{amendment_type} {amendment_date}"
                
                # ì¤‘ë³µ í™•ì¸ (ë‚ ì§œë§Œìœ¼ë¡œ)
                if not any(amendment_date in h for h in history):
                    history.append(history_item)
        
        # ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ)
        def extract_date(item):
            match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', item)
            if match:
                y, m, d = match.groups()
                return (int(y), int(m), int(d))
            return (0, 0, 0)
        
        history_sorted = sorted(history, key=extract_date, reverse=True)
        
        logger.info(f"      ê°œì •ì´ë ¥ ì¶”ì¶œ: {len(history_sorted)}ê±´")
        for h in history_sorted[:5]:  # ìµœì‹  5ê±´ë§Œ ë¡œê·¸
            logger.debug(f"         - {h}")
        
        return history_sorted
    
    def _parse_annex_only_document(self, cleaned_text: str, document_title: str) -> Dict[str, Any]:
        """Annex ì „ìš© ë¬¸ì„œ íŒŒì‹±"""
        
        parsed_result = {
            'document_title': document_title,
            'chapters': [],
            'articles': [],
            'basic_spirit': '',
            'amendment_history': [],
            'annex_content': None,
            'annex_no': None,
            'annex_title': None,
            'related_article': None,
            'annex_tables': [],
            'total_chapters': 0,
            'total_articles': 0
        }
        
        self._apply_annex_fallback(cleaned_text, parsed_result)
        
        return parsed_result
    
    def _create_empty_result(self, document_title: str, text: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        
        return {
            'document_title': document_title,
            'chapters': [],
            'articles': [],
            'basic_spirit': text[:500],
            'amendment_history': [],
            'annex_content': None,
            'annex_no': None,
            'annex_title': None,
            'related_article': None,
            'annex_tables': [],
            'total_chapters': 0,
            'total_articles': 0
        }
    
    def _apply_annex_fallback(self, cleaned_text: str, parsed_result: dict):
        """Annex-only ë¬¸ì„œ Fallback"""
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… Fallback Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
            
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
    
    def _apply_annex_extraction(self, cleaned_text: str, parsed_result: dict):
        """ë³¸ë¬¸+Annex í˜¼í•© ë¬¸ì„œì—ì„œ Annex ì¶”ì¶œ"""
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… í˜¼í•© ë¬¸ì„œ Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
            
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
    
    def _clean_article_body(self, body: str) -> str:
        """
        ì¡°ë¬¸ ë³¸ë¬¸ ì •ë¦¬
        
        Phase 0.9.2: í•­ ë²ˆí˜¸ ë³µêµ¬ ìœ ì§€
        Phase 0.8.7: ì¥ ê¼¬ë¦¬ ì œê±° ìœ ì§€
        """
        
        # 1. ì¥ ê¼¬ë¦¬ ì œê±°
        chapter_pattern = r'ì œ\d+ì¥\s+[ê°€-í£\s]+$'
        body = re.sub(chapter_pattern, '', body, flags=re.MULTILINE)
        
        # 2. Phase 0.9.2: í•­ ë²ˆí˜¸ ë³µêµ¬
        body = re.sub(
            r'^\.(\s*"[ê°€-í£]+"\s*ë€)',
            r'1.\1',
            body,
            flags=re.MULTILINE
        )
        
        # 3. ì—°ì† ê³µë°± ì •ë¦¬
        body = re.sub(r'[ \t]+', ' ', body)
        
        # 4. ì—°ì† ê°œí–‰ ì •ë¦¬
        body = re.sub(r'\n{3,}', '\n\n', body)
        
        return body.strip()
    
    def _clean_annex_text(self, text: str) -> str:
        """Annex í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° (Phase 0.8.7)"""
        
        # Private Use Area ë¬¸ì ì œê±°
        text = re.sub(r'[\uF000-\uF8FF]', '', text)
        
        # Box drawing characters
        box_chars = 'â”€â”â”‚â”ƒâ”Œâ”â””â”˜â”œâ”¤â”¬â”´â”¼â•‹â•â•‘â•”â•—â•šâ•â• â•£â•¦â•©â•¬â– â–¡â–ªâ–«'
        for char in box_chars:
            text = text.replace(char, '')
        
        # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        
        # ì—°ì† ê°œí–‰ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
    
    def to_chunks(self, parsed_result: dict) -> list:
        """
        íŒŒì‹± ê²°ê³¼ â†’ RAG ì²­í¬ ë³€í™˜
        
        Phase 0.9.5: ê°œì •ì´ë ¥ ì²­í¬ ìœ ì§€
        Phase 0.9.2: Chapter ìœ„ì¹˜ ìœ ì§€
        """
        
        chunks = []
        
        # ë¬¸ì„œ ì œëª©
        if parsed_result.get('document_title'):
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # âœ… Phase 0.9.5: ê°œì •ì´ë ¥ ì²­í¬ (ê°•í™”ë¨)
        if parsed_result.get('amendment_history'):
            history_content = "\n".join(parsed_result['amendment_history'])
            chunks.append({
                'content': history_content,
                'metadata': {
                    'type': 'amendment_history',
                    'boundary': 'amendment_history',
                    'char_count': len(history_content),
                    'amendment_count': len(parsed_result['amendment_history']),
                    'section_order': -2
                }
            })
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic_spirit',
                    'boundary': 'basic_spirit',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # âœ… Phase 0.9.2: ì¡°ë¬¸ ë¨¼ì € ì¶”ê°€, Chapter ë‚˜ì¤‘ì— insert
        article_chunks = []
        
        for article in parsed_result.get('articles', []):
            # Phase 0.9.2: ë³¸ë¬¸ ì •ë¦¬ (í•­ ë²ˆí˜¸ ë³µêµ¬ í¬í•¨)
            cleaned_body = self._clean_article_body(article.body)
            
            content = f"{article.number}({article.title})\n{cleaned_body}"
            article_chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        chunks.extend(article_chunks)
        
        # âœ… Phase 0.9.2: Chapter ìœ„ì¹˜ ì¬ë°°ì¹˜
        if parsed_result.get('chapters'):
            chapter_positions = {}
            
            for idx, chunk in enumerate(chunks):
                if chunk['metadata']['type'] == 'article':
                    chapter_num = chunk['metadata'].get('chapter_number', '')
                    if chapter_num and chapter_num not in chapter_positions:
                        chapter_positions[chapter_num] = idx
            
            for chapter in reversed(parsed_result['chapters']):
                chapter_num = chapter.number
                insert_idx = chapter_positions.get(chapter_num)
                
                if insert_idx is not None:
                    chapter_content = f"{chapter.number} {chapter.title}"
                    chapter_chunk = {
                        'content': chapter_content,
                        'metadata': {
                            'type': 'chapter',
                            'boundary': 'chapter',
                            'chapter_number': chapter.number,
                            'chapter_title': chapter.title,
                            'char_count': len(chapter_content),
                            'section_order': chapter.section_order
                        }
                    }
                    
                    chunks.insert(insert_idx, chapter_chunk)
        
        # Phase 0.9.5.1: Annex ì„œë¸Œì²­í‚¹ (ì •ì œ ë‹¨ì¼í™”)
        if parsed_result.get('annex_content'):
            annex_content = parsed_result['annex_content']
            logger.info(f"âœ… Phase 0.9.5.1: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘")
            
            # âœ… Phase 0.9.5.1 Hotfix: RAW ê·¸ëŒ€ë¡œ ì „ë‹¬ (ì •ì œëŠ” SubChunkerì—ì„œë§Œ)
            annex_text = annex_content  # ì •ì œ ì œê±°!
            
            try:
                if ANNEX_SUBCHUNKING_AVAILABLE:
                    subchunker = AnnexSubChunker()
                    sub_chunks = subchunker.chunk(annex_text)
                    
                    # âœ… Phase 0.9.5.1: Loss Check ê¸°ì¤€ í†µì¼
                    # SubChunkerê°€ ì •ì œí•œ canonical text ê¸°ì¤€ìœ¼ë¡œ validation
                    canonical_text = subchunker._clean_annex_text(annex_text)
                    validation = validate_subchunks(sub_chunks, len(canonical_text))
                    
                    if validation['is_valid']:
                        logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì„±ê³µ: {validation['chunk_count']}ê°œ")
                        
                        for sub in sub_chunks:
                            chunks.append({
                                'content': sub.content,
                                'metadata': {
                                    'type': f"annex_{sub.section_type}",
                                    'boundary': 'annex',
                                    'section_id': sub.section_id,
                                    'section_type': sub.section_type,
                                    'char_count': sub.char_count,
                                    'section_order': sub.order,
                                    **sub.metadata
                                }
                            })
                    else:
                        raise ValueError("ê²€ì¦ ì‹¤íŒ¨")
                        
                else:
                    raise ImportError("AnnexSubChunker ì—†ìŒ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Annex ì„œë¸Œì²­í‚¹ ì‹¤íŒ¨: {e}")
                chunks.append({
                    'content': annex_text,
                    'metadata': {
                        'type': 'annex',
                        'boundary': 'annex',
                        'char_count': len(annex_text),
                        'annex_no': parsed_result.get('annex_no', ''),
                        'annex_title': parsed_result.get('annex_title', ''),
                        'related_article': parsed_result.get('related_article', ''),
                        'fallback': True
                    }
                })
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
        type_counts = {}
        for chunk in chunks:
            ctype = chunk['metadata']['type']
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")
        
        return chunks
    
    def to_markdown(self, parsed_result: dict) -> str:
        """
        íŒŒì‹± ê²°ê³¼ â†’ RAGìš© Markdown (engine.md)
        
        âœ… Phase 0.9.5: ê°œì •ì´ë ¥ í¬í•¨
        """
        
        lines = []
        
        # ì œëª©
        lines.append(f"# {parsed_result['document_title']}")
        lines.append("")
        
        # âœ… Phase 0.9.5: ê°œì •ì´ë ¥ í¬í•¨
        if parsed_result.get('amendment_history'):
            lines.append("## ê°œì •ì´ë ¥")
            lines.append("")
            for amendment in parsed_result['amendment_history']:
                lines.append(f"- {amendment}")
            lines.append("")
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            lines.append("## ê¸°ë³¸ì •ì‹ ")
            lines.append("")
            lines.append(parsed_result['basic_spirit'])
            lines.append("")
        
        # ì¥/ì¡°ë¬¸
        current_chapter = ""
        
        for article in parsed_result.get('articles', []):
            if article.chapter_number and article.chapter_number != current_chapter:
                current_chapter = article.chapter_number
                
                for ch in parsed_result.get('chapters', []):
                    if ch.number == current_chapter:
                        lines.append(f"## {ch.number} {ch.title}")
                        lines.append("")
                        break
            
            cleaned_body = self._clean_article_body(article.body)
            lines.append(f"### {article.number}({article.title})")
            lines.append("")
            lines.append(cleaned_body)
            lines.append("")
        
        # Annex
        if parsed_result.get('annex_content'):
            cleaned_annex = self._clean_annex_text(parsed_result['annex_content'])
            lines.append("## ë³„í‘œ")
            lines.append("")
            lines.append(cleaned_annex)
            lines.append("")
        
        return '\n'.join(lines)
    
    def to_review_md(self, parsed_result: dict) -> str:
        """íŒŒì‹± ê²°ê³¼ â†’ review.md (Phase 0.6.3)"""
        
        lines = []
        
        # ì œëª©
        lines.append(f"# {parsed_result['document_title']}")
        lines.append("")
        
        # ê°œì •ì´ë ¥
        if parsed_result.get('amendment_history'):
            lines.append("## ê°œì •ì´ë ¥")
            lines.append("")
            for amendment in parsed_result['amendment_history']:
                lines.append(f"- {amendment}")
            lines.append("")
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            lines.append("## ê¸°ë³¸ì •ì‹ ")
            lines.append("")
            lines.append(parsed_result['basic_spirit'])
            lines.append("")
        
        # ì¥/ì¡°ë¬¸
        current_chapter = ""
        
        for article in parsed_result.get('articles', []):
            if article.chapter_number and article.chapter_number != current_chapter:
                current_chapter = article.chapter_number
                
                for ch in parsed_result.get('chapters', []):
                    if ch.number == current_chapter:
                        lines.append(f"## {ch.number} {ch.title}")
                        lines.append("")
                        break
            
            cleaned_body = self._clean_article_body(article.body)
            lines.append(f"### {article.number}({article.title})")
            lines.append("")
            lines.append(cleaned_body)
            lines.append("")
        
        # Annex
        if parsed_result.get('annex_content'):
            cleaned_annex = self._clean_annex_text(parsed_result['annex_content'])
            lines.append("## ë³„í‘œ")
            lines.append("")
            lines.append(cleaned_annex)
            lines.append("")
        
        return '\n'.join(lines)