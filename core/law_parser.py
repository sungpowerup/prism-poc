"""
law_parser.py - LawMode ì¡°ë¬¸ íŒŒì„œ
Phase 0.6.2 "Front Matter & Clean Chapters"

âœ… Phase 0.6.2 í•«í”½ìŠ¤ (GPT ìµœì¢… ê¶Œì¥):
1. íƒ€ì´í‹€ ì²­í¬ ì¶”ê°€ (type="title")
2. ê°œì •ì´ë ¥ ì²­í¬ ì¶”ê°€ (type="amendment_history")
3. ì¥ ì²­í¬ ì •ì œ (chapter_titleì— ì¡°ë¬¸ ë³¸ë¬¸ ì•ˆ ë¶™ê²Œ)
4. ë„ì–´ì“°ê¸°ëŠ” Phase 0.7ë¡œ ì—°ê¸°

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + GPT ìµœì¢… í”¼ë“œë°±
Date: 2025-11-14
Version: Phase 0.6.2
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class Chapter:
    """ì¥ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: str
    start_pos: int
    section_order: int


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: Optional[str]
    body: str
    start_pos: int
    end_pos: int
    chapter_number: Optional[str] = None
    section_order: int = 0
    article_type: str = 'article'


class LawParser:
    """
    ê·œì •/ë²•ë ¹ ì „ìš© íŒŒì„œ (Phase 0.6.2)
    
    âœ… Phase 0.6.2 ê°œì„ :
    - íƒ€ì´í‹€/ê°œì •ì´ë ¥ front matter ì¶”ì¶œ
    - ì¥ title ì •í™•íˆ ë¶„ë¦¬
    """
    
    # ê¸°ë³¸ì •ì‹  íŒ¨í„´
    BASIC_SPIRIT_PATTERN = re.compile(
        r'ê¸°\s*ë³¸\s*ì •\s*ì‹ ',
        re.MULTILINE | re.IGNORECASE
    )
    
    # ì¡°ë¬¸ í—¤ë” íŒ¨í„´
    ARTICLE_HEADER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¡°(?:\s*ì˜\s*(\d+))?\s*\(([^)]+)\)',
        re.MULTILINE
    )
    
    # âœ… Phase 0.6.2: ì¥ íŒ¨í„´ ì •ë°€ ì¡°ì •
    # "ì œ1ì¥" + "ì´ì¹™" ê¹Œì§€ë§Œ ë§¤ì¹­ (ì¡°ë¬¸ ì‹œì‘ ì „ì— ë©ˆì¶¤)
    CHAPTER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¥\s*([ê°€-í£]{2,10})',
        re.MULTILINE
    )
    
    # âœ… Phase 0.6.2: íƒ€ì´í‹€ íŒ¨í„´
    TITLE_PATTERN = re.compile(
        r'^([ê°€-í£]{2,10}ê·œì •|[ê°€-í£]{2,10}ë‚´ê·œ|[ê°€-í£]{2,10}ì •ê´€)',
        re.MULTILINE
    )
    
    # âœ… Phase 0.6.2: ê°œì •ì¼ íŒ¨í„´
    AMENDMENT_PATTERN = re.compile(
        r'(ì œ\d+ì°¨\s*)?ê°œì •\s*\d{4}\.\d{1,2}\.\d{1,2}\.?',
        re.MULTILINE
    )
    
    # ì¤„ë°”ê¿ˆ ì •ë¦¬ íŒ¨í„´ (Phase 0.6 ìœ ì§€)
    SAFE_NEWLINE_PATTERN = re.compile(
        r'(?<=[ê°€-í£0-9])(?<![.?!])[\r]?\n(?=[ê°€-í£0-9])',
        re.MULTILINE
    )
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… LawParser ì´ˆê¸°í™” (Phase 0.6.2)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        enacted_date: Optional[str] = None,
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = True
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì¡°ë¬¸ êµ¬ì¡° ì¶”ì¶œ
        
        Args:
            pdf_text: PDFì—ì„œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸
            document_title: ë¬¸ì„œ ì œëª©
            enacted_date: ì œì •ì¼
            clean_artifacts: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° ì—¬ë¶€
            normalize_linebreaks: ì¤„ë°”ê¿ˆ ì •ë¦¬ ì—¬ë¶€
        
        Returns:
            {
                'document_title': str,
                'enacted_date': str,
                'amendment_history': str,  # âœ… Phase 0.6.2 ì‹ ê·œ
                'basic_spirit': str,
                'chapters': List[Chapter],
                'articles': List[Article],
                'total_articles': int,
                'total_chapters': int
            }
        """
        logger.info(f"ğŸ“œ LawParser Phase 0.6.2 ì‹œì‘: {document_title}")
        logger.info(f"   ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸: {len(pdf_text)}ì")
        
        # Phase 0.5: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        if clean_artifacts:
            try:
                from .page_cleaner import clean_page_artifacts
                pdf_text = clean_page_artifacts(pdf_text)
                logger.info(f"   ğŸ§¹ ì•„í‹°íŒ©íŠ¸ ì œê±° í›„: {len(pdf_text)}ì")
            except ImportError:
                logger.warning("   âš ï¸ PageCleaner ë¯¸ì„¤ì¹˜ - ê±´ë„ˆëœ€")
        
        # Phase 0.6: ì¤„ë°”ê¿ˆ ì •ë¦¬
        if normalize_linebreaks:
            original_len = len(pdf_text)
            pdf_text = self._normalize_linebreaks(pdf_text)
            logger.info(f"   âœ‚ï¸ ì¤„ë°”ê¿ˆ ì •ë¦¬: {original_len}ì â†’ {len(pdf_text)}ì ({len(pdf_text)-original_len:+d})")
        
        # âœ… Phase 0.6.2: Front Matter ì¶”ì¶œ
        title, amendment_history, pdf_text = self._extract_front_matter(pdf_text, document_title)
        
        # 1. ê¸°ë³¸ì •ì‹  ì¶”ì¶œ
        basic_spirit = self._extract_basic_spirit(pdf_text)
        logger.info(f"   âœ… ê¸°ë³¸ì •ì‹ : {len(basic_spirit)}ì")
        
        # 2. ì¥ ì¶”ì¶œ (Phase 0.6.2 ì •ë°€)
        chapters = self._extract_chapters(pdf_text)
        logger.info(f"   âœ… ì¥: {len(chapters)}ê°œ")
        
        # 3. ì¡°ë¬¸ ì¶”ì¶œ
        articles = self._extract_articles_from_text(pdf_text)
        logger.info(f"   âœ… ì¡°ë¬¸: {len(articles)}ê°œ")
        
        # 4. ì¡°ë¬¸ì— chapter_number í• ë‹¹
        self._assign_chapters_to_articles(chapters, articles)
        
        # 5. section_order ë¶€ì—¬
        self._assign_section_order(chapters, articles)
        
        # 6. ê¸°ë³¸ì •ì‹ /ì¡°ë¬¸ì—ì„œ ì¥ í—¤ë” ì œê±°
        basic_spirit = self._remove_chapter_headers(basic_spirit)
        for article in articles:
            article.body = self._remove_chapter_headers(article.body)
        
        logger.info(f"âœ… LawParser Phase 0.6.2 ì™„ë£Œ: {len(chapters)}ê°œ ì¥, {len(articles)}ê°œ ì¡°ë¬¸")
        
        return {
            'document_title': title,  # âœ… Phase 0.6.2: ì¶”ì¶œëœ íƒ€ì´í‹€
            'enacted_date': enacted_date or '',
            'amendment_history': amendment_history,  # âœ… Phase 0.6.2 ì‹ ê·œ
            'basic_spirit': basic_spirit,
            'chapters': chapters,
            'articles': articles,
            'total_articles': len(articles),
            'total_chapters': len(chapters)
        }
    
    def _extract_front_matter(
        self, 
        text: str, 
        fallback_title: str = ""
    ) -> Tuple[str, str, str]:
        """
        âœ… Phase 0.6.2: ë¬¸ì„œ Front Matter ì¶”ì¶œ
        
        Returns:
            (title, amendment_history, remaining_text)
        """
        # 1. íƒ€ì´í‹€ ì¶”ì¶œ
        title = fallback_title
        title_match = self.TITLE_PATTERN.search(text[:500])  # ì• 500ì ë‚´
        if title_match:
            title = title_match.group(1)
            logger.info(f"   ğŸ“Œ íƒ€ì´í‹€ ì¶”ì¶œ: {title}")
        
        # 2. ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendment_history = ""
        amendments = []
        
        # ê¸°ë³¸ì •ì‹  ì´ì „ êµ¬ê°„ì—ì„œë§Œ ì°¾ê¸°
        spirit_match = self.BASIC_SPIRIT_PATTERN.search(text)
        search_end = spirit_match.start() if spirit_match else 1000
        
        for match in self.AMENDMENT_PATTERN.finditer(text[:search_end]):
            amendments.append(match.group(0))
        
        if amendments:
            amendment_history = ' '.join(amendments)
            logger.info(f"   ğŸ“… ê°œì •ì´ë ¥: {len(amendments)}ê±´")
        
        # 3. Front Matter ì œê±° (ì„ íƒì )
        # ì›ë³¸ í…ìŠ¤íŠ¸ëŠ” ìœ ì§€, ì²­í¬ ìƒì„± ì‹œ í™œìš©
        return title, amendment_history, text
    
    def _normalize_linebreaks(self, text: str) -> str:
        """ì¤„ë°”ê¿ˆ ì •ë¦¬ (Phase 0.6)"""
        before = text
        after = self.SAFE_NEWLINE_PATTERN.sub('', text)
        removed_count = before.count('\n') - after.count('\n')
        
        if removed_count > 0:
            logger.info(f"      âœ‚ï¸ ë‹¨ì–´ ì¤‘ê°„ ì¤„ë°”ê¿ˆ ì œê±°: {removed_count}ê°œ")
        
        return after
    
    def _extract_basic_spirit(self, text: str) -> str:
        """ê¸°ë³¸ì •ì‹  ì¶”ì¶œ"""
        match = self.BASIC_SPIRIT_PATTERN.search(text)
        if not match:
            return ""
        
        start = match.end()
        
        # ë‹¤ìŒ ì¡°ë¬¸ ë˜ëŠ” ì¥ê¹Œì§€
        next_article = self.ARTICLE_HEADER_PATTERN.search(text, start)
        next_chapter = self.CHAPTER_PATTERN.search(text, start)
        
        if next_article and next_chapter:
            end = min(next_article.start(), next_chapter.start())
        elif next_article:
            end = next_article.start()
        elif next_chapter:
            end = next_chapter.start()
        else:
            end = len(text)
        
        spirit_text = text[start:end].strip()
        return spirit_text
    
    def _extract_chapters(self, text: str) -> List[Chapter]:
        """
        âœ… Phase 0.6.2: ì¥ ì¶”ì¶œ (íƒ€ì´í‹€ ì •ë°€ ë¶„ë¦¬)
        
        "ì œ1ì¥ ì´ì¹™" â†’ chapter_title="ì´ì¹™"ë§Œ
        """
        chapters = []
        order = 0
        
        for match in self.CHAPTER_PATTERN.finditer(text):
            chapter_num = match.group(1)
            chapter_title = match.group(2).strip()
            
            chapter = Chapter(
                number=f"ì œ{chapter_num}ì¥",
                title=chapter_title,
                start_pos=match.start(),
                section_order=order
            )
            
            chapters.append(chapter)
            order += 1
            
            logger.info(f"   ğŸ“‚ {chapter.number} {chapter.title} (ìˆœì„œ: {chapter.section_order})")
        
        return chapters
    
    def _extract_articles_from_text(self, text: str) -> List[Article]:
        """ì¡°ë¬¸ ì¶”ì¶œ"""
        articles = []
        
        matches = list(self.ARTICLE_HEADER_PATTERN.finditer(text))
        
        if not matches:
            logger.warning("   âš ï¸ ì¡°ë¬¸ í—¤ë” ë¯¸ë°œê²¬")
            return articles
        
        logger.info(f"   ğŸ” ì¡°ë¬¸ í—¤ë”: {len(matches)}ê°œ ë°œê²¬")
        
        for i, match in enumerate(matches):
            article_num = match.group(1)
            article_sub = match.group(2)
            article_title = match.group(3)
            
            # ì¡°ë¬¸ ë²ˆí˜¸
            if article_sub:
                number = f"ì œ{article_num}ì¡°ì˜{article_sub}"
            else:
                number = f"ì œ{article_num}ì¡°"
            
            # ë³¸ë¬¸ ë²”ìœ„
            body_start = match.end()
            
            if i < len(matches) - 1:
                body_end = matches[i + 1].start()
            else:
                body_end = len(text)
            
            body = text[body_start:body_end].strip()
            
            article = Article(
                number=number,
                title=article_title,
                body=body,
                start_pos=match.start(),
                end_pos=body_end,
                chapter_number=None,
                section_order=0
            )
            
            articles.append(article)
            logger.info(f"   ğŸ“„ {number} ({article_title}): {len(body)}ì")
        
        return articles
    
    def _assign_chapters_to_articles(self, chapters: List[Chapter], articles: List[Article]) -> None:
        """ì¡°ë¬¸ì— chapter_number í• ë‹¹"""
        if not chapters:
            logger.info("   â„¹ï¸ ì¥ ì—†ìŒ - chapter_number í• ë‹¹ ê±´ë„ˆëœ€")
            return
        
        for article in articles:
            assigned_chapter = None
            for chapter in chapters:
                if chapter.start_pos < article.start_pos:
                    assigned_chapter = chapter.number
                else:
                    break
            
            article.chapter_number = assigned_chapter
            
            if assigned_chapter:
                logger.debug(f"      {article.number} â†’ {assigned_chapter}")
    
    def _assign_section_order(self, chapters: List[Chapter], articles: List[Article]) -> None:
        """section_order ë¶€ì—¬"""
        order = 0
        
        all_sections = []
        
        for chapter in chapters:
            all_sections.append(('chapter', chapter))
        
        for article in articles:
            all_sections.append(('article', article))
        
        all_sections.sort(key=lambda x: x[1].start_pos)
        
        for section_type, section in all_sections:
            section.section_order = order
            order += 1
        
        logger.info(f"   ğŸ”¢ section_order ë¶€ì—¬ ì™„ë£Œ: {order}ê°œ ì„¹ì…˜")
    
    def _remove_chapter_headers(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¥ í—¤ë” ì œê±°"""
        cleaned = self.CHAPTER_PATTERN.sub('', text)
        return cleaned.strip()
    
    def to_chunks(self, parsed_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        âœ… Phase 0.6.2: íŒŒì‹± ê²°ê³¼ â†’ ì²­í¬ ë³€í™˜
        
        ìˆœì„œ: title â†’ amendment_history â†’ basic â†’ chapter â†’ article
        """
        chunks = []
        
        # âœ… Phase 0.6.2: 0. íƒ€ì´í‹€ ì²­í¬
        if parsed_result['document_title']:
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # âœ… Phase 0.6.2: 1. ê°œì •ì´ë ¥ ì²­í¬
        if parsed_result['amendment_history']:
            chunks.append({
                'content': parsed_result['amendment_history'],
                'metadata': {
                    'type': 'amendment_history',
                    'boundary': 'header',
                    'title': 'ê°œì • ì´ë ¥',
                    'char_count': len(parsed_result['amendment_history']),
                    'section_order': -2
                }
            })
        
        # 2. ê¸°ë³¸ì •ì‹  ì²­í¬
        if parsed_result['basic_spirit']:
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic',
                    'boundary': 'basic_spirit',
                    'title': 'ê¸°ë³¸ì •ì‹ ',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # 3. ì¥(Chapter) ì²­í¬
        for chapter in parsed_result['chapters']:
            content = f"{chapter.number} {chapter.title}"
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'chapter',
                    'boundary': 'chapter',
                    'chapter_number': chapter.number,
                    'chapter_title': chapter.title,
                    'char_count': len(content),
                    'section_order': chapter.section_order
                }
            })
        
        # 4. ì¡°ë¬¸ ì²­í¬
        for article in parsed_result['articles']:
            content = f"{article.number}({article.title})\n{article.body}"
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        # section_order ì •ë ¬
        chunks.sort(key=lambda c: c['metadata'].get('section_order', 999))
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ (Phase 0.6.2): {len(chunks)}ê°œ")
        logger.info(f"   - íƒ€ì´í‹€: 1ê°œ")
        logger.info(f"   - ê°œì •ì´ë ¥: 1ê°œ")
        logger.info(f"   - ê¸°ë³¸ì •ì‹ : 1ê°œ")
        logger.info(f"   - ì¥: {parsed_result['total_chapters']}ê°œ")
        logger.info(f"   - ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
        
        return chunks