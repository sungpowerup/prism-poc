"""
law_parser.py - LawMode ì¡°ë¬¸ íŒŒì„œ
Phase 0.6.3 "Clean Chapter Title"

âœ… Phase 0.6.3 í•«í”½ìŠ¤ (GPT ìµœì¢… ê¶Œì¥):
1. ì¥ íŒŒì‹± ì „ì²˜ë¦¬ ê°•í™” (ì œNì¥, ì œNì¡° ì•ë’¤ ì¤„ë°”ê¿ˆ ê°•ì œ)
2. CHAPTER_PATTERN ì •ë°€ ì¡°ì • ("ì œ" ì§ì „ê¹Œì§€ë§Œ ë§¤ì¹­)
3. chapter_title ì •í™•ë„ 100% ë‹¬ì„± ("ì´ì¹™ì œ" â†’ "ì´ì¹™")

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + GPT ìµœì¢… í”¼ë“œë°±
Date: 2025-11-14
Version: Phase 0.6.3
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class Chapter:
    """ì¥ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: str
    start_pos: int
    section_order: int


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: Optional[str]
    body: str
    start_pos: int
    end_pos: int
    chapter_number: Optional[str] = None
    section_order: int = 0
    article_type: str = 'article'


class LawParser:
    """
    ê·œì •/ë²•ë ¹ ì „ìš© íŒŒì„œ (Phase 0.6.3)
    
    âœ… Phase 0.6.3 ê°œì„ :
    - ì¥ íŒŒì‹± ì „ì²˜ë¦¬ ê°•í™”
    - chapter_title ì •í™•ë„ 100%
    """
    
    # ê¸°ë³¸ì •ì‹  íŒ¨í„´
    BASIC_SPIRIT_PATTERN = re.compile(
        r'ê¸°\s*ë³¸\s*ì •\s*ì‹ ',
        re.MULTILINE | re.IGNORECASE
    )
    
    # ì¡°ë¬¸ í—¤ë” íŒ¨í„´
    ARTICLE_HEADER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¡°(?:\s*ì˜\s*(\d+))?\s*\(([^)]+)\)',
        re.MULTILINE
    )
    
    # âœ… Phase 0.6.3: ì¥ íŒ¨í„´ (ì „ì²˜ë¦¬ í›„ ì‚¬ìš©)
    # "ì œ1ì¥" + ë‹¤ìŒ "ì œ" ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ í•œê¸€ë§Œ
    CHAPTER_PATTERN = re.compile(
        r'(ì œ\s*\d+\s*ì¥)\s*([^\nì œ]+)',
        re.MULTILINE
    )
    
    # íƒ€ì´í‹€ íŒ¨í„´
    TITLE_PATTERN = re.compile(
        r'^([ê°€-í£]{2,10}ê·œì •|[ê°€-í£]{2,10}ë‚´ê·œ|[ê°€-í£]{2,10}ì •ê´€)',
        re.MULTILINE
    )
    
    # ê°œì •ì¼ íŒ¨í„´
    AMENDMENT_PATTERN = re.compile(
        r'(ì œ\d+ì°¨\s*)?ê°œì •\s*\d{4}\.\d{1,2}\.\d{1,2}\.?',
        re.MULTILINE
    )
    
    # ì¤„ë°”ê¿ˆ ì •ë¦¬ íŒ¨í„´
    SAFE_NEWLINE_PATTERN = re.compile(
        r'(?<=[ê°€-í£0-9])(?<![.?!])[\r]?\n(?=[ê°€-í£0-9])',
        re.MULTILINE
    )
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… LawParser ì´ˆê¸°í™” (Phase 0.6.3)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        enacted_date: Optional[str] = None,
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = True
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì¡°ë¬¸ êµ¬ì¡° ì¶”ì¶œ
        
        Args:
            pdf_text: PDFì—ì„œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸
            document_title: ë¬¸ì„œ ì œëª©
            enacted_date: ì œì •ì¼
            clean_artifacts: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° ì—¬ë¶€
            normalize_linebreaks: ì¤„ë°”ê¿ˆ ì •ë¦¬ ì—¬ë¶€
        
        Returns:
            {
                'document_title': str,
                'enacted_date': str,
                'amendment_history': str,
                'basic_spirit': str,
                'chapters': List[Chapter],
                'articles': List[Article],
                'total_articles': int,
                'total_chapters': int
            }
        """
        logger.info(f"ğŸ“œ LawParser Phase 0.6.3 ì‹œì‘: {document_title}")
        logger.info(f"   ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸: {len(pdf_text)}ì")
        
        # Phase 0.5: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        if clean_artifacts:
            try:
                from .page_cleaner import clean_page_artifacts
                pdf_text = clean_page_artifacts(pdf_text)
                logger.info(f"   ğŸ§¹ ì•„í‹°íŒ©íŠ¸ ì œê±° í›„: {len(pdf_text)}ì")
            except ImportError:
                logger.warning("   âš ï¸ PageCleaner ë¯¸ì„¤ì¹˜ - ê±´ë„ˆëœ€")
        
        # Phase 0.6: ì¤„ë°”ê¿ˆ ì •ë¦¬
        if normalize_linebreaks:
            original_len = len(pdf_text)
            pdf_text = self._normalize_linebreaks(pdf_text)
            logger.info(f"   âœ‚ï¸ ì¤„ë°”ê¿ˆ ì •ë¦¬: {original_len}ì â†’ {len(pdf_text)}ì ({len(pdf_text)-original_len:+d})")
        
        # âœ… Phase 0.6.3: ì¥/ì¡°ë¬¸ ì „ì²˜ë¦¬ (GPT ê¶Œì¥)
        pdf_text = self._preprocess_for_chapter_parsing(pdf_text)
        
        # Front Matter ì¶”ì¶œ
        title, amendment_history, pdf_text = self._extract_front_matter(pdf_text, document_title)
        
        # 1. ê¸°ë³¸ì •ì‹  ì¶”ì¶œ
        basic_spirit = self._extract_basic_spirit(pdf_text)
        logger.info(f"   âœ… ê¸°ë³¸ì •ì‹ : {len(basic_spirit)}ì")
        
        # 2. ì¥ ì¶”ì¶œ (Phase 0.6.3 ì •ë°€)
        chapters = self._extract_chapters(pdf_text)
        logger.info(f"   âœ… ì¥: {len(chapters)}ê°œ")
        
        # 3. ì¡°ë¬¸ ì¶”ì¶œ
        articles = self._extract_articles_from_text(pdf_text)
        logger.info(f"   âœ… ì¡°ë¬¸: {len(articles)}ê°œ")
        
        # 4. ì¡°ë¬¸ì— chapter_number í• ë‹¹
        self._assign_chapters_to_articles(chapters, articles)
        
        # 5. section_order ë¶€ì—¬
        self._assign_section_order(chapters, articles)
        
        # 6. ê¸°ë³¸ì •ì‹ /ì¡°ë¬¸ì—ì„œ ì¥ í—¤ë” ì œê±°
        basic_spirit = self._remove_chapter_headers(basic_spirit)
        for article in articles:
            article.body = self._remove_chapter_headers(article.body)
        
        logger.info(f"âœ… LawParser Phase 0.6.3 ì™„ë£Œ: {len(chapters)}ê°œ ì¥, {len(articles)}ê°œ ì¡°ë¬¸")
        
        return {
            'document_title': title,
            'enacted_date': enacted_date or '',
            'amendment_history': amendment_history,
            'basic_spirit': basic_spirit,
            'chapters': chapters,
            'articles': articles,
            'total_articles': len(articles),
            'total_chapters': len(chapters)
        }
    
    def _preprocess_for_chapter_parsing(self, text: str) -> str:
        """
        âœ… Phase 0.6.3: ì¥/ì¡°ë¬¸ íŒŒì‹± ì „ì²˜ë¦¬ (GPT ê¶Œì¥)
        
        "ì œ1ì¥ ì´ì¹™ì œ1ì¡°(ëª©ì )" â†’ "ì œ1ì¥ ì´ì¹™\nì œ1ì¡°(ëª©ì )"
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # 1. ì œNì¥ ì•ë’¤ ì¤„ë°”ê¿ˆ ê°•ì œ
        text = re.sub(r'(ì œ\s*\d+\s*ì¥)', r'\n\1 ', text)
        
        # 2. ì œNì¡° ì• ì¤„ë°”ê¿ˆ ê°•ì œ
        text = re.sub(r'(ì œ\s*\d+\s*ì¡°)', r'\n\1', text)
        
        logger.debug("   ğŸ”§ ì¥/ì¡°ë¬¸ ì „ì²˜ë¦¬ ì™„ë£Œ (ì¤„ë°”ê¿ˆ ê°•ì œ)")
        
        return text
    
    def _extract_front_matter(
        self, 
        text: str, 
        fallback_title: str = ""
    ) -> Tuple[str, str, str]:
        """
        âœ… Phase 0.6.2: ë¬¸ì„œ Front Matter ì¶”ì¶œ
        
        Returns:
            (title, amendment_history, remaining_text)
        """
        # 1. íƒ€ì´í‹€ ì¶”ì¶œ
        title = fallback_title
        title_match = self.TITLE_PATTERN.search(text[:500])
        if title_match:
            title = title_match.group(1)
            logger.info(f"   ğŸ“Œ íƒ€ì´í‹€ ì¶”ì¶œ: {title}")
        
        # 2. ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendment_history = ""
        amendments = []
        
        spirit_match = self.BASIC_SPIRIT_PATTERN.search(text)
        search_end = spirit_match.start() if spirit_match else 1000
        
        for match in self.AMENDMENT_PATTERN.finditer(text[:search_end]):
            amendments.append(match.group(0))
        
        if amendments:
            amendment_history = ' '.join(amendments)
            logger.info(f"   ğŸ“… ê°œì •ì´ë ¥: {len(amendments)}ê±´")
        
        return title, amendment_history, text
    
    def _normalize_linebreaks(self, text: str) -> str:
        """ì¤„ë°”ê¿ˆ ì •ë¦¬ (Phase 0.6)"""
        before = text
        after = self.SAFE_NEWLINE_PATTERN.sub('', text)
        removed_count = before.count('\n') - after.count('\n')
        
        if removed_count > 0:
            logger.info(f"      âœ‚ï¸ ë‹¨ì–´ ì¤‘ê°„ ì¤„ë°”ê¿ˆ ì œê±°: {removed_count}ê°œ")
        
        return after
    
    def _extract_basic_spirit(self, text: str) -> str:
        """ê¸°ë³¸ì •ì‹  ì¶”ì¶œ"""
        match = self.BASIC_SPIRIT_PATTERN.search(text)
        if not match:
            return ""
        
        start = match.end()
        
        next_article = self.ARTICLE_HEADER_PATTERN.search(text, start)
        next_chapter = self.CHAPTER_PATTERN.search(text, start)
        
        if next_article and next_chapter:
            end = min(next_article.start(), next_chapter.start())
        elif next_article:
            end = next_article.start()
        elif next_chapter:
            end = next_chapter.start()
        else:
            end = len(text)
        
        spirit_text = text[start:end].strip()
        return spirit_text
    
    def _extract_chapters(self, text: str) -> List[Chapter]:
        """
        âœ… Phase 0.6.3: ì¥ ì¶”ì¶œ (ì •ë°€ íŒŒì‹±)
        
        "ì œ1ì¥ ì´ì¹™" â†’ chapter_title="ì´ì¹™"ë§Œ
        (ì „ì²˜ë¦¬ë¡œ "ì œ1ì¥ ì´ì¹™\nì œ1ì¡°..." í˜•íƒœ ë³´ì¥ë¨)
        """
        chapters = []
        order = 0
        
        for match in self.CHAPTER_PATTERN.finditer(text):
            chapter_num_raw = match.group(1).strip()  # "ì œ 1 ì¥" â†’ "ì œ 1 ì¥"
            chapter_title_raw = match.group(2).strip()  # "ì´ì¹™ì œ" â†’ "ì´ì¹™"
            
            # ê³µë°± ì •ê·œí™”
            chapter_num = re.sub(r'\s+', '', chapter_num_raw)  # "ì œ1ì¥"
            
            # âœ… Phase 0.6.3: chapter_title ì •ì œ
            # "ì´ì¹™ì œ1ì¡°..." â†’ "ì´ì¹™"ë§Œ
            # ì´ë¯¸ ì „ì²˜ë¦¬ë¡œ "\nì œ1ì¡°" í˜•íƒœë¼ "ì œ"ê°€ ì•ˆ ë¶™ì§€ë§Œ, ë§Œì•½ì„ ìœ„í•´
            chapter_title = re.sub(r'ì œ\d+ì¡°.*$', '', chapter_title_raw).strip()
            
            chapter = Chapter(
                number=chapter_num,
                title=chapter_title,
                start_pos=match.start(),
                section_order=order
            )
            
            chapters.append(chapter)
            order += 1
            
            logger.info(f"   ğŸ“‚ {chapter.number} {chapter.title} (ìˆœì„œ: {chapter.section_order})")
        
        return chapters
    
    def _extract_articles_from_text(self, text: str) -> List[Article]:
        """ì¡°ë¬¸ ì¶”ì¶œ"""
        articles = []
        
        matches = list(self.ARTICLE_HEADER_PATTERN.finditer(text))
        
        if not matches:
            logger.warning("   âš ï¸ ì¡°ë¬¸ í—¤ë” ë¯¸ë°œê²¬")
            return articles
        
        logger.info(f"   ğŸ” ì¡°ë¬¸ í—¤ë”: {len(matches)}ê°œ ë°œê²¬")
        
        for i, match in enumerate(matches):
            article_num = match.group(1)
            article_sub = match.group(2)
            article_title = match.group(3)
            
            if article_sub:
                number = f"ì œ{article_num}ì¡°ì˜{article_sub}"
            else:
                number = f"ì œ{article_num}ì¡°"
            
            body_start = match.end()
            
            if i < len(matches) - 1:
                body_end = matches[i + 1].start()
            else:
                body_end = len(text)
            
            body = text[body_start:body_end].strip()
            
            article = Article(
                number=number,
                title=article_title,
                body=body,
                start_pos=match.start(),
                end_pos=body_end,
                chapter_number=None,
                section_order=0
            )
            
            articles.append(article)
            logger.info(f"   ğŸ“„ {number} ({article_title}): {len(body)}ì")
        
        return articles
    
    def _assign_chapters_to_articles(self, chapters: List[Chapter], articles: List[Article]) -> None:
        """ì¡°ë¬¸ì— chapter_number í• ë‹¹"""
        if not chapters:
            logger.info("   â„¹ï¸ ì¥ ì—†ìŒ - chapter_number í• ë‹¹ ê±´ë„ˆëœ€")
            return
        
        for article in articles:
            assigned_chapter = None
            for chapter in chapters:
                if chapter.start_pos < article.start_pos:
                    assigned_chapter = chapter.number
                else:
                    break
            
            article.chapter_number = assigned_chapter
            
            if assigned_chapter:
                logger.debug(f"      {article.number} â†’ {assigned_chapter}")
    
    def _assign_section_order(self, chapters: List[Chapter], articles: List[Article]) -> None:
        """section_order ë¶€ì—¬"""
        order = 0
        
        all_sections = []
        
        for chapter in chapters:
            all_sections.append(('chapter', chapter))
        
        for article in articles:
            all_sections.append(('article', article))
        
        all_sections.sort(key=lambda x: x[1].start_pos)
        
        for section_type, section in all_sections:
            section.section_order = order
            order += 1
        
        logger.info(f"   ğŸ”¢ section_order ë¶€ì—¬ ì™„ë£Œ: {order}ê°œ ì„¹ì…˜")
    
    def _remove_chapter_headers(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¥ í—¤ë” ì œê±°"""
        cleaned = self.CHAPTER_PATTERN.sub('', text)
        return cleaned.strip()
    
    def to_chunks(self, parsed_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        âœ… Phase 0.6.2: íŒŒì‹± ê²°ê³¼ â†’ ì²­í¬ ë³€í™˜
        
        ìˆœì„œ: title â†’ amendment_history â†’ basic â†’ chapter â†’ article
        """
        chunks = []
        
        # 0. íƒ€ì´í‹€ ì²­í¬
        if parsed_result['document_title']:
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # 1. ê°œì •ì´ë ¥ ì²­í¬
        if parsed_result['amendment_history']:
            chunks.append({
                'content': parsed_result['amendment_history'],
                'metadata': {
                    'type': 'amendment_history',
                    'boundary': 'header',
                    'title': 'ê°œì • ì´ë ¥',
                    'char_count': len(parsed_result['amendment_history']),
                    'section_order': -2
                }
            })
        
        # 2. ê¸°ë³¸ì •ì‹  ì²­í¬
        if parsed_result['basic_spirit']:
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic',
                    'boundary': 'basic_spirit',
                    'title': 'ê¸°ë³¸ì •ì‹ ',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # 3. ì¥(Chapter) ì²­í¬
        for chapter in parsed_result['chapters']:
            content = f"{chapter.number} {chapter.title}"
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'chapter',
                    'boundary': 'chapter',
                    'chapter_number': chapter.number,
                    'chapter_title': chapter.title,
                    'char_count': len(content),
                    'section_order': chapter.section_order
                }
            })
        
        # 4. ì¡°ë¬¸ ì²­í¬
        for article in parsed_result['articles']:
            content = f"{article.number}({article.title})\n{article.body}"
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        # section_order ì •ë ¬
        chunks.sort(key=lambda c: c['metadata'].get('section_order', 999))
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ (Phase 0.6.3): {len(chunks)}ê°œ")
        logger.info(f"   - íƒ€ì´í‹€: 1ê°œ")
        logger.info(f"   - ê°œì •ì´ë ¥: 1ê°œ")
        logger.info(f"   - ê¸°ë³¸ì •ì‹ : 1ê°œ")
        logger.info(f"   - ì¥: {parsed_result['total_chapters']}ê°œ")
        logger.info(f"   - ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
        
        return chunks
    
    def to_markdown(self, parsed_result: Dict[str, Any]) -> str:
        """íŒŒì‹± ê²°ê³¼ â†’ Markdown ë³€í™˜"""
        lines = []
        
        # íƒ€ì´í‹€
        if parsed_result['document_title']:
            lines.append(f"# {parsed_result['document_title']}\n")
        
        # ê°œì •ì´ë ¥
        if parsed_result['amendment_history']:
            lines.append("## ê°œì • ì´ë ¥\n")
            lines.append(parsed_result['amendment_history'])
            lines.append("")
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result['basic_spirit']:
            lines.append("## ê¸°ë³¸ì •ì‹ \n")
            lines.append(parsed_result['basic_spirit'])
            lines.append("")
        
        # ì¥ê³¼ ì¡°ë¬¸
        for chapter in parsed_result['chapters']:
            lines.append(f"## {chapter.number} {chapter.title}\n")
            
            # í•´ë‹¹ ì¥ì˜ ì¡°ë¬¸ë“¤
            for article in parsed_result['articles']:
                if article.chapter_number == chapter.number:
                    lines.append(f"### {article.number}({article.title})\n")
                    lines.append(article.body)
                    lines.append("")
        
        # ì¥ì´ ì—†ëŠ” ì¡°ë¬¸ë“¤ (ìˆìœ¼ë©´)
        orphan_articles = [a for a in parsed_result['articles'] if not a.chapter_number]
        if orphan_articles:
            for article in orphan_articles:
                lines.append(f"### {article.number}({article.title})\n")
                lines.append(article.body)
                lines.append("")
        
        return '\n'.join(lines)


# ============================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================

if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸
    test_text = """
ì¸ì‚¬ê·œì •
ì œ37ì°¨ê°œì •2019.05.27. ì œ38ì°¨ê°œì •2019.07.01.

ê¸°ë³¸ì •ì‹ 
ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬ ì§ì›ì˜ ...

ì œ1ì¥ ì´ì¹™ì œ1ì¡°(ëª©ì )
ì´ ê·œì •ì€ ...

ì œ2ì¡°(ì ìš©ë²”ìœ„)
ì§ì›ì˜ ì¸ì‚¬ê´€ë¦¬ëŠ” ...

ì œ2ì¥ ì±„ìš©ì œ7ì¡°(ì±„ìš©ì˜ì›ì¹™)
ì§ì›ì€ ê³µê°œê²½ìŸì±„ìš©ì‹œí—˜ì— ë”°ë¼ ...
"""
    
    parser = LawParser()
    result = parser.parse(
        pdf_text=test_text,
        document_title="ì¸ì‚¬ê·œì • í…ŒìŠ¤íŠ¸"
    )
    
    print("\n" + "="*60)
    print("Phase 0.6.3 í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"íƒ€ì´í‹€: {result['document_title']}")
    print(f"ê°œì •ì´ë ¥: {len(result['amendment_history'])}ì")
    print(f"ì¥: {result['total_chapters']}ê°œ")
    for ch in result['chapters']:
        print(f"  - {ch.number} {ch.title}")
    print(f"ì¡°ë¬¸: {result['total_articles']}ê°œ")
    
    chunks = parser.to_chunks(result)
    print(f"\nì²­í¬: {len(chunks)}ê°œ")
    for chunk in chunks[:5]:
        print(f"  - {chunk['metadata']['type']}: {chunk['metadata'].get('title', 'N/A')}")