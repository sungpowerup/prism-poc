"""
law_parser.py - LawMode ì¡°ë¬¸ íŒŒì„œ
Phase 0.6.3 â†’ Phase 0.5 Rollback (í…ìŠ¤íŠ¸ í’ˆì§ˆ ë³µêµ¬)

âœ… Phase 0.5 ë³µêµ¬ í•«í”½ìŠ¤ (GPT ê¶Œì¥):
1. _normalize_linebreaks(): sub('', text) â†’ sub(' ', text) (ê³µë°± ë³´ì¡´!)
2. parse() ê¸°ë³¸ê°’: normalize_linebreaks=True â†’ False (0.5 ìŠ¤íƒ€ì¼)

ğŸ¯ ëª©í‘œ:
- êµ¬ì¡° ë©”íƒ€ = Phase 0.6 ìœ ì§€ (íƒ€ì´í‹€/ì¥/ì¡°ë¬¸/section_order)
- í…ìŠ¤íŠ¸ í’ˆì§ˆ = Phase 0.5 ë³µêµ¬ (ë„ì–´ì“°ê¸° ë³´ì¡´)

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + GPT ìµœì¢… ê¶Œì¥
Date: 2025-11-14
Version: Phase 0.5 Rollback
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class Chapter:
    """ì¥ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: str
    start_pos: int
    section_order: int


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: Optional[str]
    body: str
    start_pos: int
    end_pos: int
    chapter_number: Optional[str] = None
    section_order: int = 0
    article_type: str = 'article'


class LawParser:
    """
    ê·œì •/ë²•ë ¹ ì „ìš© íŒŒì„œ (Phase 0.5 Rollback)
    
    âœ… Phase 0.5 ë³µêµ¬:
    - í…ìŠ¤íŠ¸ í’ˆì§ˆ = Phase 0.5 ìˆ˜ì¤€ (ë„ì–´ì“°ê¸° ë³´ì¡´)
    - êµ¬ì¡° ë©”íƒ€ = Phase 0.6 ìœ ì§€ (íƒ€ì´í‹€/ì¥/ì¡°ë¬¸)
    """
    
    # ê¸°ë³¸ì •ì‹  íŒ¨í„´
    BASIC_SPIRIT_PATTERN = re.compile(
        r'ê¸°\s*ë³¸\s*ì •\s*ì‹ ',
        re.MULTILINE | re.IGNORECASE
    )
    
    # ì¡°ë¬¸ í—¤ë” íŒ¨í„´
    ARTICLE_HEADER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¡°(?:\s*ì˜\s*(\d+))?\s*\(([^)]+)\)',
        re.MULTILINE
    )
    
    # âœ… Phase 0.6.3: ì¥ íŒ¨í„´ (ì „ì²˜ë¦¬ í›„ ì‚¬ìš©)
    # "ì œ1ì¥" + ë‹¤ìŒ "ì œ" ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ í•œê¸€ë§Œ
    CHAPTER_PATTERN = re.compile(
        r'(ì œ\s*\d+\s*ì¥)\s*([^\nì œ]+)',
        re.MULTILINE
    )
    
    # íƒ€ì´í‹€ íŒ¨í„´
    TITLE_PATTERN = re.compile(
        r'^([ê°€-í£]{2,10}ê·œì •|[ê°€-í£]{2,10}ë‚´ê·œ|[ê°€-í£]{2,10}ì •ê´€)',
        re.MULTILINE
    )
    
    # ê°œì •ì¼ íŒ¨í„´
    AMENDMENT_PATTERN = re.compile(
        r'(ì œ\d+ì°¨\s*)?ê°œì •\s*\d{4}\.\d{1,2}\.\d{1,2}\.?',
        re.MULTILINE
    )
    
    # ì¤„ë°”ê¿ˆ ì •ë¦¬ íŒ¨í„´
    SAFE_NEWLINE_PATTERN = re.compile(
        r'(?<=[ê°€-í£0-9])(?<![.?!])[\r]?\n(?=[ê°€-í£0-9])',
        re.MULTILINE
    )
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… LawParser ì´ˆê¸°í™” (Phase 0.5 Rollback)")
        logger.info("   ğŸ“Œ í…ìŠ¤íŠ¸ í’ˆì§ˆ: Phase 0.5 (ë„ì–´ì“°ê¸° ë³´ì¡´)")
        logger.info("   ğŸ“Œ êµ¬ì¡° ë©”íƒ€: Phase 0.6 (íƒ€ì´í‹€/ì¥/ì¡°ë¬¸)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        enacted_date: Optional[str] = None,
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = False  # âœ… Phase 0.5: ê¸°ë³¸ OFF
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì¡°ë¬¸ êµ¬ì¡° ì¶”ì¶œ
        
        Args:
            pdf_text: PDFì—ì„œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸
            document_title: ë¬¸ì„œ ì œëª©
            enacted_date: ì œì •ì¼
            clean_artifacts: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° ì—¬ë¶€
            normalize_linebreaks: ì¤„ë°”ê¿ˆ ì •ë¦¬ ì—¬ë¶€ (âœ… False = Phase 0.5 ìŠ¤íƒ€ì¼)
        
        Returns:
            {
                'document_title': str,
                'enacted_date': str,
                'amendment_history': str,
                'basic_spirit': str,
                'chapters': List[Chapter],
                'articles': List[Article],
                'total_articles': int,
                'total_chapters': int
            }
        """
        logger.info(f"ğŸ“œ LawParser Phase 0.5 Rollback ì‹œì‘: {document_title}")
        logger.info(f"   ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸: {len(pdf_text)}ì")
        logger.info(f"   ğŸ”§ normalize_linebreaks: {normalize_linebreaks} (False = 0.5 ìŠ¤íƒ€ì¼)")
        
        # Phase 0.5: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        if clean_artifacts:
            try:
                from .page_cleaner import clean_page_artifacts
                pdf_text = clean_page_artifacts(pdf_text)
                logger.info(f"   ğŸ§¹ ì•„í‹°íŒ©íŠ¸ ì œê±° í›„: {len(pdf_text)}ì")
            except ImportError:
                logger.warning("   âš ï¸ PageCleaner ë¯¸ì„¤ì¹˜ - ê±´ë„ˆëœ€")
        
        # Phase 0.5/0.6: ì¤„ë°”ê¿ˆ ì •ë¦¬ (ì„ íƒ)
        if normalize_linebreaks:
            original_len = len(pdf_text)
            pdf_text = self._normalize_linebreaks(pdf_text)
            logger.info(f"   âœ‚ï¸ ì¤„ë°”ê¿ˆ ì •ë¦¬: {original_len}ì â†’ {len(pdf_text)}ì ({len(pdf_text)-original_len:+d})")
        else:
            logger.info(f"   â¸ï¸ ì¤„ë°”ê¿ˆ ì •ë¦¬ ê±´ë„ˆëœ€ (Phase 0.5 ëª¨ë“œ)")
        
        # âœ… Phase 0.6.3: ì¥/ì¡°ë¬¸ ì „ì²˜ë¦¬ (GPT ê¶Œì¥)
        pdf_text = self._preprocess_for_chapter_parsing(pdf_text)
        
        # Front Matter ì¶”ì¶œ
        title, amendment_history, pdf_text = self._extract_front_matter(pdf_text, document_title)
        
        # 1. ê¸°ë³¸ì •ì‹  ì¶”ì¶œ
        basic_spirit = self._extract_basic_spirit(pdf_text)
        logger.info(f"   âœ… ê¸°ë³¸ì •ì‹ : {len(basic_spirit)}ì")
        
        # 2. ì¥ ì¶”ì¶œ (Phase 0.6.3 ì •ë°€)
        chapters = self._extract_chapters(pdf_text)
        logger.info(f"   âœ… ì¥: {len(chapters)}ê°œ")
        
        # 3. ì¡°ë¬¸ ì¶”ì¶œ
        articles = self._extract_articles_from_text(pdf_text)
        logger.info(f"   âœ… ì¡°ë¬¸: {len(articles)}ê°œ")
        
        # 4. ì¡°ë¬¸ì— chapter_number í• ë‹¹
        self._assign_chapters_to_articles(chapters, articles)
        
        # 5. section_order ë¶€ì—¬
        self._assign_section_order(chapters, articles)
        
        # 6. ê¸°ë³¸ì •ì‹ /ì¡°ë¬¸ì—ì„œ ì¥ í—¤ë” ì œê±°
        basic_spirit = self._remove_chapter_headers(basic_spirit)
        for article in articles:
            article.body = self._remove_chapter_headers(article.body)
        
        logger.info(f"âœ… LawParser Phase 0.5 Rollback ì™„ë£Œ: {len(chapters)}ê°œ ì¥, {len(articles)}ê°œ ì¡°ë¬¸")
        
        return {
            'document_title': title,
            'enacted_date': enacted_date or '',
            'amendment_history': amendment_history,
            'basic_spirit': basic_spirit,
            'chapters': chapters,
            'articles': articles,
            'total_articles': len(articles),
            'total_chapters': len(chapters)
        }
    
    def _normalize_linebreaks(self, text: str) -> str:
        """
        âœ… Phase 0.5 Rollback: ì•ˆì „ ëª¨ë“œ ì¤„ë°”ê¿ˆ ì •ë¦¬
        
        ğŸ”¥ í•µì‹¬ ìˆ˜ì •:
        - sub('', text) â†’ sub(' ', text)
        - ì¤„ë°”ê¿ˆ ì œê±° ì‹œ ê³µë°±(' ') ì‚½ì…í•˜ì—¬ ë‹¨ì–´ ë¶™ìŒ ë°©ì§€
        
        Before (Phase 0.6):
            "ì´ ê·œì •ì€\ní•œêµ­ë†ì–´ì´Œê³µì‚¬" â†’ "ì´ê·œì •ì€í•œêµ­ë†ì–´ì´Œê³µì‚¬" âŒ
        
        After (Phase 0.5):
            "ì´ ê·œì •ì€\ní•œêµ­ë†ì–´ì´Œê³µì‚¬" â†’ "ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬" âœ…
        """
        before = text
        
        # âœ… í•µì‹¬: ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ (ê³µë°± ë³´ì¡´!)
        after = self.SAFE_NEWLINE_PATTERN.sub(' ', text)  # '' â†’ ' ' (ê³µë°± í•˜ë‚˜!)
        #                                       ^
        #                                  Phase 0.5 í•µì‹¬ ìˆ˜ì •!
        
        removed_count = before.count('\n') - after.count('\n')
        
        if removed_count > 0:
            logger.info(f"      âœ‚ï¸ ë‹¨ì–´ ì¤‘ê°„ ì¤„ë°”ê¿ˆ ì œê±°: {removed_count}ê°œ (âœ… ê³µë°± ë³´ì¡´)")
        
        return after
    
    def _preprocess_for_chapter_parsing(self, text: str) -> str:
        """
        âœ… Phase 0.6.3: ì¥/ì¡°ë¬¸ íŒŒì‹± ì „ì²˜ë¦¬ (GPT ê¶Œì¥)
        
        "ì œ1ì¥ ì´ì¹™ì œ1ì¡°(ëª©ì )" â†’ "ì œ1ì¥ ì´ì¹™\nì œ1ì¡°(ëª©ì )"
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # 1. ì œNì¥ ì•ë’¤ ì¤„ë°”ê¿ˆ ê°•ì œ
        text = re.sub(r'(ì œ\s*\d+\s*ì¥)', r'\n\1 ', text)
        
        # 2. ì œNì¡° ì• ì¤„ë°”ê¿ˆ ê°•ì œ
        text = re.sub(r'(ì œ\s*\d+\s*ì¡°)', r'\n\1', text)
        
        logger.debug("   ğŸ”§ ì¥/ì¡°ë¬¸ ì „ì²˜ë¦¬ ì™„ë£Œ (ì¤„ë°”ê¿ˆ ê°•ì œ)")
        
        return text
    
    def _extract_front_matter(
        self, 
        text: str, 
        fallback_title: str = ""
    ) -> Tuple[str, str, str]:
        """
        âœ… Phase 0.6.2: ë¬¸ì„œ Front Matter ì¶”ì¶œ
        
        Returns:
            (title, amendment_history, remaining_text)
        """
        # 1. íƒ€ì´í‹€ ì¶”ì¶œ
        title = fallback_title
        title_match = self.TITLE_PATTERN.search(text[:500])
        if title_match:
            title = title_match.group(1)
            logger.info(f"   ğŸ“Œ íƒ€ì´í‹€ ì¶”ì¶œ: {title}")
        
        # 2. ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendment_history = ""
        amendments = []
        
        spirit_match = self.BASIC_SPIRIT_PATTERN.search(text)
        search_end = spirit_match.start() if spirit_match else 1000
        
        for match in self.AMENDMENT_PATTERN.finditer(text[:search_end]):
            amendments.append(match.group(0))
        
        if amendments:
            amendment_history = ' '.join(amendments)
            logger.info(f"   ğŸ“… ê°œì •ì´ë ¥: {len(amendments)}ê±´")
        
        return title, amendment_history, text
    
    def _extract_basic_spirit(self, text: str) -> str:
        """ê¸°ë³¸ì •ì‹  ì¶”ì¶œ"""
        match = self.BASIC_SPIRIT_PATTERN.search(text)
        if not match:
            return ""
        
        start = match.end()
        
        next_article = self.ARTICLE_HEADER_PATTERN.search(text, start)
        next_chapter = self.CHAPTER_PATTERN.search(text, start)
        
        if next_article and next_chapter:
            end = min(next_article.start(), next_chapter.start())
        elif next_article:
            end = next_article.start()
        elif next_chapter:
            end = next_chapter.start()
        else:
            end = len(text)
        
        spirit_text = text[start:end].strip()
        return spirit_text
    
    def _extract_chapters(self, text: str) -> List[Chapter]:
        """
        âœ… Phase 0.6.3: ì¥ ì¶”ì¶œ (ì •ë°€ íŒŒì‹±)
        
        "ì œ1ì¥ ì´ì¹™" â†’ chapter_title="ì´ì¹™"ë§Œ
        (ì „ì²˜ë¦¬ë¡œ "ì œ1ì¥ ì´ì¹™\nì œ1ì¡°..." í˜•íƒœ ë³´ì¥ë¨)
        """
        chapters = []
        order = 0
        
        for match in self.CHAPTER_PATTERN.finditer(text):
            chapter_num_raw = match.group(1).strip()  # "ì œ 1 ì¥" â†’ "ì œ 1 ì¥"
            chapter_title_raw = match.group(2).strip()  # "ì´ì¹™ì œ" â†’ "ì´ì¹™"
            
            # ê³µë°± ì •ê·œí™”
            chapter_num = re.sub(r'\s+', '', chapter_num_raw)  # "ì œ1ì¥"
            
            # âœ… Phase 0.6.3: chapter_title ì •ì œ
            # "ì´ì¹™ì œ1ì¡°..." â†’ "ì´ì¹™"ë§Œ
            # ì´ë¯¸ ì „ì²˜ë¦¬ë¡œ "\nì œ1ì¡°" í˜•íƒœë¼ "ì œ"ê°€ ì•ˆ ë¶™ì§€ë§Œ, ë§Œì•½ì„ ìœ„í•´
            chapter_title = re.sub(r'ì œ\d+ì¡°.*$', '', chapter_title_raw).strip()
            
            chapter = Chapter(
                number=chapter_num,
                title=chapter_title,
                start_pos=match.start(),
                section_order=order
            )
            chapters.append(chapter)
            
            logger.info(f"   ğŸ“‚ {chapter_num} {chapter_title} (ìˆœì„œ: {order})")
            order += 1
        
        return chapters
    
    def _extract_articles_from_text(self, text: str) -> List[Article]:
        """ì¡°ë¬¸ ì¶”ì¶œ"""
        articles = []
        matches = list(self.ARTICLE_HEADER_PATTERN.finditer(text))
        
        for i, match in enumerate(matches):
            article_num = match.group(1)
            article_title = match.group(3)
            article_full_num = f"ì œ{article_num}ì¡°"
            
            if match.group(2):
                article_full_num += f"ì˜{match.group(2)}"
            
            start_pos = match.start()
            
            if i + 1 < len(matches):
                end_pos = matches[i + 1].start()
            else:
                end_pos = len(text)
            
            body = text[start_pos:end_pos].strip()
            
            article = Article(
                number=article_full_num,
                title=article_title,
                body=body,
                start_pos=start_pos,
                end_pos=end_pos
            )
            articles.append(article)
        
        return articles
    
    def _assign_chapters_to_articles(self, chapters: List[Chapter], articles: List[Article]) -> None:
        """ì¡°ë¬¸ì— chapter_number í• ë‹¹"""
        if not chapters:
            return
        
        for article in articles:
            assigned_chapter = None
            
            for i, chapter in enumerate(chapters):
                if article.start_pos >= chapter.start_pos:
                    assigned_chapter = chapter.number
                else:
                    break
            
            article.chapter_number = assigned_chapter
            
            if assigned_chapter:
                logger.debug(f"      {article.number} â†’ {assigned_chapter}")
    
    def _assign_section_order(self, chapters: List[Chapter], articles: List[Article]) -> None:
        """section_order ë¶€ì—¬"""
        order = 0
        
        all_sections = []
        
        for chapter in chapters:
            all_sections.append(('chapter', chapter))
        
        for article in articles:
            all_sections.append(('article', article))
        
        all_sections.sort(key=lambda x: x[1].start_pos)
        
        for section_type, section in all_sections:
            section.section_order = order
            order += 1
        
        logger.info(f"   ğŸ”¢ section_order ë¶€ì—¬ ì™„ë£Œ: {order}ê°œ ì„¹ì…˜")
    
    def _remove_chapter_headers(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¥ í—¤ë” ì œê±°"""
        cleaned = self.CHAPTER_PATTERN.sub('', text)
        return cleaned.strip()
    
    def to_chunks(self, parsed_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        âœ… Phase 0.6.2: íŒŒì‹± ê²°ê³¼ â†’ ì²­í¬ ë³€í™˜
        
        ìˆœì„œ: title â†’ amendment_history â†’ basic â†’ chapter â†’ article
        """
        chunks = []
        
        # 0. íƒ€ì´í‹€ ì²­í¬
        if parsed_result['document_title']:
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # 1. ê°œì •ì´ë ¥ ì²­í¬
        if parsed_result['amendment_history']:
            chunks.append({
                'content': parsed_result['amendment_history'],
                'metadata': {
                    'type': 'amendment_history',
                    'boundary': 'header',
                    'title': 'ê°œì • ì´ë ¥',
                    'char_count': len(parsed_result['amendment_history']),
                    'section_order': -2
                }
            })
        
        # 2. ê¸°ë³¸ì •ì‹  ì²­í¬
        if parsed_result['basic_spirit']:
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic',
                    'boundary': 'basic_spirit',
                    'title': 'ê¸°ë³¸ì •ì‹ ',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # 3. ì¥ ì²­í¬
        for chapter in parsed_result['chapters']:
            content = f"{chapter.number} {chapter.title}"
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'chapter',
                    'boundary': 'chapter',
                    'chapter_number': chapter.number,
                    'chapter_title': chapter.title,
                    'char_count': len(content),
                    'section_order': chapter.section_order
                }
            })
        
        # 4. ì¡°ë¬¸ ì²­í¬
        for article in parsed_result['articles']:
            content = f"{article.number}({article.title})\n{article.body}"
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        chunks.sort(key=lambda c: c['metadata'].get('section_order', 999))
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ (Phase 0.5 Rollback): {len(chunks)}ê°œ")
        logger.info(f"   - íƒ€ì´í‹€: 1ê°œ")
        logger.info(f"   - ê°œì •ì´ë ¥: 1ê°œ")
        logger.info(f"   - ê¸°ë³¸ì •ì‹ : 1ê°œ")
        logger.info(f"   - ì¥: {parsed_result['total_chapters']}ê°œ")
        logger.info(f"   - ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
        
        return chunks
    
    def to_markdown(self, parsed_result: Dict[str, Any]) -> str:
        """
        âœ… Phase 0.6.2: íŒŒì‹± ê²°ê³¼ â†’ Markdown ë³€í™˜
        
        êµ¬ì¡°:
        # íƒ€ì´í‹€
        ## ê°œì • ì´ë ¥
        - ê°œì •1
        - ê°œì •2
        ## ê¸°ë³¸ì •ì‹ 
        ...
        ## ì œ1ì¥ ì´ì¹™
        ### ì œ1ì¡°(ëª©ì )
        ...
        """
        lines = []
        
        # 1. íƒ€ì´í‹€
        if parsed_result['document_title']:
            lines.append(f"# {parsed_result['document_title']}")
            lines.append("")
        
        # 2. ê°œì •ì´ë ¥
        if parsed_result['amendment_history']:
            lines.append("## ê°œì • ì´ë ¥")
            lines.append("")
            
            amendments = parsed_result['amendment_history'].split()
            for amendment in amendments:
                lines.append(f"- {amendment}")
            
            lines.append("")
        
        # 3. ê¸°ë³¸ì •ì‹ 
        if parsed_result['basic_spirit']:
            lines.append("## ê¸°ë³¸ì •ì‹ ")
            lines.append("")
            lines.append(parsed_result['basic_spirit'])
            lines.append("")
        
        # 4. ì¥/ì¡°ë¬¸ (section_order ìˆœ)
        all_sections = []
        
        for chapter in parsed_result['chapters']:
            all_sections.append(('chapter', chapter))
        
        for article in parsed_result['articles']:
            all_sections.append(('article', article))
        
        all_sections.sort(key=lambda x: x[1].section_order)
        
        for section_type, section in all_sections:
            if section_type == 'chapter':
                lines.append(f"## {section.number} {section.title}")
                lines.append("")
            else:
                lines.append(f"### {section.number}({section.title})")
                lines.append("")
                lines.append(section.body)
                lines.append("")
        
        markdown = "\n".join(lines)
        
        logger.info(f"âœ… Markdown ë³€í™˜ ì™„ë£Œ: {len(markdown)}ì")
        
        return markdown
    
    def to_review_md(self, parsed_result: Dict[str, Any]) -> str:
        """
        âœ… Phase 0.6.2: ë¦¬ë·°ìš© Markdown (to_markdownê³¼ ë™ì¼)
        
        Phase 0.7/0.9ì—ì„œ ì¶”ê°€ ë·° ë ˆì´ì–´ ì ìš© ê°€ëŠ¥
        """
        return self.to_markdown(parsed_result)