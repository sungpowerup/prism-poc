"""
core/law_parser.py - PRISM Phase 0.8.4 Final Fix
ë³¸ë¬¸ ì¡°ë¬¸ ì†ì‹¤ ë²„ê·¸ ì™„ì „ ìˆ˜ì •

Phase 0.8.4 í•µì‹¬ ìˆ˜ì •:
- âœ… ë²„ê·¸ FIX: TreeBuilderëŠ” 'level'ì„ ì‚¬ìš©, LawParserëŠ” 'type'ì„ ì°¾ëŠ” ë¶ˆì¼ì¹˜ í•´ê²°
- âœ… node.get('level') ë˜ëŠ” node.get('type') ëª¨ë‘ ì¸ì‹
- âœ… 'article_no' vs 'article_number' í•„ë“œëª… í˜¸í™˜

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-19
Version: Phase 0.8.4 Final Fix
"""

import re
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# Annex ì„œë¸Œì²­í‚¹
try:
    from core.annex_subchunker import AnnexSubChunker, validate_subchunks
    ANNEX_SUBCHUNKING_AVAILABLE = True
except ImportError:
    ANNEX_SUBCHUNKING_AVAILABLE = False
    logger.warning("âš ï¸ AnnexSubChunker ë¯¸ì„¤ì¹˜ - Annex ì„œë¸Œì²­í‚¹ ë¹„í™œì„±í™”")

# TreeBuilder Import
try:
    from core.tree_builder import TreeBuilder
    TREE_BUILDER_AVAILABLE = True
except ImportError:
    TREE_BUILDER_AVAILABLE = False
    logger.error("âŒ TreeBuilder í•„ìˆ˜ - import ì‹¤íŒ¨")


@dataclass
class Chapter:
    """ì¥ ë°ì´í„°"""
    number: str
    title: str
    section_order: int = 0


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„°"""
    number: str
    title: str
    body: str
    chapter_number: str = ""
    section_order: int = 0


class LawParser:
    """
    Phase 0.8.4 LawParser Final Fix
    
    í•µì‹¬ ìˆ˜ì •:
    - TreeBuilder ë…¸ë“œì˜ 'level' í•„ë“œì™€ 'type' í•„ë“œ ëª¨ë‘ ì¸ì‹
    - ë‹¤ì–‘í•œ í•„ë“œëª… í˜¸í™˜ (article_no/article_number ë“±)
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        if not TREE_BUILDER_AVAILABLE:
            raise ImportError("TreeBuilder is required but not available")
        
        self.tree_builder = TreeBuilder()
        logger.info("âœ… LawParser ì´ˆê¸°í™” ì™„ë£Œ (Phase 0.8.4 Final Fix)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = True
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ íŒŒì‹± (ë©”ì¸ ë©”ì„œë“œ)
        """
        logger.info(f"ğŸ“œ LawParser.parse() ì‹œì‘: {document_title}")
        
        # ì „ì²˜ë¦¬
        cleaned_text = pdf_text
        if normalize_linebreaks:
            cleaned_text = cleaned_text.replace('\r\n', '\n')
        
        # TreeBuilderë¡œ íŒŒì‹±
        tree_result = self.tree_builder.build(
            markdown=cleaned_text,
            document_title=document_title,
            enacted_date=None
        )
        
        # TreeBuilder ê²°ê³¼ â†’ LawParser í¬ë§· ë³€í™˜
        parsed_result = self._convert_tree_to_result(tree_result, document_title)
        
        # ë³¸ë¬¸ + Annex ì²˜ë¦¬
        has_articles = parsed_result.get('total_articles', 0) > 0
        has_annex_pattern = '[ë³„í‘œ' in cleaned_text
        
        if has_articles:
            logger.info(f"   ğŸ“‹ ë³¸ë¬¸ ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
            
            # Annexë„ ìˆìœ¼ë©´ ì¶”ì¶œ
            if has_annex_pattern and not parsed_result.get('annex_content'):
                logger.info("   ğŸ“‹ í˜¼í•© ë¬¸ì„œ - ë³¸ë¬¸ + Annex ë™ì‹œ ì²˜ë¦¬")
                self._apply_annex_extraction(cleaned_text, parsed_result)
        else:
            # ì¡°ë¬¸ì´ ì—†ìœ¼ë©´ Annex-only
            if len(cleaned_text) > 500:
                logger.warning("ğŸ”„ Annex-only ë¬¸ì„œ ê°ì§€ - Fallback Annex íŒŒì„œ ê°€ë™")
                self._apply_annex_fallback(cleaned_text, parsed_result)
        
        logger.info(f"âœ… LawParser.parse() ì™„ë£Œ:")
        logger.info(f"   - ì¥: {parsed_result['total_chapters']}ê°œ")
        logger.info(f"   - ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
        if parsed_result.get('annex_content'):
            logger.info(f"   - Annex: {len(parsed_result['annex_content'])}ì")
        
        return parsed_result
    
    def _convert_tree_to_result(
        self, 
        tree_result: Dict[str, Any],
        document_title: str
    ) -> Dict[str, Any]:
        """
        TreeBuilder ê²°ê³¼ â†’ LawParser í‘œì¤€ í¬ë§· ë³€í™˜
        
        âœ… Phase 0.8.4 í•µì‹¬ ìˆ˜ì •:
        - node.get('level') ë˜ëŠ” node.get('type') ëª¨ë‘ ì¸ì‹
        - ë‹¤ì–‘í•œ í•„ë“œëª… í˜¸í™˜
        """
        
        # Tree ì¶”ì¶œ
        tree = tree_result.get('document', {}).get('tree', [])
        
        logger.info(f"   ğŸŒ² TreeBuilder ë…¸ë“œ ìˆ˜: {len(tree)}ê°œ")
        
        chapters = []
        articles = []
        amendment_history = []
        basic_spirit = ""
        annex_content = ""
        
        current_chapter = ""
        section_order = 0
        
        for node in tree:
            # âœ… Phase 0.8.4: 'level' ë˜ëŠ” 'type' ëª¨ë‘ ì¸ì‹
            node_type = node.get('level', node.get('type', ''))
            
            if node_type == 'chapter':
                chapter = Chapter(
                    number=node.get('chapter_number', node.get('chapter', node.get('number', ''))),
                    title=node.get('chapter_title', node.get('title', '')),
                    section_order=section_order
                )
                chapters.append(chapter)
                current_chapter = chapter.number
                section_order += 1
                logger.debug(f"      ì¥ ë³€í™˜: {chapter.number} {chapter.title}")
            
            elif node_type == 'article':
                # âœ… Phase 0.8.4: ë‹¤ì–‘í•œ í•„ë“œëª… ì§€ì›
                article_no = node.get('article_no', node.get('article_number', node.get('number', '')))
                article_title = node.get('article_title', node.get('title', ''))
                article_body = node.get('content', node.get('body', ''))
                
                # ìì‹ ë…¸ë“œ(í•­Â·í˜¸)ì˜ ë‚´ìš©ë„ í¬í•¨
                if 'children' in node:
                    for child in node.get('children', []):
                        child_content = child.get('content', '')
                        child_no = child.get('clause_no', child.get('item_no', ''))
                        if child_content:
                            article_body += f"\n{child_no} {child_content}"
                
                article = Article(
                    number=article_no,
                    title=article_title,
                    body=article_body.strip(),
                    chapter_number=node.get('chapter', current_chapter),
                    section_order=section_order
                )
                articles.append(article)
                section_order += 1
                logger.debug(f"      ì¡°ë¬¸ ë³€í™˜: {article_no}({article_title})")
            
            elif node_type == 'amendment_history':
                amendment_history.append(node.get('content', ''))
            
            elif node_type == 'basic_spirit':
                basic_spirit = node.get('content', '')
            
            elif node_type == 'annex':
                annex_content = node.get('content', '')
        
        logger.info(f"   ğŸ“Š ë³€í™˜ ê²°ê³¼: ì¥ {len(chapters)}ê°œ, ì¡°ë¬¸ {len(articles)}ê°œ")
        
        return {
            'document_title': document_title,
            'chapters': chapters,
            'articles': articles,
            'amendment_history': amendment_history,
            'basic_spirit': basic_spirit,
            'annex_content': annex_content,
            'annex_title': '',
            'annex_no': None,
            'related_article': None,
            'annex_tables': [],
            'total_chapters': len(chapters),
            'total_articles': len(articles)
        }
    
    def _apply_annex_fallback(self, cleaned_text: str, parsed_result: dict):
        """Annex-only ë¬¸ì„œ Fallback"""
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… Fallback Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
            
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
    
    def _apply_annex_extraction(self, cleaned_text: str, parsed_result: dict):
        """ë³¸ë¬¸+Annex í˜¼í•© ë¬¸ì„œì—ì„œ Annex ì¶”ì¶œ"""
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… í˜¼í•© ë¬¸ì„œ Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
            
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
    
    def to_chunks(self, parsed_result: dict) -> list:
        """íŒŒì‹± ê²°ê³¼ â†’ RAG ì²­í¬ ë³€í™˜"""
        
        chunks = []
        
        # ë¬¸ì„œ ì œëª©
        if parsed_result.get('document_title'):
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic_spirit',
                    'boundary': 'basic_spirit',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # ì¥
        for chapter in parsed_result.get('chapters', []):
            content = f"{chapter.number} {chapter.title}"
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'chapter',
                    'boundary': 'chapter',
                    'chapter_number': chapter.number,
                    'chapter_title': chapter.title,
                    'char_count': len(content),
                    'section_order': chapter.section_order
                }
            })
        
        # âœ… ì¡°ë¬¸ (í•µì‹¬!)
        for article in parsed_result.get('articles', []):
            content = f"{article.number}({article.title})\n{article.body}"
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        # Annex ì„œë¸Œì²­í‚¹
        if parsed_result.get('annex_content') and ANNEX_SUBCHUNKING_AVAILABLE:
            logger.info("âœ… Phase 0.8: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘")
            
            subchunker = AnnexSubChunker()
            annex_text = parsed_result['annex_content']
            
            try:
                sub_chunks = subchunker.chunk(annex_text)
                validation = validate_subchunks(sub_chunks, len(annex_text))
                
                if validation['is_valid']:
                    logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì„±ê³µ: {validation['chunk_count']}ê°œ")
                    
                    for sub in sub_chunks:
                        chunks.append({
                            'content': sub.content,
                            'metadata': {
                                'type': f"annex_{sub.section_type}",
                                'boundary': 'annex',
                                'section_id': sub.section_id,
                                'section_type': sub.section_type,
                                'char_count': sub.char_count,
                                'section_order': sub.order,
                                **sub.metadata
                            }
                        })
                else:
                    raise ValueError("ê²€ì¦ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Annex ì„œë¸Œì²­í‚¹ ì‹¤íŒ¨: {e}")
                chunks.append({
                    'content': annex_text,
                    'metadata': {
                        'type': 'annex',
                        'boundary': 'annex',
                        'char_count': len(annex_text),
                        'section_order': 1000
                    }
                })
        
        elif parsed_result.get('annex_content'):
            chunks.append({
                'content': parsed_result['annex_content'],
                'metadata': {
                    'type': 'annex',
                    'boundary': 'annex',
                    'char_count': len(parsed_result['annex_content']),
                    'section_order': 1000
                }
            })
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for chunk in chunks:
            ctype = chunk['metadata']['type']
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")
        
        return chunks
    
    def to_markdown(self, parsed_result: dict) -> str:
        """íŒŒì‹± ê²°ê³¼ â†’ Markdown ë³€í™˜"""
        
        lines = []
        
        # ì œëª©
        if parsed_result.get('document_title'):
            lines.append(f"# {parsed_result['document_title']}")
            lines.append("")
        
        # ê°œì •ì´ë ¥
        if parsed_result.get('amendment_history'):
            lines.append("## ê°œì • ì´ë ¥")
            lines.append("")
            for amendment in parsed_result['amendment_history']:
                lines.append(f"- {amendment}")
            lines.append("")
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            lines.append("## ê¸°ë³¸ì •ì‹ ")
            lines.append("")
            lines.append(parsed_result['basic_spirit'])
            lines.append("")
        
        # ì¥ê³¼ ì¡°ë¬¸
        current_chapter = None
        for article in parsed_result.get('articles', []):
            if article.chapter_number != current_chapter:
                current_chapter = article.chapter_number
                for chapter in parsed_result.get('chapters', []):
                    if chapter.number == current_chapter:
                        lines.append(f"## {chapter.number} {chapter.title}")
                        lines.append("")
                        break
            
            lines.append(f"### {article.number}({article.title})")
            lines.append("")
            lines.append(article.body)
            lines.append("")
        
        # Annex
        if parsed_result.get('annex_content'):
            annex_no = parsed_result.get('annex_no', '')
            annex_title = parsed_result.get('annex_title', '')
            
            if annex_no and annex_title:
                lines.append(f"## [ë³„í‘œ{annex_no}] {annex_title}")
            else:
                lines.append("## ë³„í‘œ")
            lines.append("")
            lines.append("---")
            lines.append(parsed_result['annex_content'])
            lines.append("---")
        
        return "\n".join(lines)


# í•˜ìœ„ í˜¸í™˜ì„±
parse_pdf_text = LawParser.parse