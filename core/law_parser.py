"""
law_parser.py - LawMode ì¡°ë¬¸ íŒŒì„œ
Phase 0.4.0 P0-4 "LawMode"

PDF í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¡°ë¬¸ ì¶”ì¶œ (VLM ë³´ì¡°)

âœ… í•µì‹¬ ì›ì¹™:
1. ì¡°ë¬¸ êµ¬ì¡°ì˜ "ì§„ì‹¤"ì€ PDF í…ìŠ¤íŠ¸
2. VLMì€ í‘œ/ì´ë¯¸ì§€ ë³´ì™„ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
3. ì œ1ì¡°~ì œNì¡° ì™„ì „ ì¶”ì¶œ ë³´ì¥

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + GPT ì„¤ê³„
Date: 2025-11-13
Version: Phase 0.4.0 P0-4
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str  # ì œ1ì¡°, ì œ2ì¡° ë“±
    title: Optional[str]  # ëª©ì , ì ìš©ë²”ìœ„ ë“±
    body: str  # ë³¸ë¬¸
    start_pos: int  # í…ìŠ¤íŠ¸ ë‚´ ì‹œì‘ ìœ„ì¹˜
    end_pos: int  # í…ìŠ¤íŠ¸ ë‚´ ë ìœ„ì¹˜
    article_type: str = 'article'  # 'article', 'deleted' ë“±


class LawParser:
    """
    ê·œì •/ë²•ë ¹ ì „ìš© íŒŒì„œ
    
    PDF í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ íŒŒì‹±í•˜ì—¬ ì¡°ë¬¸ êµ¬ì¡° ì¶”ì¶œ
    """
    
    # ê¸°ë³¸ì •ì‹  íŒ¨í„´
    BASIC_SPIRIT_PATTERN = re.compile(
        r'ê¸°\s*ë³¸\s*ì •\s*ì‹ ',
        re.MULTILINE | re.IGNORECASE
    )
    
    # ì¡°ë¬¸ í—¤ë” íŒ¨í„´ (ì œNì¡° + ì œëª©)
    # ì˜ˆ: "ì œ1ì¡°(ëª©ì )", "ì œ2ì¡°ì˜2(íŠ¹ë¡€)", "ì œ3ì¡° (ì ìš©ë²”ìœ„)"
    ARTICLE_HEADER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¡°(?:\s*ì˜\s*(\d+))?\s*\(([^)]+)\)',
        re.MULTILINE
    )
    
    # ì¥ íŒ¨í„´
    CHAPTER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¥\s+(.+?)(?=\n|$)',
        re.MULTILINE
    )
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… LawParser ì´ˆê¸°í™” (Phase 0.4.0 P0-4)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        enacted_date: Optional[str] = None,
        clean_artifacts: bool = True
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì¡°ë¬¸ êµ¬ì¡° ì¶”ì¶œ
        
        Args:
            pdf_text: PDFì—ì„œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸
            document_title: ë¬¸ì„œ ì œëª©
            enacted_date: ì œì •ì¼ (YYYY.MM.DD)
            clean_artifacts: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° ì—¬ë¶€
        
        Returns:
            {
                'document_title': str,
                'enacted_date': str,
                'basic_spirit': str,  # ê¸°ë³¸ì •ì‹  ë³¸ë¬¸
                'chapters': List[Dict],  # ì¥ ì •ë³´
                'articles': List[Article],  # ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸
                'total_articles': int
            }
        """
        logger.info(f"ğŸ“œ LawParser ì‹œì‘: {document_title}")
        logger.info(f"   ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸: {len(pdf_text)}ì")
        
        # âœ… Phase 0.5: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        if clean_artifacts:
            from .page_cleaner import clean_page_artifacts
            pdf_text = clean_page_artifacts(pdf_text)
            logger.info(f"   ğŸ§¹ ì•„í‹°íŒ©íŠ¸ ì œê±° í›„: {len(pdf_text)}ì")
        
        # 1. ê¸°ë³¸ì •ì‹  ì¶”ì¶œ
        basic_spirit = self._extract_basic_spirit(pdf_text)
        logger.info(f"   âœ… ê¸°ë³¸ì •ì‹ : {len(basic_spirit)}ì")
        
        # 2. ì¥ ì¶”ì¶œ
        chapters = self._extract_chapters(pdf_text)
        logger.info(f"   âœ… ì¥: {len(chapters)}ê°œ")
        
        # 3. ì¡°ë¬¸ ì¶”ì¶œ (í•µì‹¬!)
        articles = self._extract_articles_from_text(pdf_text)
        logger.info(f"   âœ… ì¡°ë¬¸: {len(articles)}ê°œ")
        
        # 4. ì¡°ë¬¸ ë²ˆí˜¸ ì •ë ¬ (ì œ1ì¡° â†’ ì œ2ì¡° â†’ ...)
        articles_sorted = sorted(articles, key=lambda a: self._parse_article_number(a.number))
        
        result = {
            'document_title': document_title,
            'enacted_date': enacted_date or '',
            'basic_spirit': basic_spirit,
            'chapters': chapters,
            'articles': articles_sorted,
            'total_articles': len(articles_sorted)
        }
        
        logger.info(f"âœ… LawParser ì™„ë£Œ: {len(articles_sorted)}ê°œ ì¡°ë¬¸")
        
        return result
    
    def _extract_basic_spirit(self, text: str) -> str:
        """ê¸°ë³¸ì •ì‹  ì„¹ì…˜ ì¶”ì¶œ"""
        match = self.BASIC_SPIRIT_PATTERN.search(text)
        
        if not match:
            logger.warning("   âš ï¸ ê¸°ë³¸ì •ì‹  ë¯¸ë°œê²¬")
            return ""
        
        start = match.start()
        
        # ê¸°ë³¸ì •ì‹  ë: ë‹¤ìŒ ì¥ ë˜ëŠ” ì œ1ì¡°ê¹Œì§€
        end = len(text)
        
        # ë‹¤ìŒ ì¥ ì°¾ê¸°
        chapter_match = self.CHAPTER_PATTERN.search(text, start + 10)
        if chapter_match:
            end = min(end, chapter_match.start())
        
        # ì œ1ì¡° ì°¾ê¸°
        article_match = self.ARTICLE_HEADER_PATTERN.search(text, start + 10)
        if article_match:
            end = min(end, article_match.start())
        
        basic_text = text[start:end].strip()
        
        # "ê¸°ë³¸ì •ì‹ " í—¤ë” ì œê±°
        basic_text = self.BASIC_SPIRIT_PATTERN.sub('', basic_text, count=1).strip()
        
        return basic_text
    
    def _extract_chapters(self, text: str) -> List[Dict[str, Any]]:
        """ì¥(ç« ) ëª©ë¡ ì¶”ì¶œ"""
        chapters = []
        
        for match in self.CHAPTER_PATTERN.finditer(text):
            chapter_num = match.group(1)
            chapter_title = match.group(2).strip()
            
            chapters.append({
                'number': f'ì œ{chapter_num}ì¥',
                'title': chapter_title,
                'position': match.start()
            })
        
        return chapters
    
    def _extract_articles_from_text(self, text: str) -> List[Article]:
        """
        PDF í…ìŠ¤íŠ¸ì—ì„œ ì¡°ë¬¸ ì¶”ì¶œ (í•µì‹¬ ë©”ì„œë“œ)
        
        ì „ëµ:
        1. ì •ê·œì‹ìœ¼ë¡œ "ì œNì¡°(ì œëª©)" íŒ¨í„´ ì°¾ê¸°
        2. ê° ì¡°ë¬¸ì˜ ì‹œì‘~ë‹¤ìŒ ì¡°ë¬¸ ì§ì „ê¹Œì§€ë¥¼ ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì¶œ
        3. Article ê°ì²´ ìƒì„±
        """
        articles = []
        
        # 1. ëª¨ë“  ì¡°ë¬¸ í—¤ë” ìœ„ì¹˜ ì¶”ì¶œ
        positions = []
        for match in self.ARTICLE_HEADER_PATTERN.finditer(text):
            article_num = match.group(1)
            article_sub = match.group(2)  # 'ì˜2' ê°™ì€ ë¶€ë¶„ (ì—†ìœ¼ë©´ None)
            title = match.group(3).strip()
            
            # ì¡°ë¬¸ ë²ˆí˜¸ ìƒì„±
            if article_sub:
                full_number = f'ì œ{article_num}ì¡°ì˜{article_sub}'
            else:
                full_number = f'ì œ{article_num}ì¡°'
            
            positions.append((match.start(), full_number, title))
            
            logger.debug(f"      ğŸ“ {full_number}({title}) at {match.start()}")
        
        if not positions:
            logger.warning("   âš ï¸ ì¡°ë¬¸ í—¤ë” ë¯¸ë°œê²¬")
            return []
        
        logger.info(f"   ğŸ” ì¡°ë¬¸ í—¤ë”: {len(positions)}ê°œ ë°œê²¬")
        
        # 2. ê° ì¡°ë¬¸ì˜ ë³¸ë¬¸ ì¶”ì¶œ
        articles = self._extract_articles(text, positions)
        
        return articles
    
    def _extract_articles(
        self, 
        text: str, 
        positions: List[Tuple[int, str, Optional[str]]]
    ) -> List[Article]:
        """
        ì¡°ë¬¸ ë³¸ë¬¸ ì¶”ì¶œ
        
        Args:
            text: ì „ì²´ í…ìŠ¤íŠ¸
            positions: [(position, article_number, title), ...]
        
        Returns:
            [Article ê°ì²´ë“¤]
        """
        articles = []
        
        for i, (pos, article_num, title) in enumerate(positions):
            # ë‹¤ìŒ ì¡°ë¬¸ ìœ„ì¹˜ (ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ë)
            next_pos = positions[i + 1][0] if i + 1 < len(positions) else len(text)
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            body = text[pos:next_pos].strip()
            
            # Article ê°ì²´ ìƒì„±
            article = Article(
                number=article_num,
                title=title,
                body=body,
                start_pos=pos,
                end_pos=next_pos,
                article_type='article'
            )
            
            articles.append(article)
            
            logger.info(f"   ğŸ“„ {article_num} ({title or 'ì œëª©ì—†ìŒ'}): {len(body)}ì")
        
        return articles
    
    def to_chunks(self, parsed_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        íŒŒì‹± ê²°ê³¼ë¥¼ ì²­í¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (SemanticChunker í˜¸í™˜)
        
        Args:
            parsed_result: parse() ê²°ê³¼
        
        Returns:
            [{'content': ..., 'metadata': {...}}, ...]
        """
        chunks = []
        
        # 1. ê¸°ë³¸ì •ì‹  ì²­í¬
        if parsed_result['basic_spirit']:
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic',
                    'boundary': 'ê¸°ë³¸ì •ì‹ ',
                    'title': None,
                    'char_count': len(parsed_result['basic_spirit']),
                    'chunk_index': 1
                }
            })
        
        # 2. ì¡°ë¬¸ ì²­í¬
        for article in parsed_result['articles']:
            chunks.append({
                'content': article.body,
                'metadata': {
                    'type': 'article',
                    'boundary': article.number,
                    'title': article.title,
                    'char_count': len(article.body),
                    'chunk_index': len(chunks) + 1
                }
            })
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        return chunks
    
    def _parse_article_number(self, article_num: str) -> Tuple[int, int]:
        """
        ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ ì •ë ¬ ê°€ëŠ¥í•œ íŠœí”Œë¡œ ë³€í™˜
        
        ì˜ˆ:
        - "ì œ1ì¡°" â†’ (1, 0)
        - "ì œ2ì¡°ì˜2" â†’ (2, 2)
        - "ì œ10ì¡°" â†’ (10, 0)
        """
        match = re.match(r'ì œ(\d+)ì¡°(?:ì˜(\d+))?', article_num)
        
        if not match:
            return (999, 999)  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë§¨ ë’¤ë¡œ
        
        main_num = int(match.group(1))
        sub_num = int(match.group(2)) if match.group(2) else 0
        
        return (main_num, sub_num)


# ============================================
# í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
# ============================================

def test_law_parser():
    """LawParser ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
    
    sample_text = """
    ì¸ì‚¬ê·œì •
    
    ê¸° ë³¸ ì • ì‹ 
    ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬ ì§ì›ì˜ ë³´ì§, ìŠ¹ì§„, ì‹ ë¶„ë³´ì¥, ìƒë²Œ, ì¸ì‚¬ê³ ê³¼ ë“±ì— ê´€í•œ ì‚¬í•­ì„
    ê·œì •í•¨ìœ¼ë¡œì¨ ê³µì •í•˜ê³  íˆ¬ëª…í•œ ì¸ì‚¬ê´€ë¦¬ êµ¬í˜„ì„ í†µí•˜ì—¬ ì„¤ë¦½ëª©ì ì„ ë‹¬ì„±í•œë‹¤.
    
    ì œ1ì¥ ì´ ì¹™
    
    ì œ1ì¡°(ëª©ì ) ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬ ì§ì›ì—ê²Œ ì ìš©í•  ì¸ì‚¬ê´€ë¦¬ì˜ ê¸°ì¤€ì„ ì •í•˜ì—¬ í•©ë¦¬ì ì´ê³  ì ì •í•œ ì¸
    ì‚¬ê´€ë¦¬ë¥¼ ê¸°í•˜ê²Œ í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.
    
    ì œ2ì¡°(ì ìš©ë²”ìœ„) ì§ì›ì˜ ì¸ì‚¬ê´€ë¦¬ëŠ” ë²•ë ¹ ë° ì •ê´€ì— ì •í•œ ê²ƒì„ ì œì™¸í•˜ê³ ëŠ” ì´ ê·œì •ì— ë”°ë¥¸ë‹¤.
    
    ì œ3ì¡°(ì§ì› ë“±ì˜ êµ¬ë¶„) â‘  ì‚­ ì œ <2024.1.1.>
    â‘¡ ì§ì›ì€ ì¼ë°˜ì§, ë³„ì •ì§, ê¸°ì‚¬ì§ ë° ì „ë¬¸ì§ìœ¼ë¡œ êµ¬ë¶„í•œë‹¤.
    """
    
    parser = LawParser()
    result = parser.parse(
        pdf_text=sample_text,
        document_title="ì¸ì‚¬ê·œì • í…ŒìŠ¤íŠ¸"
    )
    
    print(f"ê¸°ë³¸ì •ì‹ : {len(result['basic_spirit'])}ì")
    print(f"ì¡°ë¬¸: {result['total_articles']}ê°œ")
    
    for article in result['articles']:
        print(f"  - {article.number} ({article.title})")
    
    # ì²­í¬ ë³€í™˜
    chunks = parser.to_chunks(result)
    print(f"\nì²­í¬: {len(chunks)}ê°œ")
    
    for chunk in chunks:
        meta = chunk['metadata']
        print(f"  - {meta['type']}: {meta['boundary']} ({meta['char_count']}ì)")


if __name__ == '__main__':
    test_law_parser()