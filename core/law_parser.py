"""
law_parser.py - LawMode ì¡°ë¬¸ íŒŒì„œ
Phase 0.7.5 Annex Fallback (ê¸´ê¸‰ í•«í”½ìŠ¤)

âœ… Phase 0.7.5 í•«í”½ìŠ¤:
1. Annex Fallback: ì¡°ë¬¸ 0ê°œ + í…ìŠ¤íŠ¸ 500ì+ â†’ annex ì²­í¬ ìƒì„±
2. [ë³„í‘œ n], <ì œnì¡° ê´€ë ¨> í—¤ë” ì¸ì‹
3. í‘œ/ë¶€ë¡ í…ìŠ¤íŠ¸ ì†ì‹¤ ë°©ì§€

ğŸ¯ ëª©í‘œ:
- ë²•ì¡°ë¬¸ í˜ì´ì§€: ê¸°ì¡´ ë¡œì§ ìœ ì§€
- ë³„í‘œ/í‘œ í˜ì´ì§€: ìµœì†Œí•œ í…ìŠ¤íŠ¸ ë³µêµ¬ (32ì â†’ 2,700ì+)
- RAG ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡ í‚¤ì›Œë“œ ë³´ì¡´

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + GPT + CEO í”¼ë“œë°±
Date: 2025-11-16
Version: Phase 0.7.5 Annex Fallback
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class Chapter:
    """ì¥ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: str
    start_pos: int
    section_order: int


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str
    title: Optional[str]
    body: str
    start_pos: int
    end_pos: int
    chapter_number: Optional[str] = None
    section_order: int = 0
    article_type: str = 'article'


class LawParser:
    """
    ê·œì •/ë²•ë ¹ ì „ìš© íŒŒì„œ (Phase 0.7.5 Annex Fallback)
    
    âœ… Phase 0.7.5 í•«í”½ìŠ¤:
    - ì¡°ë¬¸ 0ê°œ + í…ìŠ¤íŠ¸ ì¶©ë¶„ â†’ Annex ëª¨ë“œ ìë™ ì „í™˜
    - [ë³„í‘œ n], <ì œnì¡° ê´€ë ¨> í—¤ë” ì¸ì‹
    - í‘œ/ë¶€ë¡ í…ìŠ¤íŠ¸ ì†ì‹¤ ë°©ì§€
    """
    
    # ê¸°ë³¸ì •ì‹  íŒ¨í„´
    BASIC_SPIRIT_PATTERN = re.compile(
        r'ê¸°\s*ë³¸\s*ì •\s*ì‹ ',
        re.MULTILINE | re.IGNORECASE
    )
    
    # ì¡°ë¬¸ í—¤ë” íŒ¨í„´
    ARTICLE_HEADER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¡°(?:\s*ì˜\s*(\d+))?\s*\(([^)]+)\)',
        re.MULTILINE
    )
    
    # ì¥ íŒ¨í„´
    CHAPTER_PATTERN = re.compile(
        r'(ì œ\s*\d+\s*ì¥)\s*([^\nì œ]+)',
        re.MULTILINE
    )
    
    # íƒ€ì´í‹€ íŒ¨í„´
    TITLE_PATTERN = re.compile(
        r'^([ê°€-í£]{2,10}ê·œì •|[ê°€-í£]{2,10}ë‚´ê·œ|[ê°€-í£]{2,10}ì •ê´€)',
        re.MULTILINE
    )
    
    # ê°œì •ì¼ íŒ¨í„´
    AMENDMENT_PATTERN = re.compile(
        r'(ì œ\d+ì°¨\s*)?ê°œì •\s*\d{4}\.\d{1,2}\.\d{1,2}\.?',
        re.MULTILINE
    )
    
    # âœ… Phase 0.7.5: ë³„í‘œ í—¤ë” íŒ¨í„´
    ANNEX_HEADER_PATTERN = re.compile(
        r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)',
        re.MULTILINE
    )
    
    # âœ… Phase 0.7.5: ê´€ë ¨ ì¡°ë¬¸ íŒ¨í„´
    RELATED_ARTICLE_PATTERN = re.compile(
        r'<\s*ì œ(\d+)ì¡°(?:ì œ(\d+)í•­)?\s*ê´€ë ¨\s*>',
        re.MULTILINE
    )
    
    # ì¤„ë°”ê¿ˆ ì •ë¦¬ íŒ¨í„´
    SAFE_NEWLINE_PATTERN = re.compile(
        r'(?<=[ê°€-í£0-9])(?<![.?!])[\r]?\n(?=[ê°€-í£0-9])',
        re.MULTILINE
    )
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… LawParser ì´ˆê¸°í™” (Phase 0.7.5 Annex Fallback)")
        logger.info("   ğŸ“Œ ë²•ì¡°ë¬¸ ëª¨ë“œ: ê¸°ì¡´ ë¡œì§ ìœ ì§€")
        logger.info("   ğŸ“Œ Annex ëª¨ë“œ: ë³„í‘œ/í‘œ í˜ì´ì§€ ìë™ ë³µêµ¬")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        enacted_date: Optional[str] = None,
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = False
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì¡°ë¬¸ êµ¬ì¡° ì¶”ì¶œ
        
        âœ… Phase 0.7.5: Annex Fallback ì¶”ê°€
        - ì¡°ë¬¸ 0ê°œ + í…ìŠ¤íŠ¸ 500ì+ â†’ annex ëª¨ë“œ ìë™ ì „í™˜
        
        Args:
            pdf_text: PDFì—ì„œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸
            document_title: ë¬¸ì„œ ì œëª©
            enacted_date: ì œì •ì¼
            clean_artifacts: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° ì—¬ë¶€
            normalize_linebreaks: ì¤„ë°”ê¿ˆ ì •ë¦¬ ì—¬ë¶€
        
        Returns:
            {
                'document_title': str,
                'enacted_date': str,
                'amendment_history': str,
                'basic_spirit': str,
                'chapters': List[Chapter],
                'articles': List[Article],
                'total_articles': int,
                'total_chapters': int,
                'is_annex_mode': bool,  # âœ… Phase 0.7.5
                'annex_content': str     # âœ… Phase 0.7.5
            }
        """
        logger.info(f"ğŸ“œ LawParser Phase 0.7.5 ì‹œì‘: {document_title}")
        logger.info(f"   ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸: {len(pdf_text)}ì")
        
        original_text = pdf_text
        
        # í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        if clean_artifacts:
            try:
                from .page_cleaner import clean_page_artifacts
                pdf_text = clean_page_artifacts(pdf_text)
                logger.info(f"   ğŸ§¹ ì•„í‹°íŒ©íŠ¸ ì œê±° í›„: {len(pdf_text)}ì")
            except ImportError:
                logger.warning("   âš ï¸ PageCleaner ë¯¸ì„¤ì¹˜ - ê±´ë„ˆëœ€")
        
        cleaned_text = pdf_text
        
        # ì¤„ë°”ê¿ˆ ì •ë¦¬
        if normalize_linebreaks:
            original_len = len(pdf_text)
            pdf_text = self._normalize_linebreaks(pdf_text)
            logger.info(f"   âœ‚ï¸ ì¤„ë°”ê¿ˆ ì •ë¦¬: {original_len}ì â†’ {len(pdf_text)}ì")
        
        # ì¥/ì¡°ë¬¸ ì „ì²˜ë¦¬
        pdf_text = self._preprocess_for_chapter_parsing(pdf_text)
        
        # Front Matter ì¶”ì¶œ
        title, amendment_history, pdf_text = self._extract_front_matter(pdf_text, document_title)
        
        # 1. ê¸°ë³¸ì •ì‹  ì¶”ì¶œ
        basic_spirit = self._extract_basic_spirit(pdf_text)
        logger.info(f"   âœ… ê¸°ë³¸ì •ì‹ : {len(basic_spirit)}ì")
        
        # 2. ì¥ ì¶”ì¶œ
        chapters = self._extract_chapters(pdf_text)
        logger.info(f"   âœ… ì¥: {len(chapters)}ê°œ")
        
        # 3. ì¡°ë¬¸ ì¶”ì¶œ
        articles = self._extract_articles_from_text(pdf_text)
        logger.info(f"   âœ… ì¡°ë¬¸: {len(articles)}ê°œ")
        
        # âœ… Phase 0.7.5: Annex Fallback ì²´í¬
        is_annex_mode = False
        annex_content = ""
        
        if len(articles) == 0 and len(cleaned_text) > 500:
            logger.warning(f"   ğŸ”„ Annex Fallback í™œì„±í™”!")
            logger.warning(f"      ì¡°ê±´: ì¡°ë¬¸ 0ê°œ + í…ìŠ¤íŠ¸ {len(cleaned_text)}ì")
            
            is_annex_mode = True
            annex_content = self._extract_annex_content(cleaned_text)
            
            logger.info(f"   âœ… Annex ì½˜í…ì¸  ì¶”ì¶œ: {len(annex_content)}ì")
        
        # 4. ì¡°ë¬¸ì— chapter_number í• ë‹¹
        self._assign_chapters_to_articles(chapters, articles)
        
        # 5. section_order ë¶€ì—¬
        self._assign_section_order(chapters, articles)
        
        # 6. ê¸°ë³¸ì •ì‹ /ì¡°ë¬¸ì—ì„œ ì¥ í—¤ë” ì œê±°
        basic_spirit = self._remove_chapter_headers(basic_spirit)
        for article in articles:
            article.body = self._remove_chapter_headers(article.body)
        
        result = {
            'document_title': title,
            'enacted_date': enacted_date or '',
            'amendment_history': amendment_history,
            'basic_spirit': basic_spirit,
            'chapters': chapters,
            'articles': articles,
            'total_articles': len(articles),
            'total_chapters': len(chapters),
            'is_annex_mode': is_annex_mode,      # âœ… Phase 0.7.5
            'annex_content': annex_content        # âœ… Phase 0.7.5
        }
        
        if is_annex_mode:
            logger.info(f"âœ… LawParser Phase 0.7.5 ì™„ë£Œ (Annex ëª¨ë“œ)")
            logger.info(f"   ğŸ“Š ë³„í‘œ/ë¶€ë¡ ì½˜í…ì¸ : {len(annex_content)}ì")
        else:
            logger.info(f"âœ… LawParser Phase 0.7.5 ì™„ë£Œ (ë²•ì¡°ë¬¸ ëª¨ë“œ)")
            logger.info(f"   ğŸ“Š {len(chapters)}ê°œ ì¥, {len(articles)}ê°œ ì¡°ë¬¸")
        
        return result
    
    def _extract_annex_content(self, text: str) -> str:
        """
        âœ… Phase 0.7.5: Annex ì½˜í…ì¸  ì¶”ì¶œ
        
        [ë³„í‘œ n] ... <ì œnì¡° ê´€ë ¨> ... í‘œ ë³¸ë¬¸
        
        Args:
            text: ì •ì œëœ í…ìŠ¤íŠ¸
        
        Returns:
            Annex ì½˜í…ì¸  (í—¤ë” + ë³¸ë¬¸)
        """
        logger.info("   ğŸ” Annex í—¤ë” ê°ì§€ ì¤‘...")
        
        # ë³„í‘œ í—¤ë” ì°¾ê¸°
        annex_match = self.ANNEX_HEADER_PATTERN.search(text)
        if annex_match:
            annex_no = annex_match.group(1)
            annex_title = annex_match.group(2).strip()
            logger.info(f"      âœ… [ë³„í‘œ {annex_no}] {annex_title}")
        
        # ê´€ë ¨ ì¡°ë¬¸ ì°¾ê¸°
        related_match = self.RELATED_ARTICLE_PATTERN.search(text)
        if related_match:
            article_no = related_match.group(1)
            paragraph_no = related_match.group(2) or ""
            related_article = f"ì œ{article_no}ì¡°" + (f"ì œ{paragraph_no}í•­" if paragraph_no else "")
            logger.info(f"      âœ… ê´€ë ¨ ì¡°ë¬¸: {related_article}")
        
        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ annex_contentë¡œ ë°˜í™˜
        # (ë‚˜ì¤‘ì— ë” ì •ë°€í•˜ê²Œ ë¶„ë¦¬ ê°€ëŠ¥)
        return text.strip()
    
    def _normalize_linebreaks(self, text: str) -> str:
        """ì¤„ë°”ê¿ˆ ì •ë¦¬ (Phase 0.5 ìŠ¤íƒ€ì¼)"""
        before = text
        after = self.SAFE_NEWLINE_PATTERN.sub(' ', text)
        
        removed_count = before.count('\n') - after.count('\n')
        if removed_count > 0:
            logger.info(f"      âœ‚ï¸ ë‹¨ì–´ ì¤‘ê°„ ì¤„ë°”ê¿ˆ ì œê±°: {removed_count}ê°œ")
        
        return after
    
    def _preprocess_for_chapter_parsing(self, text: str) -> str:
        """ì¥/ì¡°ë¬¸ íŒŒì‹± ì „ì²˜ë¦¬"""
        text = re.sub(r'(ì œ\s*\d+\s*ì¥)', r'\n\1 ', text)
        text = re.sub(r'(ì œ\s*\d+\s*ì¡°)', r'\n\1', text)
        return text
    
    def _extract_front_matter(
        self, 
        text: str, 
        fallback_title: str = ""
    ) -> Tuple[str, str, str]:
        """ë¬¸ì„œ Front Matter ì¶”ì¶œ"""
        # íƒ€ì´í‹€ ì¶”ì¶œ
        title = fallback_title
        title_match = self.TITLE_PATTERN.search(text[:500])
        if title_match:
            title = title_match.group(1)
            logger.info(f"   ğŸ“Œ íƒ€ì´í‹€ ì¶”ì¶œ: {title}")
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendment_history = ""
        amendments = []
        
        spirit_match = self.BASIC_SPIRIT_PATTERN.search(text)
        search_end = spirit_match.start() if spirit_match else 1000
        
        for match in self.AMENDMENT_PATTERN.finditer(text[:search_end]):
            amendments.append(match.group(0))
        
        if amendments:
            amendment_history = ' '.join(amendments)
            logger.info(f"   ğŸ“… ê°œì •ì´ë ¥: {len(amendments)}ê±´")
        
        return title, amendment_history, text
    
    def _extract_basic_spirit(self, text: str) -> str:
        """ê¸°ë³¸ì •ì‹  ì¶”ì¶œ"""
        match = self.BASIC_SPIRIT_PATTERN.search(text)
        if not match:
            return ""
        
        start = match.end()
        
        next_article = self.ARTICLE_HEADER_PATTERN.search(text, start)
        next_chapter = self.CHAPTER_PATTERN.search(text, start)
        
        if next_article and next_chapter:
            end = min(next_article.start(), next_chapter.start())
        elif next_article:
            end = next_article.start()
        elif next_chapter:
            end = next_chapter.start()
        else:
            end = len(text)
        
        spirit_text = text[start:end].strip()
        return spirit_text
    
    def _extract_chapters(self, text: str) -> List[Chapter]:
        """ì¥ ì¶”ì¶œ"""
        chapters = []
        order = 0
        
        for match in self.CHAPTER_PATTERN.finditer(text):
            chapter_num_raw = match.group(1).strip()
            chapter_title_raw = match.group(2).strip()
            
            chapter_num = re.sub(r'\s+', '', chapter_num_raw)
            chapter_title = re.sub(r'ì œ\d+ì¡°.*$', '', chapter_title_raw).strip()
            
            if not chapter_title:
                continue
            
            chapters.append(Chapter(
                number=chapter_num,
                title=chapter_title,
                start_pos=match.start(),
                section_order=order
            ))
            
            order += 1
        
        return chapters
    
    def _extract_articles_from_text(self, text: str) -> List[Article]:
        """ì¡°ë¬¸ ì¶”ì¶œ"""
        articles = []
        
        for match in self.ARTICLE_HEADER_PATTERN.finditer(text):
            article_no = match.group(1)
            article_no_sub = match.group(2)
            article_title = match.group(3)
            
            if article_no_sub:
                article_number = f"ì œ{article_no}ì¡°ì˜{article_no_sub}"
            else:
                article_number = f"ì œ{article_no}ì¡°"
            
            start = match.end()
            
            next_match = self.ARTICLE_HEADER_PATTERN.search(text, start)
            end = next_match.start() if next_match else len(text)
            
            body = text[start:end].strip()
            
            articles.append(Article(
                number=article_number,
                title=article_title,
                body=body,
                start_pos=match.start(),
                end_pos=end,
                section_order=0
            ))
        
        return articles
    
    def _assign_chapters_to_articles(self, chapters: List[Chapter], articles: List[Article]):
        """ì¡°ë¬¸ì— chapter_number í• ë‹¹"""
        for article in articles:
            article.chapter_number = None
            
            for i, chapter in enumerate(chapters):
                next_chapter_pos = chapters[i+1].start_pos if i+1 < len(chapters) else float('inf')
                
                if chapter.start_pos <= article.start_pos < next_chapter_pos:
                    article.chapter_number = chapter.number
                    break
    
    def _assign_section_order(self, chapters: List[Chapter], articles: List[Article]):
        """section_order ë¶€ì—¬"""
        all_sections = []
        
        for chapter in chapters:
            all_sections.append(('chapter', chapter))
        
        for article in articles:
            all_sections.append(('article', article))
        
        all_sections.sort(key=lambda x: x[1].start_pos)
        
        for order, (section_type, section) in enumerate(all_sections):
            section.section_order = order
        
        logger.info(f"   ğŸ”¢ section_order ë¶€ì—¬ ì™„ë£Œ: {len(all_sections)}ê°œ ì„¹ì…˜")
    
    def _remove_chapter_headers(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¥ í—¤ë” ì œê±°"""
        return self.CHAPTER_PATTERN.sub('', text).strip()
    
    def to_chunks(self, parsed_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        âœ… Phase 0.7.5: íŒŒì‹± ê²°ê³¼ â†’ ì²­í¬ ë³€í™˜ (Annex ì§€ì›)
        """
        chunks = []
        
        # 1. íƒ€ì´í‹€ ì²­í¬
        if parsed_result['document_title']:
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # 2. ê°œì •ì´ë ¥ ì²­í¬
        if parsed_result['amendment_history']:
            chunks.append({
                'content': parsed_result['amendment_history'],
                'metadata': {
                    'type': 'amendment_history',
                    'boundary': 'header',
                    'title': 'ê°œì • ì´ë ¥',
                    'char_count': len(parsed_result['amendment_history']),
                    'section_order': -2
                }
            })
        
        # âœ… Phase 0.7.5: Annex ëª¨ë“œ ì²´í¬
        if parsed_result.get('is_annex_mode', False):
            # Annex ì²­í¬ ìƒì„±
            annex_content = parsed_result.get('annex_content', '')
            
            if annex_content:
                # ë³„í‘œ í—¤ë” ê°ì§€
                annex_header_match = self.ANNEX_HEADER_PATTERN.search(annex_content)
                related_match = self.RELATED_ARTICLE_PATTERN.search(annex_content)
                
                annex_no = None
                annex_title = "ë³„í‘œ/ë¶€ë¡"
                related_article = None
                
                if annex_header_match:
                    annex_no = annex_header_match.group(1)
                    annex_title = annex_header_match.group(2).strip()
                
                if related_match:
                    article_no = related_match.group(1)
                    paragraph_no = related_match.group(2) or ""
                    related_article = f"ì œ{article_no}ì¡°" + (f"ì œ{paragraph_no}í•­" if paragraph_no else "")
                
                chunks.append({
                    'content': annex_content,
                    'metadata': {
                        'type': 'annex',
                        'boundary': 'annex',
                        'title': annex_title,
                        'annex_no': annex_no,
                        'related_article': related_article,
                        'char_count': len(annex_content),
                        'section_order': 0
                    }
                })
                
                logger.info(f"   âœ… Annex ì²­í¬ ìƒì„±: [ë³„í‘œ {annex_no}] {annex_title}")
        
        else:
            # ê¸°ì¡´ ë²•ì¡°ë¬¸ ëª¨ë“œ ì²­í¬
            # 3. ê¸°ë³¸ì •ì‹  ì²­í¬
            if parsed_result['basic_spirit']:
                chunks.append({
                    'content': parsed_result['basic_spirit'],
                    'metadata': {
                        'type': 'basic',
                        'boundary': 'basic_spirit',
                        'title': 'ê¸°ë³¸ì •ì‹ ',
                        'char_count': len(parsed_result['basic_spirit']),
                        'section_order': -1
                    }
                })
            
            # 4. ì¥ ì²­í¬
            for chapter in parsed_result['chapters']:
                content = f"{chapter.number} {chapter.title}"
                
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'chapter',
                        'boundary': 'chapter',
                        'chapter_number': chapter.number,
                        'chapter_title': chapter.title,
                        'char_count': len(content),
                        'section_order': chapter.section_order
                    }
                })
            
            # 5. ì¡°ë¬¸ ì²­í¬
            for article in parsed_result['articles']:
                content = f"{article.number}({article.title})\n{article.body}"
                
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'article',
                        'boundary': 'article',
                        'article_number': article.number,
                        'article_title': article.title,
                        'chapter_number': article.chapter_number,
                        'char_count': len(content),
                        'section_order': article.section_order
                    }
                })
        
        chunks.sort(key=lambda c: c['metadata'].get('section_order', 999))
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ (Phase 0.7.5): {len(chunks)}ê°œ")
        
        if parsed_result.get('is_annex_mode'):
            logger.info(f"   - íƒ€ì´í‹€: 1ê°œ")
            logger.info(f"   - ê°œì •ì´ë ¥: {1 if parsed_result['amendment_history'] else 0}ê°œ")
            logger.info(f"   - Annex: 1ê°œ")
        else:
            logger.info(f"   - íƒ€ì´í‹€: 1ê°œ")
            logger.info(f"   - ê°œì •ì´ë ¥: {1 if parsed_result['amendment_history'] else 0}ê°œ")
            logger.info(f"   - ê¸°ë³¸ì •ì‹ : {1 if parsed_result['basic_spirit'] else 0}ê°œ")
            logger.info(f"   - ì¥: {parsed_result['total_chapters']}ê°œ")
            logger.info(f"   - ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
        
        return chunks
    
    def to_markdown(self, parsed_result: Dict[str, Any]) -> str:
        """
        âœ… Phase 0.7.5: íŒŒì‹± ê²°ê³¼ â†’ Markdown ë³€í™˜ (Annex ì§€ì›)
        """
        lines = []
        
        # 1. íƒ€ì´í‹€
        if parsed_result['document_title']:
            lines.append(f"# {parsed_result['document_title']}")
            lines.append("")
        
        # 2. ê°œì •ì´ë ¥
        if parsed_result['amendment_history']:
            lines.append("## ê°œì • ì´ë ¥")
            lines.append("")
            
            amendments = parsed_result['amendment_history'].split()
            for amendment in amendments:
                lines.append(f"- {amendment}")
            
            lines.append("")
        
        # âœ… Phase 0.7.5: Annex ëª¨ë“œ ì²´í¬
        if parsed_result.get('is_annex_mode', False):
            annex_content = parsed_result.get('annex_content', '')
            
            if annex_content:
                # ë³„í‘œ í—¤ë” ê°ì§€
                annex_header_match = self.ANNEX_HEADER_PATTERN.search(annex_content)
                
                if annex_header_match:
                    annex_no = annex_header_match.group(1)
                    annex_title = annex_header_match.group(2).strip()
                    lines.append(f"## [ë³„í‘œ {annex_no}] {annex_title}")
                    lines.append("")
                
                lines.append(annex_content)
                lines.append("")
        
        else:
            # ê¸°ì¡´ ë²•ì¡°ë¬¸ ëª¨ë“œ
            # 3. ê¸°ë³¸ì •ì‹ 
            if parsed_result['basic_spirit']:
                lines.append("## ê¸°ë³¸ì •ì‹ ")
                lines.append("")
                lines.append(parsed_result['basic_spirit'])
                lines.append("")
            
            # 4. ì¥/ì¡°ë¬¸ (section_order ìˆœ)
            all_sections = []
            
            for chapter in parsed_result['chapters']:
                all_sections.append(('chapter', chapter))
            
            for article in parsed_result['articles']:
                all_sections.append(('article', article))
            
            all_sections.sort(key=lambda x: x[1].section_order)
            
            for section_type, section in all_sections:
                if section_type == 'chapter':
                    lines.append(f"## {section.number} {section.title}")
                    lines.append("")
                else:
                    lines.append(f"### {section.number}({section.title})")
                    lines.append("")
                    lines.append(section.body)
                    lines.append("")
        
        markdown = "\n".join(lines)
        
        logger.info(f"âœ… Markdown ë³€í™˜ ì™„ë£Œ: {len(markdown)}ì")
        
        return markdown
    
    def to_review_md(self, parsed_result: Dict[str, Any]) -> str:
        """ë¦¬ë·°ìš© Markdown (to_markdownê³¼ ë™ì¼)"""
        return self.to_markdown(parsed_result)