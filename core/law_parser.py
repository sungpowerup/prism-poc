"""
core/law_parser.py - Phase 0.8 í†µí•©
Annex ì„œë¸Œì²­í‚¹ í†µí•©

ìˆ˜ì • ì‚¬í•­:
- to_chunks() í•¨ìˆ˜ì— AnnexSubChunker í†µí•©
- Annex ì²­í¬ë¥¼ ì„œë¸Œì²­í¬ë¡œ ë¶„í•´
"""

# ê¸°ì¡´ import ìœ ì§€
from core.annex_subchunker import AnnexSubChunker, validate_subchunks

class LawParser:
    # ... (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    
    def to_chunks(self, parsed_result: dict) -> list:
        """
        íŒŒì‹± ê²°ê³¼ â†’ RAG ì²­í¬ ë³€í™˜
        
        âœ… Phase 0.8: Annex ì„œë¸Œì²­í‚¹ í†µí•©
        """
        chunks = []
        
        # Title
        if parsed_result['document_title']:
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # ê°œì •ì´ë ¥
        if parsed_result['amendment_history']:
            for i, amendment in enumerate(parsed_result['amendment_history']):
                chunks.append({
                    'content': amendment,
                    'metadata': {
                        'type': 'amendment_history',
                        'boundary': 'header',
                        'title': 'ê°œì • ì´ë ¥',
                        'char_count': len(amendment),
                        'section_order': -2 - i
                    }
                })
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result['basic_spirit']:
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic',
                    'boundary': 'header',
                    'title': 'ê¸°ë³¸ì •ì‹ ',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # ì¥
        for chapter in parsed_result['chapters']:
            chunks.append({
                'content': f"{chapter.number} {chapter.title}",
                'metadata': {
                    'type': 'chapter',
                    'boundary': 'chapter',
                    'chapter_number': chapter.number,
                    'chapter_title': chapter.title,
                    'char_count': len(chapter.number) + len(chapter.title),
                    'section_order': chapter.section_order
                }
            })
        
        # ì¡°ë¬¸
        for article in parsed_result['articles']:
            content = f"{article.number}({article.title})\n{article.body}"
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        # âœ… Phase 0.8: Annex ì„œë¸Œì²­í‚¹
        if parsed_result.get('annex_content'):
            logger.info("âœ… Phase 0.8: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘")
            
            subchunker = AnnexSubChunker()
            annex_text = parsed_result['annex_content']
            
            # ì„œë¸Œì²­í¬ ìƒì„±
            sub_chunks = subchunker.chunk(annex_text)
            
            # ê²€ì¦
            validation = validate_subchunks(sub_chunks, len(annex_text))
            
            if validation['is_valid']:
                logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì„±ê³µ: {validation['chunk_count']}ê°œ")
                logger.info(f"   ğŸ“Š ì†ì‹¤ë¥ : {validation['loss_rate']:.2%}")
                logger.info(f"   ğŸ“Š íƒ€ì…: {validation['type_counts']}")
                
                # ì„œë¸Œì²­í¬ â†’ í‘œì¤€ ì²­í¬ í¬ë§· ë³€í™˜
                for sub in sub_chunks:
                    chunks.append({
                        'content': sub.content,
                        'metadata': {
                            'type': f"annex_{sub.section_type}",
                            'boundary': 'annex',
                            'section_id': sub.section_id,
                            'section_type': sub.section_type,
                            'char_count': sub.char_count,
                            'section_order': sub.order,
                            **sub.metadata
                        }
                    })
            else:
                logger.warning("âš ï¸ Annex ì„œë¸Œì²­í‚¹ ê²€ì¦ ì‹¤íŒ¨ - Fallback to ê¸°ì¡´ ë¡œì§")
                # Fallback: ê¸°ì¡´ ë‹¨ì¼ ì²­í¬
                chunks.append({
                    'content': annex_text,
                    'metadata': {
                        'type': 'annex',
                        'boundary': 'annex',
                        'title': parsed_result.get('annex_title', ''),
                        'annex_no': parsed_result.get('annex_no'),
                        'related_article': parsed_result.get('related_article'),
                        'char_count': len(annex_text),
                        'section_order': 0
                    }
                })
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ (Phase 0.8): {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for chunk in chunks:
            ctype = chunk['metadata']['type']
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")
        
        return chunks
    
    # ... (ë‚˜ë¨¸ì§€ ì½”ë“œ ìœ ì§€)