"""
law_parser.py - LawMode ì¡°ë¬¸ íŒŒì„œ
Phase 0.5 "Polishing & Standardization"

âœ… Phase 0.5 ê°œì„ :
- PageCleaner í†µí•© (clean_artifacts=True)
- í˜ì´ì§€ ë²ˆí˜¸ ì™„ì „ ì œê±°
- ë¬¸ì„œ í”„ë¡œíŒŒì¼ ì§€ì› (ì˜µì…˜)

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + GPT ì„¤ê³„
Date: 2025-11-14
Version: Phase 0.5
"""

import re
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    number: str  # ì œ1ì¡°, ì œ2ì¡° ë“±
    title: Optional[str]  # ëª©ì , ì ìš©ë²”ìœ„ ë“±
    body: str  # ë³¸ë¬¸
    start_pos: int  # í…ìŠ¤íŠ¸ ë‚´ ì‹œì‘ ìœ„ì¹˜
    end_pos: int  # í…ìŠ¤íŠ¸ ë‚´ ë ìœ„ì¹˜
    article_type: str = 'article'  # 'article', 'deleted' ë“±


class LawParser:
    """
    ê·œì •/ë²•ë ¹ ì „ìš© íŒŒì„œ (Phase 0.5)
    
    âœ… Phase 0.5 ê°œì„ :
    - PageCleaner í†µí•© â†’ í˜ì´ì§€ ë²ˆí˜¸ ìë™ ì œê±°
    - PDF í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™•í•œ ì¡°ë¬¸ ì¶”ì¶œ
    """
    
    # ê¸°ë³¸ì •ì‹  íŒ¨í„´
    BASIC_SPIRIT_PATTERN = re.compile(
        r'ê¸°\s*ë³¸\s*ì •\s*ì‹ ',
        re.MULTILINE | re.IGNORECASE
    )
    
    # ì¡°ë¬¸ í—¤ë” íŒ¨í„´ (ì œNì¡° + ì œëª©)
    # ì˜ˆ: "ì œ1ì¡°(ëª©ì )", "ì œ2ì¡°ì˜2(íŠ¹ë¡€)", "ì œ3ì¡° (ì ìš©ë²”ìœ„)"
    ARTICLE_HEADER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¡°(?:\s*ì˜\s*(\d+))?\s*\(([^)]+)\)',
        re.MULTILINE
    )
    
    # ì¥ íŒ¨í„´
    CHAPTER_PATTERN = re.compile(
        r'ì œ\s*(\d+)\s*ì¥\s+(.+?)(?=\n|$)',
        re.MULTILINE
    )
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… LawParser ì´ˆê¸°í™” (Phase 0.5)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        enacted_date: Optional[str] = None,
        clean_artifacts: bool = True
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì¡°ë¬¸ êµ¬ì¡° ì¶”ì¶œ
        
        Args:
            pdf_text: PDFì—ì„œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸
            document_title: ë¬¸ì„œ ì œëª©
            enacted_date: ì œì •ì¼ (YYYY.MM.DD)
            clean_artifacts: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±° ì—¬ë¶€ (Phase 0.5)
        
        Returns:
            {
                'document_title': str,
                'enacted_date': str,
                'basic_spirit': str,  # ê¸°ë³¸ì •ì‹  ë³¸ë¬¸
                'chapters': List[Dict],  # ì¥ ì •ë³´
                'articles': List[Article],  # ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸
                'total_articles': int
            }
        """
        logger.info(f"ğŸ“œ LawParser ì‹œì‘: {document_title}")
        logger.info(f"   ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸: {len(pdf_text)}ì")
        
        # âœ… Phase 0.5: í˜ì´ì§€ ì•„í‹°íŒ©íŠ¸ ì œê±°
        if clean_artifacts:
            try:
                from .page_cleaner import clean_page_artifacts
                pdf_text = clean_page_artifacts(pdf_text)
                logger.info(f"   ğŸ§¹ ì•„í‹°íŒ©íŠ¸ ì œê±° í›„: {len(pdf_text)}ì")
            except ImportError:
                logger.warning("   âš ï¸ PageCleaner ë¯¸ì„¤ì¹˜ - ê±´ë„ˆëœ€")
        
        # 1. ê¸°ë³¸ì •ì‹  ì¶”ì¶œ
        basic_spirit = self._extract_basic_spirit(pdf_text)
        logger.info(f"   âœ… ê¸°ë³¸ì •ì‹ : {len(basic_spirit)}ì")
        
        # 2. ì¥ ì¶”ì¶œ
        chapters = self._extract_chapters(pdf_text)
        logger.info(f"   âœ… ì¥: {len(chapters)}ê°œ")
        
        # 3. ì¡°ë¬¸ ì¶”ì¶œ (í•µì‹¬!)
        articles = self._extract_articles_from_text(pdf_text)
        logger.info(f"   âœ… ì¡°ë¬¸: {len(articles)}ê°œ")
        
        # 4. ê²°ê³¼ ì¡°ë¦½
        logger.info(f"âœ… LawParser ì™„ë£Œ: {len(articles)}ê°œ ì¡°ë¬¸")
        
        return {
            'document_title': document_title,
            'enacted_date': enacted_date or '',
            'basic_spirit': basic_spirit,
            'chapters': chapters,
            'articles': articles,
            'total_articles': len(articles)
        }
    
    def _extract_basic_spirit(self, text: str) -> str:
        """ê¸°ë³¸ì •ì‹  ì¶”ì¶œ"""
        match = self.BASIC_SPIRIT_PATTERN.search(text)
        if not match:
            return ""
        
        start = match.end()
        
        # ë‹¤ìŒ ì¡°ë¬¸ ë˜ëŠ” ì¥ê¹Œì§€
        next_article = self.ARTICLE_HEADER_PATTERN.search(text, start)
        next_chapter = self.CHAPTER_PATTERN.search(text, start)
        
        if next_article and next_chapter:
            end = min(next_article.start(), next_chapter.start())
        elif next_article:
            end = next_article.start()
        elif next_chapter:
            end = next_chapter.start()
        else:
            end = len(text)
        
        spirit_text = text[start:end].strip()
        return spirit_text
    
    def _extract_chapters(self, text: str) -> List[Dict[str, Any]]:
        """ì¥ ì¶”ì¶œ"""
        chapters = []
        for match in self.CHAPTER_PATTERN.finditer(text):
            chapters.append({
                'number': f"ì œ{match.group(1)}ì¥",
                'title': match.group(2).strip(),
                'start_pos': match.start()
            })
        return chapters
    
    def _extract_articles_from_text(self, text: str) -> List[Article]:
        """
        ì¡°ë¬¸ ì¶”ì¶œ (í•µì‹¬ ë¡œì§)
        
        ì œNì¡°(ì œëª©) íŒ¨í„´ìœ¼ë¡œ ì‹œì‘ â†’ ë‹¤ìŒ ì¡°ë¬¸ê¹Œì§€ê°€ ë³¸ë¬¸
        """
        articles = []
        
        matches = list(self.ARTICLE_HEADER_PATTERN.finditer(text))
        
        if not matches:
            logger.warning("   âš ï¸ ì¡°ë¬¸ í—¤ë” ë¯¸ë°œê²¬")
            return articles
        
        logger.info(f"   ğŸ” ì¡°ë¬¸ í—¤ë”: {len(matches)}ê°œ ë°œê²¬")
        
        for i, match in enumerate(matches):
            article_num = match.group(1)
            article_sub = match.group(2)  # "ì˜2"
            article_title = match.group(3)
            
            # ì¡°ë¬¸ ë²ˆí˜¸ ì¡°ë¦½
            if article_sub:
                number = f"ì œ{article_num}ì¡°ì˜{article_sub}"
            else:
                number = f"ì œ{article_num}ì¡°"
            
            # ë³¸ë¬¸ ë²”ìœ„
            body_start = match.end()
            
            if i < len(matches) - 1:
                body_end = matches[i + 1].start()
            else:
                body_end = len(text)
            
            body = text[body_start:body_end].strip()
            
            # Article ê°ì²´ ìƒì„±
            article = Article(
                number=number,
                title=article_title,
                body=body,
                start_pos=match.start(),
                end_pos=body_end
            )
            
            articles.append(article)
            logger.info(f"   ğŸ“„ {number} ({article_title}): {len(body)}ì")
        
        return articles
    
    def to_chunks(self, parsed_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        íŒŒì‹± ê²°ê³¼ â†’ ì²­í¬ ë³€í™˜
        
        Returns:
            List[Chunk] - RAGìš© ì²­í¬
        """
        chunks = []
        
        # 1. ê¸°ë³¸ì •ì‹  ì²­í¬
        if parsed_result['basic_spirit']:
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic',
                    'boundary': 'basic_spirit',
                    'title': 'ê¸°ë³¸ì •ì‹ ',
                    'char_count': len(parsed_result['basic_spirit'])
                }
            })
        
        # 2. ì¡°ë¬¸ ì²­í¬
        for article in parsed_result['articles']:
            content = f"{article.number}({article.title})\n{article.body}"
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'char_count': len(content)
                }
            })
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        return chunks


# ============================================
# í…ŒìŠ¤íŠ¸
# ============================================

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    
    sample_text = """
    ê¸°ë³¸ì •ì‹ 
    
    ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬ ì§ì›ì˜ ì¸ì‚¬ê´€ë¦¬ì— ê´€í•œ ì‚¬í•­ì„ ì •í•¨ìœ¼ë¡œì¨
    ì¸ì‚¬ì˜ ê³µì •ì„±ì„ í™•ë³´í•˜ê³  ì§ì›ì˜ ê·¼ë¬´ì˜ìš•ì„ ë†’ì„ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.
    
    ì¸ì‚¬ê·œì •
    402-3
    
    ì œ1ì¥ ì´ì¹™
    
    ì œ1ì¡°(ëª©ì ) ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬ ì§ì›ì˜ ì¸ì‚¬ê´€ë¦¬ì— ê´€í•˜ì—¬
    í•„ìš”í•œ ì‚¬í•­ì„ ì •í•¨ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.
    
    ì¸ì‚¬ê·œì •
    402-4
    
    ì œ2ì¡°(ì ìš©ë²”ìœ„) ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬(ì´í•˜ "ê³µì‚¬"ë¼ í•œë‹¤)ì˜
    ì„ì› ë° ì§ì›ì—ê²Œ ì ìš©í•œë‹¤.
    """
    
    parser = LawParser()
    result = parser.parse(
        pdf_text=sample_text,
        document_title="ì¸ì‚¬ê·œì •",
        clean_artifacts=True  # âœ… Phase 0.5
    )
    
    print(f"\nì´ ì¡°ë¬¸: {result['total_articles']}ê°œ")
    print(f"ê¸°ë³¸ì •ì‹ : {len(result['basic_spirit'])}ì")
    print(f"ì¥: {len(result['chapters'])}ê°œ")
    
    chunks = parser.to_chunks(result)
    print(f"ì²­í¬: {len(chunks)}ê°œ")