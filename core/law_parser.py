"""
core/law_parser.py - PRISM Phase 0.8.2 Hotfix
LawParser ì•ˆì •íŒ (ë³¸ë¬¸+Annex ë™ì‹œ ì²˜ë¦¬ ë²„ê·¸ ìˆ˜ì •)

Phase 0.8.2 í•µì‹¬ ìˆ˜ì •:
- âœ… Annex-only ê°ì§€ ì¡°ê±´ ìˆ˜ì •: TreeBuilder ê²°ê³¼ ë°˜ì˜ í›„ ì²´í¬
- âœ… ë³¸ë¬¸ ì¡°ë¬¸ + Annex ë™ì‹œ ë³´ì¡´
- âœ… DualQA ë§¤ì¹­ë¥  100% ë³µêµ¬

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-19
Version: Phase 0.8.2 Hotfix
"""

import re
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# Annex ì„œë¸Œì²­í‚¹
try:
    from core.annex_subchunker import AnnexSubChunker, validate_subchunks
    ANNEX_SUBCHUNKING_AVAILABLE = True
except ImportError:
    ANNEX_SUBCHUNKING_AVAILABLE = False
    logger.warning("âš ï¸ AnnexSubChunker ë¯¸ì„¤ì¹˜ - Annex ì„œë¸Œì²­í‚¹ ë¹„í™œì„±í™”")

# TreeBuilder Import
try:
    from core.tree_builder import TreeBuilder
    TREE_BUILDER_AVAILABLE = True
except ImportError:
    TREE_BUILDER_AVAILABLE = False
    logger.error("âŒ TreeBuilder í•„ìˆ˜ - import ì‹¤íŒ¨")


@dataclass
class Chapter:
    """ì¥ ë°ì´í„°"""
    number: str
    title: str
    section_order: int = 0


@dataclass
class Article:
    """ì¡°ë¬¸ ë°ì´í„°"""
    number: str
    title: str
    body: str
    chapter_number: str = ""
    section_order: int = 0


class LawParser:
    """
    Phase 0.8.2 LawParser Hotfix
    
    í•µì‹¬ ìˆ˜ì •:
    - Annex-only ê°ì§€ ì¡°ê±´ì„ TreeBuilder ê²°ê³¼ ë³€í™˜ í›„ë¡œ ì´ë™
    - ë³¸ë¬¸ ì¡°ë¬¸ì´ ìˆìœ¼ë©´ Annex-only ëª¨ë“œ ë¹„í™œì„±í™”
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        if not TREE_BUILDER_AVAILABLE:
            raise ImportError("TreeBuilder is required but not available")
        
        self.tree_builder = TreeBuilder()
        logger.info("âœ… LawParser ì´ˆê¸°í™” ì™„ë£Œ (Phase 0.8.2 Hotfix)")
    
    def parse(
        self,
        pdf_text: str,
        document_title: str = "",
        clean_artifacts: bool = True,
        normalize_linebreaks: bool = True
    ) -> Dict[str, Any]:
        """
        PDF í…ìŠ¤íŠ¸ íŒŒì‹± (ë©”ì¸ ë©”ì„œë“œ)
        
        âœ… Phase 0.8.2 Hotfix:
        - TreeBuilder ê²°ê³¼ë¥¼ ë¨¼ì € ë³€í™˜
        - ê·¸ ë‹¤ìŒ Annex-only ì²´í¬
        """
        logger.info(f"ğŸ“œ LawParser.parse() ì‹œì‘: {document_title}")
        
        # ì „ì²˜ë¦¬
        cleaned_text = pdf_text
        if normalize_linebreaks:
            cleaned_text = cleaned_text.replace('\r\n', '\n')
        
        # TreeBuilderë¡œ íŒŒì‹±
        tree_result = self.tree_builder.build(
            markdown=cleaned_text,
            document_title=document_title,
            enacted_date=None
        )
        
        # âœ… Phase 0.8.2: TreeBuilder ê²°ê³¼ë¥¼ ë¨¼ì € ë³€í™˜
        parsed_result = self._convert_tree_to_result(tree_result, document_title)
        
        # âœ… Phase 0.8.2: Annex-only ì²´í¬ë¥¼ ë³€í™˜ í›„ë¡œ ì´ë™
        # ì´ì œ total_articlesê°€ ì •í™•í•˜ê²Œ ë°˜ì˜ë¨
        is_annex_only = (
            parsed_result.get('total_chapters', 0) == 0 and
            parsed_result.get('total_articles', 0) == 0 and
            not parsed_result.get('annex_content') and
            len(cleaned_text) > 500
        )
        
        if is_annex_only:
            logger.warning("ğŸ”„ Annex-only ë¬¸ì„œ ê°ì§€ - Fallback Annex íŒŒì„œ ê°€ë™")
            self._apply_annex_fallback(cleaned_text, parsed_result)
        
        # âœ… Phase 0.8.2 ì¶”ê°€: ë³¸ë¬¸+Annex í˜¼í•© ë¬¸ì„œ ì²˜ë¦¬
        # ì¡°ë¬¸ì´ ìˆëŠ”ë° Annexë„ ìˆëŠ” ê²½ìš°, ë³„ë„ë¡œ Annex ì¶”ì¶œ
        elif (
            parsed_result.get('total_articles', 0) > 0 and
            not parsed_result.get('annex_content') and
            '[ë³„í‘œ' in cleaned_text
        ):
            logger.info("ğŸ“‹ í˜¼í•© ë¬¸ì„œ ê°ì§€ - ë³¸ë¬¸ + Annex ë™ì‹œ ì²˜ë¦¬")
            self._apply_annex_extraction(cleaned_text, parsed_result)
        
        logger.info(f"âœ… LawParser.parse() ì™„ë£Œ:")
        logger.info(f"   - ì¥: {parsed_result['total_chapters']}ê°œ")
        logger.info(f"   - ì¡°ë¬¸: {parsed_result['total_articles']}ê°œ")
        if parsed_result.get('annex_content'):
            logger.info(f"   - Annex: {len(parsed_result['annex_content'])}ì")
        
        return parsed_result
    
    def _convert_tree_to_result(
        self, 
        tree_result: Dict[str, Any],
        document_title: str
    ) -> Dict[str, Any]:
        """TreeBuilder ê²°ê³¼ â†’ LawParser í‘œì¤€ í¬ë§· ë³€í™˜"""
        
        # Tree ì¶”ì¶œ
        tree = tree_result.get('document', {}).get('tree', [])
        
        chapters = []
        articles = []
        amendment_history = []
        basic_spirit = ""
        annex_content = ""
        
        current_chapter = ""
        section_order = 0
        
        for node in tree:
            node_type = node.get('type', '')
            
            if node_type == 'chapter':
                chapter = Chapter(
                    number=node.get('chapter_number', ''),
                    title=node.get('chapter_title', ''),
                    section_order=section_order
                )
                chapters.append(chapter)
                current_chapter = chapter.number
                section_order += 1
            
            elif node_type == 'article':
                article = Article(
                    number=node.get('article_number', ''),
                    title=node.get('article_title', ''),
                    body=node.get('content', ''),
                    chapter_number=current_chapter,
                    section_order=section_order
                )
                articles.append(article)
                section_order += 1
            
            elif node_type == 'amendment_history':
                amendment_history.append(node.get('content', ''))
            
            elif node_type == 'basic_spirit':
                basic_spirit = node.get('content', '')
            
            elif node_type == 'annex':
                annex_content = node.get('content', '')
        
        return {
            'document_title': document_title,
            'chapters': chapters,
            'articles': articles,
            'amendment_history': amendment_history,
            'basic_spirit': basic_spirit,
            'annex_content': annex_content,
            'annex_title': '',
            'annex_no': None,
            'related_article': None,
            'annex_tables': [],  # Phase 0.9ìš©
            'total_chapters': len(chapters),
            'total_articles': len(articles)
        }
    
    def _apply_annex_fallback(self, cleaned_text: str, parsed_result: dict):
        """
        ğŸ”¥ Phase 0.8 Hotfix: Annex-only ë¬¸ì„œ Fallback
        
        TreeBuilderê°€ ëª» ì¡ì€ Annexë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¶”ì¶œ
        """
        # [ë³„í‘œ N] íŒ¨í„´ ì°¾ê¸°
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… Fallback Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            # í—¤ë” íŒŒì‹±
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
                logger.info(f"   ğŸ“‹ Annex ì œëª©: [ë³„í‘œ{parsed_result['annex_no']}] {parsed_result['annex_title']}")
            
            # ê´€ë ¨ ì¡°ë¬¸ íŒŒì‹±
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
                logger.info(f"   ğŸ”— ê´€ë ¨ ì¡°ë¬¸: {parsed_result['related_article']}")
        else:
            logger.warning("   âš ï¸ Fallback: [ë³„í‘œ] íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    def _apply_annex_extraction(self, cleaned_text: str, parsed_result: dict):
        """
        âœ… Phase 0.8.2 ì‹ ê·œ: ë³¸ë¬¸+Annex í˜¼í•© ë¬¸ì„œì—ì„œ Annex ì¶”ì¶œ
        
        ì¡°ë¬¸ì€ ì´ë¯¸ íŒŒì‹±ë¨, Annexë§Œ ì¶”ê°€ë¡œ ì¶”ì¶œ
        """
        # [ë³„í‘œ N] íŒ¨í„´ ì°¾ê¸°
        pattern = r'(\[ë³„í‘œ\s*\d+\][\s\S]+)'
        match = re.search(pattern, cleaned_text)
        
        if match:
            annex_text = match.group(1).strip()
            parsed_result['annex_content'] = annex_text
            
            logger.info(f"   âœ… í˜¼í•© ë¬¸ì„œ Annex ì¶”ì¶œ: {len(annex_text)}ì")
            
            # í—¤ë” íŒŒì‹±
            header_match = re.search(r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)', annex_text)
            if header_match:
                parsed_result['annex_no'] = header_match.group(1)
                parsed_result['annex_title'] = header_match.group(2).strip()
                logger.info(f"   ğŸ“‹ Annex ì œëª©: [ë³„í‘œ{parsed_result['annex_no']}] {parsed_result['annex_title']}")
            
            # ê´€ë ¨ ì¡°ë¬¸ íŒŒì‹±
            rel_match = re.search(r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>', annex_text)
            if rel_match:
                parsed_result['related_article'] = rel_match.group(1).strip()
                logger.info(f"   ğŸ”— ê´€ë ¨ ì¡°ë¬¸: {parsed_result['related_article']}")
    
    def to_chunks(self, parsed_result: dict) -> list:
        """
        íŒŒì‹± ê²°ê³¼ â†’ RAG ì²­í¬ ë³€í™˜
        
        âœ… Phase 0.8.2: ë³¸ë¬¸ ì¡°ë¬¸ + Annex ë™ì‹œ ì²­í¬ ìƒì„±
        """
        chunks = []
        
        # Title
        if parsed_result.get('document_title'):
            chunks.append({
                'content': parsed_result['document_title'],
                'metadata': {
                    'type': 'title',
                    'boundary': 'document_title',
                    'title': parsed_result['document_title'],
                    'char_count': len(parsed_result['document_title']),
                    'section_order': -3
                }
            })
        
        # ê°œì •ì´ë ¥
        if parsed_result.get('amendment_history'):
            for i, amendment in enumerate(parsed_result['amendment_history']):
                chunks.append({
                    'content': amendment,
                    'metadata': {
                        'type': 'amendment_history',
                        'boundary': 'header',
                        'title': 'ê°œì • ì´ë ¥',
                        'char_count': len(amendment),
                        'section_order': -2 - i
                    }
                })
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            chunks.append({
                'content': parsed_result['basic_spirit'],
                'metadata': {
                    'type': 'basic_spirit',
                    'boundary': 'header',
                    'title': 'ê¸°ë³¸ì •ì‹ ',
                    'char_count': len(parsed_result['basic_spirit']),
                    'section_order': -1
                }
            })
        
        # ì¥
        for chapter in parsed_result.get('chapters', []):
            chunks.append({
                'content': f"{chapter.number} {chapter.title}",
                'metadata': {
                    'type': 'chapter',
                    'boundary': 'chapter',
                    'chapter_number': chapter.number,
                    'chapter_title': chapter.title,
                    'char_count': len(chapter.number) + len(chapter.title),
                    'section_order': chapter.section_order
                }
            })
        
        # âœ… Phase 0.8.2: ì¡°ë¬¸ ì²­í¬ ìƒì„± (í•µì‹¬!)
        for article in parsed_result.get('articles', []):
            content = f"{article.number}({article.title})\n{article.body}"
            chunks.append({
                'content': content,
                'metadata': {
                    'type': 'article',
                    'boundary': 'article',
                    'article_number': article.number,
                    'article_title': article.title,
                    'chapter_number': article.chapter_number,
                    'char_count': len(content),
                    'section_order': article.section_order
                }
            })
        
        # âœ… Phase 0.8: Annex ì„œë¸Œì²­í‚¹
        if parsed_result.get('annex_content') and ANNEX_SUBCHUNKING_AVAILABLE:
            logger.info("âœ… Phase 0.8: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘")
            
            subchunker = AnnexSubChunker()
            annex_text = parsed_result['annex_content']
            
            try:
                # ì„œë¸Œì²­í¬ ìƒì„±
                sub_chunks = subchunker.chunk(annex_text)
                
                # ê²€ì¦
                validation = validate_subchunks(sub_chunks, len(annex_text))
                
                if validation['is_valid']:
                    logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì„±ê³µ: {validation['chunk_count']}ê°œ")
                    logger.info(f"   ğŸ“Š ì†ì‹¤ë¥ : {validation['loss_rate']:.2%}")
                    logger.info(f"   ğŸ“Š íƒ€ì…: {validation['type_counts']}")
                    
                    # ì„œë¸Œì²­í¬ â†’ í‘œì¤€ ì²­í¬ í¬ë§· ë³€í™˜
                    for sub in sub_chunks:
                        chunks.append({
                            'content': sub.content,
                            'metadata': {
                                'type': f"annex_{sub.section_type}",
                                'boundary': 'annex',
                                'section_id': sub.section_id,
                                'section_type': sub.section_type,
                                'char_count': sub.char_count,
                                'section_order': sub.order,
                                **sub.metadata
                            }
                        })
                else:
                    raise ValueError("Annex ì„œë¸Œì²­í‚¹ ê²€ì¦ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Annex ì„œë¸Œì²­í‚¹ ì‹¤íŒ¨: {e} - Fallback")
                # Fallback: ê¸°ì¡´ ë‹¨ì¼ ì²­í¬
                chunks.append({
                    'content': annex_text,
                    'metadata': {
                        'type': 'annex',
                        'boundary': 'annex',
                        'title': parsed_result.get('annex_title', ''),
                        'char_count': len(annex_text),
                        'section_order': 0
                    }
                })
        
        elif parsed_result.get('annex_content'):
            # Annex ì„œë¸Œì²­í‚¹ ë¹„í™œì„±í™” - ê¸°ì¡´ ë°©ì‹
            chunks.append({
                'content': parsed_result['annex_content'],
                'metadata': {
                    'type': 'annex',
                    'boundary': 'annex',
                    'title': parsed_result.get('annex_title', ''),
                    'char_count': len(parsed_result['annex_content']),
                    'section_order': 0
                }
            })
        
        logger.info(f"âœ… ì²­í¬ ë³€í™˜ ì™„ë£Œ (Phase 0.8.2): {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for chunk in chunks:
            ctype = chunk['metadata']['type']
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")
        
        return chunks
    
    def to_markdown(self, parsed_result: dict) -> str:
        """
        íŒŒì‹± ê²°ê³¼ â†’ Markdown ë³€í™˜
        
        âœ… Phase 0.8.2: ë³¸ë¬¸ ì¡°ë¬¸ + Annex ëª¨ë‘ í¬í•¨
        """
        
        lines = []
        
        # ì œëª©
        if parsed_result.get('document_title'):
            lines.append(f"# {parsed_result['document_title']}")
            lines.append("")
        
        # ê°œì •ì´ë ¥
        if parsed_result.get('amendment_history'):
            lines.append("## ê°œì • ì´ë ¥")
            lines.append("")
            for amendment in parsed_result['amendment_history']:
                lines.append(f"- {amendment}")
            lines.append("")
        
        # ê¸°ë³¸ì •ì‹ 
        if parsed_result.get('basic_spirit'):
            lines.append("## ê¸°ë³¸ì •ì‹ ")
            lines.append("")
            lines.append(parsed_result['basic_spirit'])
            lines.append("")
        
        # ì¥ê³¼ ì¡°ë¬¸
        current_chapter = None
        for article in parsed_result.get('articles', []):
            # ìƒˆ ì¥ì´ë©´ ì¶”ê°€
            if article.chapter_number != current_chapter:
                current_chapter = article.chapter_number
                for chapter in parsed_result.get('chapters', []):
                    if chapter.number == current_chapter:
                        lines.append(f"## {chapter.number} {chapter.title}")
                        lines.append("")
                        break
            
            # ì¡°ë¬¸
            lines.append(f"### {article.number}({article.title})")
            lines.append("")
            lines.append(article.body)
            lines.append("")
        
        # âœ… Phase 0.8: Annex ì„¹ì…˜
        if parsed_result.get('annex_content'):
            annex_no = parsed_result.get('annex_no', '')
            annex_title = parsed_result.get('annex_title', '')
            related = parsed_result.get('related_article', '')
            
            # ëª…í™•í•œ ì„¹ì…˜ í—¤ë”
            if annex_no and annex_title:
                lines.append(f"## [ë³„í‘œ{annex_no}] {annex_title}")
            else:
                lines.append("## ë³„í‘œ")
            lines.append("")
            
            # ê´€ë ¨ ì¡°ë¬¸
            if related:
                lines.append(f"**ê´€ë ¨ ì¡°ë¬¸**: {related}")
                lines.append("")
            
            # í‘œ ì˜ì—­ ì‹œì‘ í‘œì‹œ
            lines.append("---")
            lines.append("")
            
            # Annex ë³¸ë¬¸
            lines.append(parsed_result['annex_content'])
            lines.append("")
            
            # í‘œ ì˜ì—­ ì¢…ë£Œ í‘œì‹œ
            lines.append("---")
            lines.append("")
        
        return "\n".join(lines)


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ Alias
parse_pdf_text = LawParser.parse