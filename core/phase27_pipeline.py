"""
PRISM Phase 2.7 Pipeline - ê°„ë‹¨í•œ OCR ê¸°ë°˜ ë²„ì „

Author: ì´ì„œì˜ (Backend Lead)
Date: 2025-10-20
Fix: OCR ê¸°ë°˜ ê°„ë‹¨í•œ êµ¬í˜„ (LayoutDetector, HybridExtractor, IntelligentChunker ë‹¨ìˆœí™”)
"""

import os
import json
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from PIL import Image

# ê°„ë‹¨í•œ ëª¨ë“ˆë“¤ import
from core.layout_detector import LayoutDetector, Region
from core.hybrid_extractor import HybridExtractor, ExtractedContent
from core.intelligent_chunker import IntelligentChunker, Chunk


class Phase27Pipeline:
    """
    PRISM Phase 2.7 íŒŒì´í”„ë¼ì¸ (ê°„ë‹¨ ë²„ì „)
    
    Features:
    - PyMuPDF ê¸°ë°˜ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    - ì „ì²´ í˜ì´ì§€ë¥¼ TEXT ì˜ì—­ìœ¼ë¡œ ì²˜ë¦¬
    - OCR (Tesseract/PaddleOCR) í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - ê¸¸ì´ ê¸°ë°˜ ì²­í‚¹
    """
    
    def __init__(self, vlm_provider: str = "claude"):
        """
        Args:
            vlm_provider: VLM ì œê³µì (í˜„ì¬ ë¯¸ì‚¬ìš©, OCRë§Œ ì‚¬ìš©)
        """
        self.vlm_provider = vlm_provider
        
        # ì„œë¸Œ ëª¨ë“ˆ ì´ˆê¸°í™”
        print("\n" + "="*60)
        print("PRISM Phase 2.7 Pipeline Initialization")
        print("="*60)
        
        self.layout_detector = LayoutDetector(vlm_provider=vlm_provider)
        self.extractor = HybridExtractor(vlm_provider=vlm_provider, ocr_engine='tesseract')
        self.chunker = IntelligentChunker(
            min_chunk_size=100,
            max_chunk_size=500,
            overlap=50
        )
        
        print("="*60 + "\n")
    
    def process_pdf(
        self,
        pdf_path: str,
        output_dir: Optional[str] = None
    ) -> Dict:
        """
        PDF ë¬¸ì„œ ì²˜ë¦¬
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ íƒ)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        start_time = time.time()
        
        print("\n" + "="*60)
        print(f"ğŸ”· PRISM Phase 2.7 - PDF Processing")
        print("="*60)
        print(f"ğŸ“„ Input: {pdf_path}")
        print(f"ğŸ¤– VLM Provider: {self.vlm_provider} (OCR mode)")
        print("="*60 + "\n")
        
        # Step 1: PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
        print("ğŸ–¼ï¸  Step 1: Converting PDF to images...")
        page_images = self._pdf_to_images(pdf_path)
        total_pages = len(page_images)
        print(f"   âœ“ Converted {total_pages} page(s)\n")
        
        # Step 2: í˜ì´ì§€ë³„ ì²˜ë¦¬
        all_chunks = []
        all_stats = []
        
        for page_num, page_image in enumerate(page_images, start=1):
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“„ Processing Page {page_num}/{total_pages}")
            print(f"{'â”€'*60}")
            
            # í˜ì´ì§€ ì²˜ë¦¬
            page_chunks, stats = self._process_page(page_image, page_num)
            
            all_chunks.extend(page_chunks)
            all_stats.append(stats)
            
            print(f"âœ“ Page {page_num} completed: {len(page_chunks)} chunk(s)")
        
        # Step 3: ê²°ê³¼ ìƒì„±
        elapsed_time = time.time() - start_time
        
        result = {
            'metadata': {
                'filename': Path(pdf_path).name,
                'total_pages': total_pages,
                'total_chunks': len(all_chunks),
                'processing_time_sec': elapsed_time,
                'vlm_provider': self.vlm_provider,
                'processed_at': datetime.now().isoformat(),
                'chunk_types': self._count_chunk_types(all_chunks)
            },
            'stage1_elements': self._extract_stage1_from_stats(all_stats),
            'stage2_chunks': [self._chunk_to_dict(chunk) for chunk in all_chunks]
        }
        
        # Step 4: íŒŒì¼ ì €ì¥ (ì„ íƒ)
        if output_dir:
            self._save_results(result, output_dir)
        
        # ì™„ë£Œ ë©”ì‹œì§€
        print("\n" + "="*60)
        print("âœ… Processing Complete!")
        print("="*60)
        print(f"â±ï¸  Total time: {elapsed_time:.1f}s")
        print(f"ğŸ“Š Total chunks: {len(all_chunks)}")
        print(f"ğŸ“ˆ Chunk types: {result['metadata']['chunk_types']}")
        
        return result
    
    def _process_page(
        self,
        page_image: Image.Image,
        page_num: int
    ) -> Tuple[List[Chunk], Dict]:
        """
        í˜ì´ì§€ ì²˜ë¦¬
        
        Args:
            page_image: PIL Image
            page_num: í˜ì´ì§€ ë²ˆí˜¸
            
        Returns:
            (ì²­í¬ ë¦¬ìŠ¤íŠ¸, í†µê³„)
        """
        
        # Step 1: ì´ë¯¸ì§€ í™•ì¸
        print("ğŸ–¼ï¸  Step 1: Converting page to image...")
        print(f"   Image size: {page_image.width}x{page_image.height}")
        
        # Step 2: ë ˆì´ì•„ì›ƒ ê°ì§€ (ì „ì²´ í˜ì´ì§€ = 1ê°œ region)
        print("ğŸ” Step 2: Detecting layout regions...")
        regions = self.layout_detector.detect_regions(page_image)
        print(f"   Found {len(regions)} region(s)")
        
        # Step 3: ì˜ì—­ë³„ ì»¨í…ì¸  ì¶”ì¶œ
        print("ğŸ“ Step 3: Extracting content from regions...")
        page_chunks = []
        
        for i, region in enumerate(regions, start=1):
            # Bbox ê²€ì¦
            valid_bbox = self._validate_and_fix_bbox(
                region.bbox, 
                page_image.width, 
                page_image.height
            )
            
            if valid_bbox is None:
                print(f"   âš ï¸  Region {i}: Invalid bbox, skipping...")
                continue
            
            # ì˜ì—­ ì´ë¯¸ì§€ ì˜ë¼ë‚´ê¸°
            try:
                region_image = page_image.crop(valid_bbox)
            except Exception as e:
                print(f"   âŒ Region {i}: Crop failed - {e}")
                continue
            
            # ì»¨í…ì¸  ì¶”ì¶œ (OCR)
            print(f"   Region {i}/{len(regions)}: {region.type} - {region.description}")
            
            try:
                extracted = self.extractor.extract(
                    image=region_image,
                    region_type=region.type,
                    description=region.description
                )
                
                content_len = len(extracted.content)
                print(f"      âœ“ Extracted {content_len} characters (confidence: {extracted.confidence:.2f})")
                
                # ë‚´ìš©ì´ ìˆìœ¼ë©´ ì²­í‚¹
                if content_len > 0:
                    print(f"      ğŸ”„ Chunking {content_len} characters...")
                    
                    region_chunks = self.chunker.chunk_region(
                        content=extracted.content,
                        region_type=extracted.type,
                        page_num=page_num,
                        section_path=region.description,
                        source=extracted.metadata.get('source', 'unknown')
                    )
                    
                    page_chunks.extend(region_chunks)
                    print(f"      âœ“ Created {len(region_chunks)} chunk(s)")
                else:
                    print(f"      âš ï¸  No content extracted")
                
            except Exception as e:
                print(f"      âŒ Extraction failed: {e}")
                continue
        
        # Step 4: ì²­í‚¹ ì™„ë£Œ
        print("âœ‚ï¸  Step 4: Creating intelligent chunks...")
        print(f"   Total: {len(page_chunks)} chunk(s) created")
        
        # í†µê³„
        stats = {
            'page_num': page_num,
            'regions': len(regions),
            'chunks': len(page_chunks),
            'region_types': self._count_region_types(regions)
        }
        
        return page_chunks, stats
    
    def _validate_and_fix_bbox(
        self,
        bbox: Tuple[int, int, int, int],
        img_width: int,
        img_height: int
    ) -> Optional[Tuple[int, int, int, int]]:
        """Bbox ì¢Œí‘œ ê²€ì¦ ë° ìˆ˜ì •"""
        
        left, top, right, bottom = bbox
        
        # 1. ì¢Œí‘œ ìˆœì„œ í™•ì¸
        if left > right:
            left, right = right, left
        if top > bottom:
            top, bottom = bottom, top
        
        # 2. ì¢Œí‘œê°€ ë™ì¼í•˜ë©´ ë¬´íš¨
        if left == right or top == bottom:
            return None
        
        # 3. ì´ë¯¸ì§€ ë²”ìœ„ ë‚´ë¡œ ì œí•œ
        left = max(0, min(left, img_width - 1))
        top = max(0, min(top, img_height - 1))
        right = max(left + 1, min(right, img_width))
        bottom = max(top + 1, min(bottom, img_height))
        
        # 4. ìµœì†Œ í¬ê¸° í™•ì¸
        if (right - left) < 1 or (bottom - top) < 1:
            return None
        
        return (left, top, right, bottom)
    
    def _pdf_to_images(self, pdf_path: str) -> List[Image.Image]:
        """PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ (PyMuPDF)"""
        
        import fitz  # PyMuPDF
        
        doc = fitz.open(pdf_path)
        images = []
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # ê³ í•´ìƒë„ ë Œë”ë§ (300 DPI)
            mat = fitz.Matrix(300/72, 300/72)
            pix = page.get_pixmap(matrix=mat)
            
            # PIL Imageë¡œ ë³€í™˜
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            images.append(img)
        
        doc.close()
        
        return images
    
    def _count_region_types(self, regions: List[Region]) -> Dict[str, int]:
        """Region íƒ€ì… ì§‘ê³„"""
        counts = {}
        for region in regions:
            counts[region.type] = counts.get(region.type, 0) + 1
        return counts
    
    def _count_chunk_types(self, chunks: List[Chunk]) -> Dict[str, int]:
        """Chunk íƒ€ì… ì§‘ê³„"""
        counts = {}
        for chunk in chunks:
            counts[chunk.type] = counts.get(chunk.type, 0) + 1
        return counts
    
    def _extract_stage1_from_stats(self, all_stats: List[Dict]) -> List[Dict]:
        """í†µê³„ì—ì„œ Stage 1 ì •ë³´ ì¶”ì¶œ"""
        elements = []
        for stats in all_stats:
            for region_type, count in stats.get('region_types', {}).items():
                elements.append({
                    'page_number': stats['page_num'],
                    'type': region_type,
                    'count': count
                })
        return elements
    
    def _chunk_to_dict(self, chunk: Chunk) -> Dict:
        """Chunk ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'chunk_id': f"chunk_{chunk.page_num}_{id(chunk)}",
            'page_number': chunk.page_num,
            'element_type': chunk.type,
            'content': chunk.content,
            'metadata': chunk.metadata,
            'model_used': self.vlm_provider,
            'processing_time_sec': 0  # í˜„ì¬ ë¯¸ì¸¡ì •
        }
    
    def _save_results(self, result: Dict, output_dir: str):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ì €ì¥
        json_path = output_path / f"result_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Results saved to: {json_path}")


# CLI ì‹¤í–‰
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python phase27_pipeline.py <pdf_path> [output_dir]")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else 'output'
    
    pipeline = Phase27Pipeline()
    result = pipeline.process_pdf(pdf_path, output_dir)