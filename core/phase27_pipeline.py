"""
PRISM Phase 2.7 - Main Processing Pipeline
2-Stage Pipeline + Intelligent Chunking

Stage 1: Layout Detection
Stage 2: Hybrid Extraction
Stage 3: Intelligent Chunking

Author: ì´ì„œì˜ (Backend Lead)
Date: 2025-10-20
"""

import time
from pathlib import Path
from typing import List, Dict, Optional
from PIL import Image
import fitz  # PyMuPDF

from core.layout_detector import LayoutDetector, Region
from core.hybrid_extractor import HybridExtractor, ExtractedContent
from core.intelligent_chunker import IntelligentChunker, Chunk


class Phase27Pipeline:
    """
    PRISM Phase 2.7 ë©”ì¸ íŒŒì´í”„ë¼ì¸
    
    íŠ¹ì§•:
    - 2-Stage ì²˜ë¦¬ (Layout â†’ Extraction)
    - í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ (OCR + VLM)
    - ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹
    - RAG ìµœì í™”
    """
    
    def __init__(
        self,
        min_chunk_size: int = 100,
        max_chunk_size: int = 500,
        overlap_size: int = 50
    ):
        """
        Args:
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸° (í† í°)
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (í† í°)
            overlap_size: ì²­í¬ ê°„ ì˜¤ë²„ë© (í† í°)
        """
        print("\n" + "="*60)
        print("PRISM Phase 2.7 Pipeline Initialization")
        print("="*60 + "\n")
        
        # Stage 1: Layout Detection
        self.layout_detector = LayoutDetector()
        
        # Stage 2: Hybrid Extraction
        self.extractor = HybridExtractor()
        
        # Stage 3: Intelligent Chunking
        self.chunker = IntelligentChunker(
            min_chunk_size=min_chunk_size,
            max_chunk_size=max_chunk_size,
            overlap_size=overlap_size
        )
        
        print("\nâœ… Pipeline ready!\n")
    
    def process_pdf(
        self,
        pdf_path: str,
        max_pages: Optional[int] = None
    ) -> Dict:
        """
        PDF ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        print(f"ğŸ“„ Processing PDF: {pdf_path}")
        print(f"â±ï¸  Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # PDF ì—´ê¸°
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        if max_pages:
            total_pages = min(total_pages, max_pages)
        
        print(f"ğŸ“– Total pages: {total_pages}\n")
        
        # í˜ì´ì§€ë³„ ì²˜ë¦¬
        all_chunks = []
        
        for page_num in range(total_pages):
            print(f"{'='*60}")
            print(f"ğŸ“„ Processing Page {page_num + 1}/{total_pages}")
            print(f"{'='*60}\n")
            
            page_chunks = self._process_page(doc, page_num)
            all_chunks.extend(page_chunks)
            
            print(f"\nâœ… Page {page_num + 1} completed: {len(page_chunks)} chunks generated\n")
        
        doc.close()
        
        # ê²°ê³¼ í†µê³„
        elapsed_time = time.time() - start_time
        
        result = {
            'metadata': {
                'processed_at': time.strftime('%Y-%m-%dT%H:%M:%S'),
                'total_pages': total_pages,
                'total_chunks': len(all_chunks),
                'processing_time_seconds': round(elapsed_time, 2),
                'chunk_types': self._count_chunk_types(all_chunks)
            },
            'chunks': [chunk.to_dict() for chunk in all_chunks]
        }
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ Processing Complete!")
        print(f"{'='*60}")
        print(f"â±ï¸  Total time: {elapsed_time:.1f}s")
        print(f"ğŸ“Š Total chunks: {len(all_chunks)}")
        print(f"ğŸ“ˆ Chunk types: {result['metadata']['chunk_types']}")
        print()
        
        return result
    
    def _process_page(self, doc: fitz.Document, page_num: int) -> List[Chunk]:
        """
        ë‹¨ì¼ í˜ì´ì§€ ì²˜ë¦¬
        
        Args:
            doc: PDF ë¬¸ì„œ ê°ì²´
            page_num: í˜ì´ì§€ ë²ˆí˜¸ (0-based)
            
        Returns:
            Chunk ë¦¬ìŠ¤íŠ¸
        """
        page = doc[page_num]
        
        # 1. í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        print("ğŸ–¼ï¸  Step 1: Converting page to image...")
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x resolution
        img_data = pix.tobytes("png")
        
        from io import BytesIO
        page_image = Image.open(BytesIO(img_data))
        print(f"   Image size: {page_image.size[0]}x{page_image.size[1]}\n")
        
        # 2. Stage 1: Layout Detection
        print("ğŸ” Step 2: Detecting layout regions...")
        regions = self.layout_detector.detect(page_image)
        
        if not regions:
            print("   âš ï¸  No regions detected, treating whole page as text\n")
            # ì „ì²´ í˜ì´ì§€ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ì˜ì—­ìœ¼ë¡œ
            regions = [Region(
                type='text',
                bbox=(0, 0, page_image.size[0], page_image.size[1]),
                confidence=0.5,
                description='Full page'
            )]
        
        print(f"   Found {len(regions)} regions\n")
        
        # 3. Stage 2: Hybrid Extraction
        print("ğŸ“ Step 3: Extracting content from regions...")
        extracted_contents = []
        
        for i, region in enumerate(regions, 1):
            print(f"   Region {i}/{len(regions)}: {region.type} - {region.description}")
            
            # ì˜ì—­ ì´ë¯¸ì§€ crop
            region_image = self.layout_detector.crop_region(page_image, region)
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ
            content = self.extractor.extract(
                region_image=region_image,
                region_type=region.type,
                description=region.description
            )
            
            extracted_contents.append(content)
            print(f"      âœ“ Extracted {len(content.content)} characters (confidence: {content.confidence:.2f})")
        
        print()
        
        # 4. Stage 3: Intelligent Chunking
        print("âœ‚ï¸  Step 4: Creating intelligent chunks...")
        all_chunks = []
        
        for i, content in enumerate(extracted_contents, 1):
            region_desc = regions[i-1].description if i <= len(regions) else "Region"
            
            chunks = self.chunker.chunk_content(
                content=content.content,
                content_type=content.type,
                page_num=page_num + 1,  # 1-based page number
                base_section=region_desc,
                metadata=content.metadata
            )
            
            all_chunks.extend(chunks)
            print(f"   Region {i}: {len(chunks)} chunk(s) created")
        
        return all_chunks
    
    def _count_chunk_types(self, chunks: List[Chunk]) -> Dict[str, int]:
        """ì²­í¬ íƒ€ì…ë³„ ê°œìˆ˜ ì„¸ê¸°"""
        counts = {}
        for chunk in chunks:
            counts[chunk.type] = counts.get(chunk.type, 0) + 1
        return counts


# ============================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("PRISM Phase 2.7 - Pipeline Test")
    print("="*60 + "\n")
    
    pipeline = Phase27Pipeline()
    
    # í…ŒìŠ¤íŠ¸ PDF ê²½ë¡œ í™•ì¸
    test_pdf = Path("input/test_document.pdf")
    
    if test_pdf.exists():
        print(f"âœ… Test PDF found: {test_pdf}\n")
        
        # ì²˜ë¦¬ ì‹¤í–‰
        result = pipeline.process_pdf(str(test_pdf), max_pages=3)
        
        print("\nğŸ“Š Result Summary:")
        print(f"   Chunks: {result['metadata']['total_chunks']}")
        print(f"   Types: {result['metadata']['chunk_types']}")
        print(f"   Time: {result['metadata']['processing_time_seconds']}s")
    else:
        print(f"âš ï¸  Test PDF not found: {test_pdf}")
        print(f"   Please place a PDF file at: {test_pdf.absolute()}")