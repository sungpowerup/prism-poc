"""
PRISM Phase 2.7 - Complete Pipeline
ë ˆì´ì•„ì›ƒ ê°ì§€ â†’ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ â†’ ì§€ëŠ¥í˜• ì²­í‚¹

Author: ì´ì„œì˜ (Backend Lead) + ë°•ì¤€í˜¸ (AI/ML Lead)
Date: 2025-10-20
Fix: IntelligentChunker íŒŒë¼ë¯¸í„° ìˆ˜ì • (overlap_size â†’ overlap)
"""

import os
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from PIL import Image
import json

from core.layout_detector import LayoutDetector, Region
from core.hybrid_extractor import HybridExtractor
from core.intelligent_chunker import IntelligentChunker, Chunk


class Phase27Pipeline:
    """
    Phase 2.7 ì™„ì „ ìë™ íŒŒì´í”„ë¼ì¸
    
    ë‹¨ê³„:
    1. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    2. ë ˆì´ì•„ì›ƒ ê°ì§€ (LayoutDetector)
    3. ì˜ì—­ë³„ ì»¨í…ì¸  ì¶”ì¶œ (HybridExtractor)
    4. ì§€ëŠ¥í˜• ì²­í‚¹ (IntelligentChunker)
    """
    
    def __init__(
        self,
        vlm_provider: str = 'claude',
        chunk_min_size: int = 100,
        chunk_max_size: int = 500,
        chunk_overlap: int = 50  # âœ… ìˆ˜ì •: overlap_size â†’ chunk_overlap
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            vlm_provider: VLM í”„ë¡œë°”ì´ë”
            chunk_min_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            chunk_max_size: ìµœëŒ€ ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ  # âœ… ìˆ˜ì •
        """
        
        print("\n" + "="*60)
        print("PRISM Phase 2.7 Pipeline Initialization")
        print("="*60)
        print()
        print(f"ğŸ¤– VLM Provider: {vlm_provider.upper()}")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.layout_detector = LayoutDetector(vlm_provider=vlm_provider)
        self.extractor = HybridExtractor(vlm_provider=vlm_provider)
        
        # âœ… ìˆ˜ì •: íŒŒë¼ë¯¸í„° ì´ë¦„ ë³€ê²½
        self.chunker = IntelligentChunker(
            min_chunk_size=chunk_min_size,
            max_chunk_size=chunk_max_size,
            overlap=chunk_overlap  # âœ… ìˆ˜ì •: overlap_size â†’ overlap
        )
        
        print()
        print("âœ… Pipeline ready!")
        print()
    
    def process_pdf(
        self,
        pdf_path: str,
        output_dir: Optional[str] = None
    ) -> Dict:
        """
        PDF ì²˜ë¦¬ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            output_dir: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        start_time = time.time()
        
        # PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
        print(f"ğŸ“„ Processing PDF: {pdf_path}")
        print(f"â±ï¸  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        page_images = self._pdf_to_images(pdf_path)
        print(f"ğŸ“– Total pages: {len(page_images)}")
        print()
        
        # í˜ì´ì§€ë³„ ì²˜ë¦¬
        all_chunks = []
        page_stats = []
        
        for page_num, page_image in enumerate(page_images, start=1):
            print("="*60)
            print(f"ğŸ“„ Processing Page {page_num}/{len(page_images)}")
            print("="*60)
            print()
            
            page_chunks, stats = self._process_page(page_image, page_num)
            
            all_chunks.extend(page_chunks)
            page_stats.append(stats)
            
            print(f"\nâœ… Page {page_num} completed: {len(page_chunks)} chunks generated")
            print()
        
        # ê²°ê³¼ ì •ë¦¬
        elapsed_time = time.time() - start_time
        
        result = {
            'metadata': {
                'processed_at': datetime.now().isoformat(),
                'total_pages': len(page_images),
                'total_chunks': len(all_chunks),
                'processing_time_seconds': round(elapsed_time, 2),
                'chunk_types': self._count_chunk_types(all_chunks),
                'vlm_provider': os.getenv('DEFAULT_VLM_PROVIDER', 'claude')
            },
            'chunks': [chunk.to_dict() for chunk in all_chunks],
            'page_stats': page_stats
        }
        
        # ê²°ê³¼ ì €ì¥
        if output_dir:
            self._save_results(result, output_dir, pdf_path)
        
        # ì™„ë£Œ ë©”ì‹œì§€
        print("="*60)
        print("ğŸ‰ Processing Complete!")
        print("="*60)
        print(f"â±ï¸  Total time: {elapsed_time:.1f}s")
        print(f"ğŸ“Š Total chunks: {len(all_chunks)}")
        print(f"ğŸ“ˆ Chunk types: {result['metadata']['chunk_types']}")
        
        return result
    
    def _process_page(
        self,
        page_image: Image.Image,
        page_num: int
    ) -> Tuple[List[Chunk], Dict]:
        """
        í˜ì´ì§€ ì²˜ë¦¬
        
        Args:
            page_image: PIL Image
            page_num: í˜ì´ì§€ ë²ˆí˜¸
            
        Returns:
            (ì²­í¬ ë¦¬ìŠ¤íŠ¸, í†µê³„)
        """
        
        # Step 1: ì´ë¯¸ì§€ ë³€í™˜ í™•ì¸
        print("ğŸ–¼ï¸  Step 1: Converting page to image...")
        print(f"   Image size: {page_image.width}x{page_image.height}")
        
        # Step 2: ë ˆì´ì•„ì›ƒ ê°ì§€
        print("ğŸ” Step 2: Detecting layout regions...")
        regions = self.layout_detector.detect_regions(page_image)
        print(f"   Found {len(regions)} regions")
        
        # Step 3: ì˜ì—­ë³„ ì»¨í…ì¸  ì¶”ì¶œ
        print("ğŸ“ Step 3: Extracting content from regions...")
        page_chunks = []
        
        for i, region in enumerate(regions, start=1):
            # ì˜ì—­ ì´ë¯¸ì§€ ì˜ë¼ë‚´ê¸°
            region_image = page_image.crop(region.bbox)
            
            # ì»¨í…ì¸  ì¶”ì¶œ
            print(f"   Region {i}/{len(regions)}: {region.type} - {region.description}")
            extracted = self.extractor.extract(
                image=region_image,
                region_type=region.type,
                description=region.description
            )
            
            print(f"      âœ“ Extracted {len(extracted.content)} characters (confidence: {extracted.confidence:.2f})")
            
            # ì²­í‚¹
            region_chunks = self.chunker.chunk_region(
                content=extracted.content,
                region_type=extracted.type,
                page_num=page_num,
                section_path=region.description,
                source=extracted.metadata.get('source', 'unknown')
            )
            
            page_chunks.extend(region_chunks)
        
        # Step 4: ì²­í‚¹ ì™„ë£Œ
        print("âœ‚ï¸  Step 4: Creating intelligent chunks...")
        for i, region in enumerate(regions, start=1):
            region_chunk_count = sum(
                1 for chunk in page_chunks
                if chunk.metadata['section_path'] == region.description
            )
            print(f"   Region {i}: {region_chunk_count} chunk(s) created")
        
        # í†µê³„
        stats = {
            'page_num': page_num,
            'regions': len(regions),
            'chunks': len(page_chunks),
            'region_types': self._count_region_types(regions)
        }
        
        return page_chunks, stats
    
    def _pdf_to_images(self, pdf_path: str) -> List[Image.Image]:
        """PDF â†’ ì´ë¯¸ì§€ ë³€í™˜"""
        
        import fitz  # PyMuPDF
        
        doc = fitz.open(pdf_path)
        images = []
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # ê³ í•´ìƒë„ ë Œë”ë§ (300 DPI)
            mat = fitz.Matrix(300/72, 300/72)
            pix = page.get_pixmap(matrix=mat)
            
            # PIL Imageë¡œ ë³€í™˜
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            images.append(img)
        
        doc.close()
        
        return images
    
    def _count_chunk_types(self, chunks: List[Chunk]) -> Dict[str, int]:
        """ì²­í¬ íƒ€ì…ë³„ ì¹´ìš´íŠ¸"""
        counts = {}
        for chunk in chunks:
            counts[chunk.type] = counts.get(chunk.type, 0) + 1
        return counts
    
    def _count_region_types(self, regions: List[Region]) -> Dict[str, int]:
        """ì˜ì—­ íƒ€ì…ë³„ ì¹´ìš´íŠ¸"""
        counts = {}
        for region in regions:
            counts[region.type] = counts.get(region.type, 0) + 1
        return counts
    
    def _save_results(self, result: Dict, output_dir: str, pdf_path: str):
        """ê²°ê³¼ ì €ì¥"""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        pdf_name = Path(pdf_path).stem
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ì €ì¥
        json_path = output_path / f"prism_result_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Results saved to: {json_path}")
        
        # Markdown ì €ì¥
        md_path = output_path / f"prism_result_{timestamp}.md"
        self._save_markdown(result, md_path)
        print(f"ğŸ’¾ Markdown saved to: {md_path}")
    
    def _save_markdown(self, result: Dict, md_path: Path):
        """Markdown í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        
        with open(md_path, 'w', encoding='utf-8') as f:
            # í—¤ë”
            f.write("# PRISM Phase 2.7 - ì²˜ë¦¬ ê²°ê³¼\n\n")
            
            meta = result['metadata']
            f.write(f"**ì²˜ë¦¬ ì¼ì‹œ:** {meta['processed_at']}\n")
            f.write(f"**ì´ í˜ì´ì§€:** {meta['total_pages']}\n")
            f.write(f"**ì´ ì²­í¬:** {meta['total_chunks']}\n")
            f.write(f"**ì²˜ë¦¬ ì‹œê°„:** {meta['processing_time_seconds']:.2f}ì´ˆ\n\n")
            
            # ì²­í¬ íƒ€ì… í†µê³„
            f.write("## ì²­í¬ íƒ€ì…ë³„ í†µê³„\n\n")
            for chunk_type, count in meta['chunk_types'].items():
                f.write(f"- **{chunk_type}**: {count}ê°œ\n")
            f.write("\n---\n\n")
            
            # ì²­í¬ë³„ ë‚´ìš©
            for chunk in result['chunks']:
                f.write(f"## ğŸ“ {chunk['chunk_id']}\n\n")
                f.write(f"**í˜ì´ì§€:** {chunk['page_num']} | ")
                f.write(f"**íƒ€ì…:** {chunk['type']} | ")
                f.write(f"**í† í°:** {chunk['metadata']['token_count']}\n")
                f.write(f"**ê²½ë¡œ:** {chunk['metadata']['section_path']}\n\n")
                f.write("### ë‚´ìš©\n\n")
                f.write(f"{chunk['content']}\n\n")
                f.write("---\n\n")


# CLI ì‹¤í–‰
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python phase27_pipeline.py <pdf_path> [output_dir]")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else 'output'
    
    pipeline = Phase27Pipeline()
    result = pipeline.process_pdf(pdf_path, output_dir)