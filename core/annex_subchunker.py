"""
core/annex_subchunker.py - Phase 0.9.5 Complete Restructure

Phase 0.9.5 í•µì‹¬ ìˆ˜ì •ì‚¬í•­ (ë¯¸ì†¡ë‹˜ ê°€ì´ë“œ):
1. âœ… Annex ì™„ì „ ë¶„ë¦¬ (raw ë‹¨ìœ„)
2. âœ… ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™” (annex_paragraph ì‹ ì„¤)
3. âœ… í‘œ ìš°ì„  ì²˜ë¦¬ ê¸ˆì§€ â†’ ë¬¸ë‹¨ ê¸°ì¤€ ì¬ì„¤ê³„
4. âœ… noteëŠ” ê°€ì¥ ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œë§Œ ì¶”ì¶œ
5. âœ… Annex Loss CheckëŠ” ë§ˆì§€ë§‰ì—ë§Œ ìˆ˜í–‰ (Â±3% ì´í•˜)
6. âœ… DualQA ì¡°ë¬¸ ë¹„êµ ë¡œì§ì€ ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€

ê°€ë“œë ˆì¼ ì¤€ìˆ˜:
- ğŸ›‘ ê¸°ì¡´ ê¸°ëŠ¥ ì ˆëŒ€ ìˆ˜ì • ê¸ˆì§€ (TableParser, spacing, DualQA ë³´ì¡´)
- ğŸ›‘ spacingì€ review ë‹¨ê³„ì—ì„œë§Œ ì ìš©
- ğŸ›‘ Loss CheckëŠ” ìµœì¢… ë‹¨ê³„ì—ë§Œ

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-22
Version: Phase 0.9.5
"""

import re
import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class SubChunk:
    """ì„œë¸Œì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    section_id: str
    section_type: str
    content: str
    metadata: Dict[str, Any]
    char_count: int
    order: int


class AnnexSubChunker:
    """
    Annex ì„œë¸Œì²­í‚¹ (Phase 0.9.5 Complete Restructure)
    
    ë¯¸ì†¡ë‹˜ ì›ì¹™:
    - í‘œ ì—†ëŠ” ë³„í‘œ: header + paragraphÃ—N + note
    - í‘œ ìˆëŠ” ë³„í‘œ: header + paragraph + table_rows + paragraph + note
    - í…ìŠ¤íŠ¸ ì†ì‹¤ Â±3% ì´í•˜ ë³´ì¥
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        
        self.patterns = {
            'annex_header': r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)',
            'related_article': r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>',
            'amendment': r'\[(.*?(\d{4}\.\d{1,2}\.\d{1,2}).*?)\]',
            'note_marker': r'^\*\s*(.+)$',  # Note íŒì •ìš©
            'table_start': r'^\d+\s+',  # í‘œ ì‹œì‘ íŒì •ìš©
        }
        
        logger.info("âœ… AnnexSubChunker v0.9.5 ì´ˆê¸°í™” (Complete Restructure)")
    
    def chunk(self, annex_text: str, annex_no: str = "1") -> List[SubChunk]:
        """
        Annex í…ìŠ¤íŠ¸ â†’ ì„œë¸Œì²­í‚¹ (Phase 0.9.5)
        
        ë¯¸ì†¡ë‹˜ 6ë‹¨ê³„:
        1. Annex ì™„ì „ ë¶„ë¦¬
        2. ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™”
        3. í‘œ ìœ„/ì•„ë˜ ë¬¸ë‹¨ ë³´ì¡´
        4. noteëŠ” ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œë§Œ
        5. Loss Check ìµœì¢… ë‹¨ê³„
        6. DualQA ì˜í–¥ ì—†ìŒ
        """
        logger.info(f"ğŸ”§ Phase 0.9.5: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘: {len(annex_text)}ì")
        
        # Step 1: Annex ì™„ì „ ë¶„ë¦¬ (raw ë‹¨ìœ„)
        annex_sections = self._split_by_annex(annex_text)
        
        if not annex_sections:
            logger.warning("âš ï¸ ë³„í‘œ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - fallback ì²˜ë¦¬")
            return self._fallback_chunk(annex_text, annex_no)
        
        logger.info(f"âœ… Step 1: ë³„í‘œ ë¶„ë¦¬ ì™„ë£Œ: {len(annex_sections)}ê°œ")
        
        # âœ… Phase 0.9.5.1: Canonical text ìƒì„± (Loss Check ê¸°ì¤€)
        canonical_text = self._clean_annex_text(annex_text)
        
        # Step 2-4: ê° ë³„í‘œë§ˆë‹¤ ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™”
        all_chunks = []
        global_order = 0
        
        for annex_sec in annex_sections:
            annex_num = annex_sec['annex_no']
            raw_content = annex_sec['content']
            header_end_pos = annex_sec['header_end_pos']  # âœ… Phase 0.9.5.1
            
            logger.info(f"   ğŸ”¹ ë³„í‘œ{annex_num} ì²˜ë¦¬ ì¤‘... ({len(raw_content)}ì)")
            
            # ë…¸ì´ì¦ˆ ì œê±° (ë³„í‘œ ë¶„ë¦¬ í›„)
            cleaned_content = self._clean_annex_text(raw_content)
            
            # ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™”
            section_chunks = self._process_single_annex_v095(
                cleaned_content,
                annex_num,
                global_order,
                header_end_pos  # âœ… Phase 0.9.5.1: Header ë ìœ„ì¹˜ ì „ë‹¬
            )
            
            all_chunks.extend(section_chunks)
            global_order += len(section_chunks)
            
            logger.info(f"   âœ… ë³„í‘œ{annex_num}: {len(section_chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # Step 5: Annex Loss Check (ìµœì¢… ë‹¨ê³„ë§Œ!)
        # âœ… Phase 0.9.5.1: Canonical text ê¸°ì¤€ìœ¼ë¡œ ê²€ì¦
        self._validate_annex_loss(annex_sections, all_chunks, canonical_text)
        
        logger.info(f"âœ… Phase 0.9.5.1: Annex ì„œë¸Œì²­í‚¹ ì™„ë£Œ: ì´ {len(all_chunks)}ê°œ")
        self._log_chunk_types(all_chunks)
        
        return all_chunks
    
    def _split_by_annex(self, text: str) -> List[Dict[str, Any]]:
        """
        Step 1: ë³„í‘œë³„ë¡œ í…ìŠ¤íŠ¸ ì™„ì „ ë¶„ë¦¬ (raw ë‹¨ìœ„)
        
        âœ… Phase 0.9.5.1 Hotfix: Header end position ì •í™•íˆ ê³„ì‚°
        
        Returns:
            [
                {
                    'annex_no': '1',
                    'title': '...',
                    'content': '...',  â† RAW í…ìŠ¤íŠ¸ (êµ¬ì¡°í™” ì „)
                    'header_end_pos': 100,  â† ì‹ ê·œ: Header ë ìœ„ì¹˜
                    'start_pos': 0,
                    'end_pos': 100
                },
                ...
            ]
        """
        matches = list(re.finditer(self.patterns['annex_header'], text))
        
        if not matches:
            return []
        
        sections = []
        
        for i, match in enumerate(matches):
            annex_no = match.group(1)
            annex_title = match.group(2).strip()
            start_pos = match.start()
            header_end_pos = match.end()  # âœ… Phase 0.9.5.1: Header ë = Body ì‹œì‘
            
            # ë‹¤ìŒ ë³„í‘œê¹Œì§€ or ëê¹Œì§€
            if i + 1 < len(matches):
                end_pos = matches[i + 1].start()
            else:
                end_pos = len(text)
            
            content = text[start_pos:end_pos]
            
            sections.append({
                'annex_no': annex_no,
                'title': annex_title,
                'content': content,  # RAW í…ìŠ¤íŠ¸
                'header_end_pos': header_end_pos - start_pos,  # âœ… ìƒëŒ€ ìœ„ì¹˜
                'start_pos': start_pos,
                'end_pos': end_pos
            })
        
        return sections
    
    def _process_single_annex_v095(
        self,
        content: str,
        annex_no: str,
        start_order: int,
        header_end_pos: int  # âœ… Phase 0.9.5.1: Header ë ìœ„ì¹˜
    ) -> List[SubChunk]:
        """
        Step 2-4: ë‹¨ì¼ ë³„í‘œ ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™” (Phase 0.9.5.1 Hotfix)
        
        âœ… Phase 0.9.5.1 ë³€ê²½ì‚¬í•­:
        - header_end_posë¥¼ ì •ê·œì‹ ê¸°ì¤€ìœ¼ë¡œ ë°›ì•„ ì •í™•íˆ ë¶„ë¦¬
        - HeaderëŠ” ë§¤ì¹˜ëœ ë¼ì¸ë§Œ í¬í•¨
        - BodyëŠ” header_end_posë¶€í„° ì‹œì‘
        
        ë¯¸ì†¡ë‹˜ ì›ì¹™:
        - ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ìë¦„
        - í‘œê°€ ìˆëŠ” ë¬¸ë‹¨ë§Œ table_rowsë¡œ ë³€í™˜
        - ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ paragraph
        - noteëŠ” ê°€ì¥ ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œë§Œ
        
        Args:
            content: ë³„í‘œ ì˜ì—­ í…ìŠ¤íŠ¸ (cleaned)
            annex_no: ë³„í‘œ ë²ˆí˜¸
            start_order: ì‹œì‘ ìˆœì„œ
            header_end_pos: Header ë ìœ„ì¹˜ (ì •ê·œì‹ match.end())
        
        Returns:
            ì„œë¸Œì²­í¬ ë¦¬ìŠ¤íŠ¸ [header, paragraphÃ—N, table_rows, note]
        """
        chunks = []
        order = start_order
        
        # âœ… Phase 0.9.5.1: Header/Body ì •í™•íˆ ë¶„ë¦¬
        header_text = content[:header_end_pos].strip()
        body_text = content[header_end_pos:].strip()
        
        # Step 2-1: Header ì²­í¬ (ì •ê·œì‹ ë§¤ì¹˜ ë¼ì¸ë§Œ)
        if header_text:
            # ê´€ë ¨ ì¡°ë¬¸, ê°œì •ì´ë ¥ ì¶”ê°€ ì¶”ì¶œ
            header_chunk = self._extract_header_chunk(header_text, annex_no, order)
            if header_chunk:
                chunks.append(header_chunk)
                order += 1
        
        # Step 3: ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ë¶„ë¦¬ (ë¹ˆ ì¤„ ê¸°ì¤€)
        paragraphs = self._split_into_paragraphs(body_text)
        
        logger.info(f"      ë¬¸ë‹¨ ë¶„ë¦¬: {len(paragraphs)}ê°œ")
        
        # Step 3-4: ê° ë¬¸ë‹¨ ì²˜ë¦¬
        for i, para_text in enumerate(paragraphs):
            is_last_paragraph = (i == len(paragraphs) - 1)
            
            # Step 4: ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œë§Œ note ì¶”ì¶œ
            if is_last_paragraph:
                note_chunk, remaining_text = self._extract_note_from_paragraph(
                    para_text, annex_no, order
                )
                
                if remaining_text and remaining_text.strip():
                    # Note ì œì™¸í•œ ë‚˜ë¨¸ì§€ê°€ ìˆìœ¼ë©´ paragraphë¡œ
                    para_chunk = self._create_paragraph_or_table(
                        remaining_text, annex_no, order, i
                    )
                    if para_chunk:
                        chunks.append(para_chunk)
                        order += 1
                
                if note_chunk:
                    chunks.append(note_chunk)
                    order += 1
            else:
                # ì¤‘ê°„ ë¬¸ë‹¨: paragraph ë˜ëŠ” table_rowsë¡œ ë³€í™˜
                para_chunk = self._create_paragraph_or_table(
                    para_text, annex_no, order, i
                )
                if para_chunk:
                    chunks.append(para_chunk)
                    order += 1
        
        return chunks
    
    def _extract_header_chunk(
        self,
        header_text: str,
        annex_no: str,
        order: int
    ) -> Optional[SubChunk]:
        """
        âœ… Phase 0.9.5.1: Header ì²­í¬ ìƒì„± (ì •ê·œì‹ ë§¤ì¹˜ ë¼ì¸ë§Œ)
        
        Args:
            header_text: Header ì˜ì—­ í…ìŠ¤íŠ¸ (ì •ê·œì‹ ë§¤ì¹˜ ëê¹Œì§€)
            annex_no: ë³„í‘œ ë²ˆí˜¸
            order: ìˆœì„œ
        
        Returns:
            Header ì²­í¬
        """
        # ì œëª© ì¶”ì¶œ
        match = re.search(self.patterns['annex_header'], header_text)
        if not match:
            return None
        
        annex_num = match.group(1)
        title = match.group(2).strip()
        
        # ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ (header_text ë‚´ì—ì„œë§Œ)
        related_article = ""
        article_match = re.search(self.patterns['related_article'], header_text)
        if article_match:
            related_article = article_match.group(1)
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ (header_text ë‚´ì—ì„œë§Œ)
        amendments = []
        for m in re.finditer(self.patterns['amendment'], header_text):
            amendments.append(m.group(1).strip())
        
        # âœ… Phase 0.9.5.1: header_text ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¤‘ë³µ ì—†ìŒ)
        
        return SubChunk(
            section_id=f"annex_{annex_no}_header",
            section_type="header",
            content=header_text,
            metadata={
                "annex_no": annex_num,
                "title": title,
                "related_article": related_article,
                "amendments": amendments
            },
            char_count=len(header_text),
            order=order
        )
    
    def _split_into_paragraphs(self, text: str) -> List[str]:
        """
        Step 3: ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬ (ë¹ˆ ì¤„ ê¸°ì¤€)
        
        Args:
            text: ë³¸ë¬¸ í…ìŠ¤íŠ¸
        
        Returns:
            ë¬¸ë‹¨ ë¦¬ìŠ¤íŠ¸
        """
        # ë¹ˆ ì¤„(2ê°œ ì´ìƒ ê°œí–‰)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        paragraphs = re.split(r'\n\s*\n', text)
        
        # ë¹ˆ ë¬¸ë‹¨ ì œê±° + strip
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        return paragraphs
    
    def _create_paragraph_or_table(
        self,
        para_text: str,
        annex_no: str,
        order: int,
        para_index: int
    ) -> Optional[SubChunk]:
        """
        Step 3: ë¬¸ë‹¨ì„ paragraph ë˜ëŠ” table_rowsë¡œ ë³€í™˜
        
        ë¯¸ì†¡ë‹˜ ì›ì¹™:
        - í‘œ íŒ¨í„´ì´ ìˆìœ¼ë©´ table_rows
        - ì—†ìœ¼ë©´ paragraph
        - ë‘˜ ë‹¤ ë³´ì¡´ (ì†ì‹¤ ì—†ìŒ)
        
        Args:
            para_text: ë¬¸ë‹¨ í…ìŠ¤íŠ¸
            annex_no: ë³„í‘œ ë²ˆí˜¸
            order: ìˆœì„œ
            para_index: ë¬¸ë‹¨ ì¸ë±ìŠ¤
        
        Returns:
            SubChunk (paragraph ë˜ëŠ” table_rows)
        """
        if not para_text or len(para_text.strip()) < 10:
            return None
        
        # í‘œ íŒì •: ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ì´ 3ê°œ ì´ìƒ
        lines = para_text.split('\n')
        digit_lines = [l for l in lines if re.match(self.patterns['table_start'], l)]
        
        has_table = len(digit_lines) >= 3
        
        if has_table:
            # table_rows ì²­í¬
            return SubChunk(
                section_id=f"annex_{annex_no}_table_{para_index+1}",
                section_type="table_rows",
                content=para_text.strip(),
                metadata={
                    "para_index": para_index,
                    "row_count_estimate": len(digit_lines),
                    "has_table": True
                },
                char_count=len(para_text.strip()),
                order=order
            )
        else:
            # paragraph ì²­í¬ (ì‹ ê·œ!)
            return SubChunk(
                section_id=f"annex_{annex_no}_paragraph_{para_index+1}",
                section_type="paragraph",
                content=para_text.strip(),
                metadata={
                    "para_index": para_index,
                    "has_table": False
                },
                char_count=len(para_text.strip()),
                order=order
            )
    
    def _extract_note_from_paragraph(
        self,
        para_text: str,
        annex_no: str,
        order: int
    ) -> tuple[Optional[SubChunk], str]:
        """
        Step 4: ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œë§Œ note ì¶”ì¶œ
        
        Args:
            para_text: ë¬¸ë‹¨ í…ìŠ¤íŠ¸
            annex_no: ë³„í‘œ ë²ˆí˜¸
            order: ìˆœì„œ
        
        Returns:
            (note_chunk, remaining_text)
            - note_chunk: Note ì²­í¬ (ì—†ìœ¼ë©´ None)
            - remaining_text: Note ì œì™¸í•œ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸
        """
        lines = para_text.split('\n')
        
        # * ì‹œì‘í•˜ëŠ” ì¤„ ì°¾ê¸°
        note_start_idx = -1
        for i, line in enumerate(lines):
            if re.match(self.patterns['note_marker'], line):
                note_start_idx = i
                break
        
        if note_start_idx == -1:
            # Note ì—†ìŒ
            return None, para_text
        
        # Note ë¼ì¸ë“¤
        note_lines = lines[note_start_idx:]
        note_content = '\n'.join(note_lines).strip()
        
        # Note ì œì™¸í•œ ë‚˜ë¨¸ì§€
        remaining_lines = lines[:note_start_idx]
        remaining_text = '\n'.join(remaining_lines).strip()
        
        # Note ì²­í¬ ìƒì„±
        note_chunk = SubChunk(
            section_id=f"annex_{annex_no}_notes",
            section_type="note",
            content=note_content,
            metadata={
                "note_count": len(note_lines)
            },
            char_count=len(note_content),
            order=order
        )
        
        return note_chunk, remaining_text
    
    def _clean_annex_text(self, text: str) -> str:
        """
        Annex í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°
        
        âœ… Phase 0.9.5: ë³„í‘œ ë¶„ë¦¬ í›„ ì‹¤í–‰ (ë³€ê²½ ì—†ìŒ)
        """
        # 1. Private Use Area (U+F000 ~ U+F8FF) ì œê±°
        text = re.sub(r'[\uF000-\uF8FF]', '', text)
        
        # 2. Box drawing characters ì œê±°
        box_chars = 'â”€â”â”‚â”ƒâ”Œâ”â””â”˜â”œâ”¤â”¬â”´â”¼â•‹â•â•‘â•”â•—â•šâ•â• â•£â•¦â•©â•¬â– â–¡â–ªâ–«'
        for char in box_chars:
            text = text.replace(char, '')
        
        # 3. ê¸°íƒ€ íŠ¹ìˆ˜ ê³µë°± ë¬¸ì ì •ë¦¬
        text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)
        
        # 4. ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 5. ì—°ì† ê°œí–‰ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 6. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        return text.strip()
    
    def _validate_annex_loss(
        self,
        annex_sections: List[Dict],
        chunks: List[SubChunk],
        canonical_text: str  # âœ… Phase 0.9.5.1: Cleaned text ê¸°ì¤€
    ):
        """
        Step 5: Annex Loss Check (ìµœì¢… ë‹¨ê³„ë§Œ!)
        
        âœ… Phase 0.9.5.1 ë³€ê²½ì‚¬í•­:
        - original_text ëŒ€ì‹  canonical_text (ì •ì œ í›„) ê¸°ì¤€
        - SubChunkerê°€ ì •ì œí•œ í…ìŠ¤íŠ¸ì™€ ë™ì¼ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
        - í—ˆìš© ì˜¤ì°¨: Â±3% ì´í•˜
        
        Args:
            annex_sections: ë³„í‘œ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
            chunks: ìµœì¢… ì²­í¬ ë¦¬ìŠ¤íŠ¸
            canonical_text: ì •ì œëœ Annex í…ìŠ¤íŠ¸ (ê¸°ì¤€)
        """
        logger.info("ğŸ” Step 5: Annex Loss Check")
        
        # âœ… Phase 0.9.5.1: Canonical text ê¸¸ì´ (ë™ì¼ ì •ì œ ê¸°ì¤€)
        canonical_len = len(canonical_text)
        
        # ì²­í¬ í•©ê³„ ê¸¸ì´
        chunk_total_len = sum(c.char_count for c in chunks)
        
        # ì†ì‹¤ë¥  ê³„ì‚°
        loss_rate = abs(canonical_len - chunk_total_len) / canonical_len if canonical_len > 0 else 0
        loss_pct = loss_rate * 100
        
        logger.info(f"   Canonical í…ìŠ¤íŠ¸: {canonical_len:,}ì")
        logger.info(f"   ì²­í¬ í•©ê³„: {chunk_total_len:,}ì")
        logger.info(f"   ì†ì‹¤ë¥ : {loss_pct:.1f}%")
        
        # í—ˆìš© ì˜¤ì°¨: Â±3%
        if loss_rate > 0.03:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì†ì‹¤ {loss_pct:.1f}% - í—ˆìš©ì¹˜(3%) ì´ˆê³¼!")
        else:
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ì†ì‹¤ í—ˆìš© ë²”ìœ„ ë‚´ ({loss_pct:.1f}% < 3%)")
        
        # ë³„í‘œë³„ ê²€ì¦ (per-annex check)
        for annex_sec in annex_sections:
            annex_no = annex_sec['annex_no']
            
            # âœ… Phase 0.9.5.1: ì •ì œëœ ê¸¸ì´ë¡œ ê³„ì‚°
            annex_raw = annex_sec['content']
            annex_cleaned = self._clean_annex_text(annex_raw)
            annex_len = len(annex_cleaned)
            
            # í•´ë‹¹ ë³„í‘œ ì²­í¬ë“¤
            annex_chunks = [
                c for c in chunks
                if c.section_id.startswith(f"annex_{annex_no}")
            ]
            
            annex_chunk_len = sum(c.char_count for c in annex_chunks)
            annex_loss = abs(annex_len - annex_chunk_len) / annex_len if annex_len > 0 else 0
            
            logger.info(
                f"   ë³„í‘œ{annex_no}: {annex_len}ì â†’ {annex_chunk_len}ì "
                f"({len(annex_chunks)}ê°œ ì²­í¬, ì†ì‹¤ {annex_loss*100:.1f}%)"
            )
    
    def _fallback_chunk(self, text: str, annex_no: str) -> List[SubChunk]:
        """
        Fallback: ë³„í‘œ íŒ¨í„´ ì—†ì„ ë•Œ
        """
        logger.warning("   âš ï¸ Fallback ëª¨ë“œ: ì „ì²´ë¥¼ ë‹¨ì¼ ì²­í¬ë¡œ ì²˜ë¦¬")
        
        cleaned = self._clean_annex_text(text)
        
        return [SubChunk(
            section_id=f"annex_{annex_no}_fallback",
            section_type="paragraph",  # Phase 0.9.5: paragraphë¡œ ë³€ê²½
            content=cleaned,
            metadata={"fallback": True},
            char_count=len(cleaned),
            order=0
        )]
    
    def _log_chunk_types(self, chunks: List[SubChunk]):
        """ì²­í¬ íƒ€ì…ë³„ í†µê³„ ë¡œê¹…"""
        type_counts = {}
        for chunk in chunks:
            chunk_type = chunk.section_type
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        logger.info("ğŸ“Š ì²­í¬ íƒ€ì… ë¶„í¬:")
        for chunk_type, count in sorted(type_counts.items()):
            logger.info(f"   - {chunk_type}: {count}ê°œ")


def validate_subchunks(chunks: List[SubChunk], original_length: int) -> dict:
    """
    ì„œë¸Œì²­í‚¹ ê²°ê³¼ ê²€ì¦
    
    Phase 0.9.5: paragraph íƒ€ì… ì¶”ê°€
    """
    if not chunks:
        return {
            'is_valid': False,
            'reason': 'ì²­í¬ ì—†ìŒ',
            'chunk_count': 0,
            'loss_rate': 1.0
        }
    
    # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
    type_counts = {}
    for chunk in chunks:
        chunk_type = chunk.section_type
        type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
    
    # ì´ ë¬¸ì ìˆ˜
    total_chars = sum(c.char_count for c in chunks)
    
    # ì†ì‹¤ë¥ 
    loss_rate = abs(original_length - total_chars) / original_length if original_length > 0 else 0
    
    # ê²€ì¦ ê¸°ì¤€
    has_header = 'header' in type_counts
    has_content = 'table_rows' in type_counts or 'paragraph' in type_counts
    
    is_valid = has_header and has_content and loss_rate < 0.05
    
    return {
        'is_valid': is_valid,
        'chunk_count': len(chunks),
        'type_counts': type_counts,
        'total_chars': total_chars,
        'loss_rate': loss_rate,
        'has_header': has_header,
        'has_content': has_content
    }