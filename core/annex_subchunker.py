"""
core/annex_subchunker.py - PRISM Phase 0.8.7 Polishing
Annex ì„œë¸Œì²­í‚¹ ì—”ì§„ (ë…¸ì´ì¦ˆ ì œê±° ê°•í™”)

Phase 0.8.7 í•µì‹¬ ìˆ˜ì •:
- âœ… ì…ë ¥ í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° (â–¡â–  ë“±)
- âœ… table_rowsì—ì„œ ì“°ë ˆê¸° ë¬¸ì ì™„ì „ ì œê±°

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + ì´ì„œì˜ (Backend Lead)
Date: 2025-11-20
Version: Phase 0.8.7 Polishing
"""

import re
import logging
from dataclasses import dataclass
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)


@dataclass
class SubChunk:
    """ì„œë¸Œì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    section_id: str
    section_type: str  # 'header' | 'table_rows' | 'note' | 'exception'
    content: str
    metadata: Dict
    char_count: int
    order: int


class AnnexSubChunker:
    """
    Annex ì„œë¸Œì²­í‚¹ ì—”ì§„
    
    Phase 0.8.7 ìˆ˜ì •:
    - ì…ë ¥ í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° ê°•í™”
    - ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬
    """
    
    # âœ… Phase 0.8.7: ë…¸ì´ì¦ˆ ë¬¸ì íŒ¨í„´
    NOISE_CHARS = ''.join([
        'â–¡', 'â– ', 'â—‹', 'â—', 'â—‡', 'â—†',  # ë°•ìŠ¤/ë„í˜•
        'â”', 'â”€', 'â•', 'â”ƒ', 'â”‚', 'â”Œ', 'â”', 'â””', 'â”˜', 'â”œ', 'â”¤', 'â”¬', 'â”´', 'â”¼',  # ë¼ì¸/í…Œì´ë¸”
        '', '', '',  # Private Use Area
        'â–ª', 'â–«', 'â—¦', 'â€¢',  # ë¶ˆë¦¿
    ])
    NOISE_PATTERN = re.compile(f'[{re.escape(NOISE_CHARS)}]')
    
    def __init__(self):
        self.patterns = {
            'header': r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n]+)',
            'related_article': r'<(ì œ\d+ì¡°.*?)ê´€ë ¨>',
            'amendment': r'\(ê°œì •([0-9.,\s]+)\)',
            'note_marker': r'^\*',
            'table_separator': r'^ì„ìš©í•˜ê³ ìí•˜ëŠ”ì¸ì›ìˆ˜ì—ëŒ€í•œìŠ¹ì§„í›„ë³´ìë²”ìœ„\((\d+ê¸‰.*?)\)'
        }
        logger.info("âœ… AnnexSubChunker ì´ˆê¸°í™” (Phase 0.8.7 Polishing)")
    
    def _clean_annex_text(self, text: str) -> str:
        """
        âœ… Phase 0.8.7: Annex í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°
        
        1. ë°•ìŠ¤/ë¼ì¸ ë¬¸ì ì œê±°
        2. ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬
        3. ì—°ì† ê³µë°± ì •ë¦¬
        """
        if not text:
            return text
        
        # 1. ë…¸ì´ì¦ˆ ë¬¸ì ì œê±°
        result = self.NOISE_PATTERN.sub('', text)
        
        # 2. ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
        result = re.sub(r'\n{3,}', '\n\n', result)
        
        # 3. ì—°ì† ê³µë°± ì •ë¦¬
        result = re.sub(r' {2,}', ' ', result)
        
        # 4. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì •ë¦¬
        lines = [line.strip() for line in result.split('\n')]
        result = '\n'.join(lines)
        
        return result.strip()
    
    def chunk(self, annex_content: str, annex_no: str = "1") -> List[SubChunk]:
        """
        Annex í…ìŠ¤íŠ¸ â†’ ì„œë¸Œì²­í¬ ë¦¬ìŠ¤íŠ¸
        
        Args:
            annex_content: Annex ì›ë¬¸
            annex_no: ë³„í‘œ ë²ˆí˜¸
        
        Returns:
            ì„œë¸Œì²­í¬ ë¦¬ìŠ¤íŠ¸ (ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬)
        """
        if not annex_content or len(annex_content) < 50:
            logger.warning("âš ï¸ Annex ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ - ì„œë¸Œì²­í‚¹ ìŠ¤í‚µ")
            return []
        
        # âœ… Phase 0.8.7: ì…ë ¥ í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°
        cleaned_content = self._clean_annex_text(annex_content)
        
        logger.info(f"ğŸ“‹ Annex ì„œë¸Œì²­í‚¹ ì‹œì‘: {len(annex_content)}ì â†’ {len(cleaned_content)}ì (ë…¸ì´ì¦ˆ ì œê±°)")
        
        chunks = []
        order = 0
        
        # 1. Header ì²­í¬
        header_chunk = self._extract_header(cleaned_content, annex_no, order)
        if header_chunk:
            chunks.append(header_chunk)
            order += 1
        
        # 2. Table ì˜ì—­ ë¶„ë¦¬ (3ê¸‰ì œì™¸ / 3ê¸‰ìŠ¹ì§„ ë“±)
        table_chunks = self._extract_table_sections(cleaned_content, annex_no, order)
        chunks.extend(table_chunks)
        order += len(table_chunks)
        
        # 3. Note ì²­í¬
        note_chunks = self._extract_notes(cleaned_content, annex_no, order)
        chunks.extend(note_chunks)
        
        # 4. í…ìŠ¤íŠ¸ ì†ì‹¤ ê²€ì¦
        total_chars = sum(c.char_count for c in chunks)
        loss_rate = abs(total_chars - len(cleaned_content)) / len(cleaned_content) if len(cleaned_content) > 0 else 0
        
        if loss_rate > 0.05:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì†ì‹¤ {loss_rate:.1%} - ê¸°ì¤€ ì´ˆê³¼!")
        else:
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ì†ì‹¤ {loss_rate:.1%} (í—ˆìš© ë²”ìœ„)")
        
        logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        self._log_chunk_types(chunks)
        
        return chunks
    
    def _extract_header(
        self, 
        text: str, 
        annex_no: str, 
        order: int
    ) -> Optional[SubChunk]:
        """í—¤ë” ì²­í¬ ì¶”ì¶œ"""
        
        match = re.search(self.patterns['header'], text)
        if not match:
            return None
        
        annex_num = match.group(1)
        title = match.group(2).strip()
        
        # ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
        related_article = ""
        article_match = re.search(self.patterns['related_article'], text)
        if article_match:
            related_article = article_match.group(1)
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendments = []
        for m in re.finditer(self.patterns['amendment'], text):
            amendments.append(m.group(1).strip())
        
        # í—¤ë” ì˜ì—­ í…ìŠ¤íŠ¸ (ì²« 200ì ì •ë„)
        header_text = text[:min(200, len(text))]
        header_text = re.sub(r'\n+', '\n', header_text)
        
        return SubChunk(
            section_id=f"annex_{annex_no}_header",
            section_type="header",
            content=header_text,
            metadata={
                "annex_no": annex_num,
                "title": title,
                "related_article": related_article,
                "amendments": amendments
            },
            char_count=len(header_text),
            order=order
        )
    
    def _extract_table_sections(
        self, 
        text: str, 
        annex_no: str, 
        start_order: int
    ) -> List[SubChunk]:
        """Table ì„¹ì…˜ ë¶„ë¦¬"""
        chunks = []
        order = start_order
        
        # í‘œ êµ¬ë¶„ íŒ¨í„´ ì°¾ê¸°
        separators = list(re.finditer(
            self.patterns['table_separator'], 
            text, 
            re.MULTILINE
        ))
        
        if not separators:
            # êµ¬ë¶„ì ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ
            table_text = self._extract_table_body(text)
            if table_text:
                # âœ… Phase 0.8.7: table_textë„ ë…¸ì´ì¦ˆ ì œê±°
                table_text = self._clean_annex_text(table_text)
                
                chunks.append(SubChunk(
                    section_id=f"annex_{annex_no}_table_main",
                    section_type="table_rows",
                    content=table_text,
                    metadata={
                        "table_title": "main",
                        "row_count_estimate": self._estimate_row_count(table_text)
                    },
                    char_count=len(table_text),
                    order=order
                ))
            return chunks
        
        # êµ¬ë¶„ì ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¦¬
        for i, sep_match in enumerate(separators):
            section_title = sep_match.group(1)
            
            # ì„¹ì…˜ ì‹œì‘/ë ìœ„ì¹˜
            start_pos = sep_match.start()
            end_pos = (separators[i+1].start() 
                      if i+1 < len(separators) 
                      else len(text))
            
            section_text = text[start_pos:end_pos]
            
            # Note ë¶„ë¦¬
            section_text_no_notes = re.sub(
                r'^\*.*$', 
                '', 
                section_text, 
                flags=re.MULTILINE
            )
            
            if len(section_text_no_notes.strip()) > 20:
                # âœ… Phase 0.8.7: ì„¹ì…˜ í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°
                cleaned_section = self._clean_annex_text(section_text_no_notes.strip())
                
                chunks.append(SubChunk(
                    section_id=f"annex_{annex_no}_table_{i+1}",
                    section_type="table_rows",
                    content=cleaned_section,
                    metadata={
                        "table_title": section_title,
                        "row_count_estimate": self._estimate_row_count(cleaned_section)
                    },
                    char_count=len(cleaned_section),
                    order=order
                ))
                order += 1
        
        return chunks
    
    def _extract_table_body(self, text: str) -> str:
        """í‘œ ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ (í—¤ë” ì œì™¸)"""
        
        lines = text.split('\n')
        table_start = 0
        
        for i, line in enumerate(lines):
            if re.match(r'^\d+\s', line):
                table_start = i
                break
        
        if table_start == 0:
            return ""
        
        # Note ì‹œì‘ ì „ê¹Œì§€
        table_lines = []
        for line in lines[table_start:]:
            if line.startswith('*'):
                break
            if line.strip():
                table_lines.append(line)
        
        return '\n'.join(table_lines)
    
    def _extract_notes(
        self, 
        text: str, 
        annex_no: str, 
        start_order: int
    ) -> List[SubChunk]:
        """Note ì²­í¬ ì¶”ì¶œ (* ì‹œì‘)"""
        
        chunks = []
        order = start_order
        
        # * ì‹œì‘í•˜ëŠ” ëª¨ë“  ì¤„
        note_lines = []
        for line in text.split('\n'):
            if line.strip().startswith('*'):
                note_lines.append(line.strip())
        
        # ê° Noteë¥¼ ê°œë³„ ì²­í¬ë¡œ
        for i, note in enumerate(note_lines):
            # âœ… Phase 0.8.7: ë…¸ì´ì¦ˆ ì œê±° ê°•í™”
            note_clean = self._clean_note(note)
            
            chunks.append(SubChunk(
                section_id=f"annex_{annex_no}_note_{i+1}",
                section_type="note",
                content=note_clean,
                metadata={
                    "note_type": self._classify_note(note_clean)
                },
                char_count=len(note_clean),
                order=order
            ))
            order += 1
        
        return chunks
    
    def _clean_note(self, note: str) -> str:
        """Note ë…¸ì´ì¦ˆ ì œê±°"""
        # ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ì ì œê±° (â”€, â•, â” ë“±)
        note = re.sub(r'[â”€â•â”]{2,}', '', note)
        
        # âœ… Phase 0.8.7: ë…¸ì´ì¦ˆ ë¬¸ì ì œê±°
        note = self.NOISE_PATTERN.sub('', note)
        
        # ì—°ì† ê³µë°± ì •ë¦¬
        note = re.sub(r'\s{2,}', ' ', note)
        
        return note.strip()
    
    def _classify_note(self, note: str) -> str:
        """Note ìœ í˜• ë¶„ë¥˜"""
        note_lower = note.lower()
        
        if 'ì˜ˆì™¸' in note or 'ë‹¨,' in note or 'ë‹¤ë§Œ' in note:
            return "exception"
        elif 'í¬í•¨' in note or 'ë²”ìœ„' in note:
            return "rule"
        else:
            return "general"
    
    def _estimate_row_count(self, text: str) -> int:
        """Row ê°œìˆ˜ ì¶”ì •"""
        lines = text.split('\n')
        count = sum(1 for line in lines if re.match(r'^\d+\s', line))
        return count
    
    def _log_chunk_types(self, chunks: List[SubChunk]):
        """ì²­í¬ íƒ€ì… í†µê³„ ë¡œê¹…"""
        type_counts = {}
        for chunk in chunks:
            type_counts[chunk.section_type] = type_counts.get(chunk.section_type, 0) + 1
        
        logger.info(f"   ğŸ“Š ì²­í¬ íƒ€ì… ë¶„í¬:")
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"      - {ctype}: {count}ê°œ")


# ============================================
# ìœ í‹¸ë¦¬í‹°
# ============================================

def validate_subchunks(chunks: List[SubChunk], original_length: int) -> Dict:
    """ì„œë¸Œì²­í¬ ê²€ì¦"""
    total_chars = sum(c.char_count for c in chunks)
    loss_rate = abs(total_chars - original_length) / original_length if original_length > 0 else 0
    
    type_counts = {}
    for chunk in chunks:
        type_counts[chunk.section_type] = type_counts.get(chunk.section_type, 0) + 1
    
    has_header = 'header' in type_counts
    has_table = 'table_rows' in type_counts
    has_note = 'note' in type_counts
    has_multiple_types = len(type_counts) >= 2
    
    is_valid = (
        loss_rate < 0.10 and  # Phase 0.8.7: ë…¸ì´ì¦ˆ ì œê±°ë¡œ ì¸í•´ 10%ê¹Œì§€ í—ˆìš©
        has_multiple_types and
        len(chunks) >= 3
    )
    
    return {
        'is_valid': is_valid,
        'loss_rate': loss_rate,
        'chunk_count': len(chunks),
        'type_counts': type_counts,
        'has_header': has_header,
        'has_table': has_table,
        'has_note': has_note
    }


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    
    # ê°„ë‹¨ í…ŒìŠ¤íŠ¸
    sample = """
[ë³„í‘œ 1] ìŠ¹ì§„í›„ë³´ìë²”ìœ„(3ê¸‰ìŠ¹ì§„ì œì™¸)
<ì œ20ì¡°ì œ2í•­ê´€ë ¨>(ê°œì •2003.3.29)
â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡
ì„ìš©í•˜ê³ ìí•˜ëŠ”ì¸ì›ìˆ˜ì—ëŒ€í•œìŠ¹ì§„í›„ë³´ìë²”ìœ„(3ê¸‰ìŠ¹ì§„ì œì™¸)
1 5ë²ˆê¹Œì§€
2 10ë²ˆê¹Œì§€
*ì„ìš©í•˜ê³ ìí•˜ëŠ”ì¸ì›ìˆ˜ê°€5ëª…ê¹Œì§€ëŠ”ì„œì—´ëª…ë¶€ìˆœìœ„ì˜5ë°°ìˆ˜
"""
    
    chunker = AnnexSubChunker()
    chunks = chunker.chunk(sample)
    
    print(f"\nâœ… ì²­í¬ {len(chunks)}ê°œ ìƒì„±")
    for c in chunks:
        print(f"   - {c.section_type}: {c.char_count}ì")