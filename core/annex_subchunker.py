# annex_subchunker.py - Phase 0.9.2
#
# Phase 0.9.2 ìˆ˜ì •ì‚¬í•­:
# - âœ… Private Use Area ë¬¸ì ì œê±° ê°•í™” (U+F000~U+F8FF)
# - âœ… Box drawing ë¬¸ì ì œê±° ê°•í™”

"""
annex_subchunker.py - PRISM Annex SubChunker

Annex ì„¹ì…˜ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-20  
Version: Phase 0.9.2
"""

import re
import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class SubChunk:
    """ì„œë¸Œì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    section_id: str
    section_type: str
    content: str
    metadata: Dict[str, Any]
    char_count: int
    order: int


class AnnexSubChunker:
    """
    Annex ì„œë¸Œì²­í‚¹
    
    Phase 0.9.2:
    - âœ… ë…¸ì´ì¦ˆ ë¬¸ì ì œê±° ê°•í™”
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        
        self.patterns = {
            'header': r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)',
            'related_article': r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>',
            'amendment': r'\[(.*?(\d{4}\.\d{1,2}\.\d{1,2}).*?)\]',
            'table_separator': r'^([â‘ -â‘³â– â–ªâ—â—†â—‡]?\s*[ê°€-í£\s]+(?:ëª…ì¹­|ê¸‰ë³„|ë²”ìœ„|ëŒ€ìƒ|êµ¬ë¶„))',
            'note': r'^\*\s*(.+)$'
        }
        
        logger.info("âœ… AnnexSubChunker v0.9.2 ì´ˆê¸°í™” (Noise Removal Enhanced)")
    
    def chunk(self, annex_text: str, annex_no: str = "1") -> List[SubChunk]:
        """
        Annex í…ìŠ¤íŠ¸ â†’ ì„œë¸Œì²­í¬ ë³€í™˜
        """
        logger.info(f"ğŸ”§ Annex ì„œë¸Œì²­í‚¹ ì‹œì‘: {len(annex_text)}ì")
        
        # Phase 0.9.2: ë…¸ì´ì¦ˆ ì œê±° ê°•í™”
        cleaned_content = self._clean_annex_text(annex_text)
        
        chunks = []
        order = 0
        
        # 1. Header ì²­í¬
        header_chunk = self._extract_header(cleaned_content, annex_no, order)
        if header_chunk:
            chunks.append(header_chunk)
            order += 1
        
        # 2. Table ì˜ì—­ ë¶„ë¦¬ (3ê¸‰ì œì™¸ / 3ê¸‰ìŠ¹ì§„ ë“±)
        table_chunks = self._extract_table_sections(cleaned_content, annex_no, order)
        chunks.extend(table_chunks)
        order += len(table_chunks)
        
        # 3. Note ì²­í¬
        note_chunks = self._extract_notes(cleaned_content, annex_no, order)
        chunks.extend(note_chunks)
        
        # 4. í…ìŠ¤íŠ¸ ì†ì‹¤ ê²€ì¦
        total_chars = sum(c.char_count for c in chunks)
        loss_rate = abs(total_chars - len(cleaned_content)) / len(cleaned_content) if len(cleaned_content) > 0 else 0
        
        if loss_rate > 0.05:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì†ì‹¤ {loss_rate:.1%} - ê¸°ì¤€ ì´ˆê³¼!")
        else:
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ì†ì‹¤ {loss_rate:.1%} (í—ˆìš© ë²”ìœ„)")
        
        logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        self._log_chunk_types(chunks)
        
        return chunks
    
    def _clean_annex_text(self, text: str) -> str:
        """
        Annex í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° (Phase 0.9.2 ê°•í™”)
        
        âœ… Private Use Area ë¬¸ì ì œê±°
        âœ… Box drawing ë¬¸ì ì œê±°
        """
        
        # 1. Private Use Area (U+F000 ~ U+F8FF) ì œê±°
        text = re.sub(r'[\uF000-\uF8FF]', '', text)
        
        # 2. Box drawing characters ì œê±°
        box_chars = 'â”€â”â”‚â”ƒâ”Œâ”â””â”˜â”œâ”¤â”¬â”´â”¼â•‹â•â•‘â•”â•—â•šâ•â• â•£â•¦â•©â•¬â– â–¡â–ªâ–«'
        for char in box_chars:
            text = text.replace(char, '')
        
        # 3. ê¸°íƒ€ íŠ¹ìˆ˜ ê³µë°± ë¬¸ì ì •ë¦¬
        text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)  # Zero-width spaces
        
        # 4. ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 5. ì—°ì† ê°œí–‰ ì •ë¦¬ (3ì¤„ ì´ìƒ â†’ 2ì¤„)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 6. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = []
        for line in text.split('\n'):
            cleaned_line = line.strip()
            lines.append(cleaned_line)
        
        text = '\n'.join(lines)
        
        return text.strip()
    
    def _extract_header(
        self, 
        text: str, 
        annex_no: str, 
        order: int
    ) -> Optional[SubChunk]:
        """í—¤ë” ì²­í¬ ì¶”ì¶œ"""
        
        match = re.search(self.patterns['header'], text)
        if not match:
            return None
        
        annex_num = match.group(1)
        title = match.group(2).strip()
        
        # ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
        related_article = ""
        article_match = re.search(self.patterns['related_article'], text)
        if article_match:
            related_article = article_match.group(1)
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendments = []
        for m in re.finditer(self.patterns['amendment'], text):
            amendments.append(m.group(1).strip())
        
        # í—¤ë” ì˜ì—­ í…ìŠ¤íŠ¸ (ì²« 200ì ì •ë„)
        header_text = text[:min(200, len(text))]
        header_text = re.sub(r'\n+', '\n', header_text)
        
        return SubChunk(
            section_id=f"annex_{annex_no}_header",
            section_type="header",
            content=header_text,
            metadata={
                "annex_no": annex_num,
                "title": title,
                "related_article": related_article,
                "amendments": amendments
            },
            char_count=len(header_text),
            order=order
        )
    
    def _extract_table_sections(
        self, 
        text: str, 
        annex_no: str, 
        start_order: int
    ) -> List[SubChunk]:
        """Table ì„¹ì…˜ ë¶„ë¦¬"""
        chunks = []
        order = start_order
        
        # í‘œ êµ¬ë¶„ íŒ¨í„´ ì°¾ê¸°
        separators = list(re.finditer(
            self.patterns['table_separator'], 
            text, 
            re.MULTILINE
        ))
        
        if not separators:
            # êµ¬ë¶„ì ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ
            table_text = self._extract_table_body(text)
            if table_text:
                # Phase 0.9.2: ë…¸ì´ì¦ˆ ì œê±°
                table_text = self._clean_annex_text(table_text)
                
                chunks.append(SubChunk(
                    section_id=f"annex_{annex_no}_table_main",
                    section_type="table_rows",
                    content=table_text,
                    metadata={
                        "table_title": "main",
                        "row_count_estimate": self._estimate_row_count(table_text)
                    },
                    char_count=len(table_text),
                    order=order
                ))
            return chunks
        
        # êµ¬ë¶„ì ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¦¬
        for i, sep_match in enumerate(separators):
            section_title = sep_match.group(1)
            
            # ì„¹ì…˜ ì‹œì‘/ë ìœ„ì¹˜
            start_pos = sep_match.start()
            end_pos = (separators[i+1].start() 
                      if i+1 < len(separators) 
                      else len(text))
            
            section_text = text[start_pos:end_pos]
            
            # Note ë¶„ë¦¬
            section_text_no_notes = re.sub(
                r'^\*.*$', 
                '', 
                section_text, 
                flags=re.MULTILINE
            )
            
            if len(section_text_no_notes.strip()) > 20:
                # Phase 0.9.2: ì„¹ì…˜ í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°
                cleaned_section = self._clean_annex_text(section_text_no_notes.strip())
                
                chunks.append(SubChunk(
                    section_id=f"annex_{annex_no}_table_{i+1}",
                    section_type="table_rows",
                    content=cleaned_section,
                    metadata={
                        "table_title": section_title,
                        "row_count_estimate": self._estimate_row_count(cleaned_section)
                    },
                    char_count=len(cleaned_section),
                    order=order
                ))
                order += 1
        
        return chunks
    
    def _extract_table_body(self, text: str) -> str:
        """í‘œ ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ (í—¤ë” ì œì™¸)"""
        
        lines = text.split('\n')
        table_start = 0
        
        for i, line in enumerate(lines):
            if re.match(r'^\d+\s', line):
                table_start = i
                break
        
        if table_start == 0:
            return ""
        
        # Note ì‹œì‘ ì „ê¹Œì§€
        table_lines = []
        for line in lines[table_start:]:
            if line.startswith('*'):
                break
            if line.strip():
                table_lines.append(line)
        
        return '\n'.join(table_lines)
    
    def _extract_notes(
        self, 
        text: str, 
        annex_no: str, 
        start_order: int
    ) -> List[SubChunk]:
        """Note ì²­í¬ ì¶”ì¶œ (* ì‹œì‘)"""
        
        chunks = []
        order = start_order
        
        # * ì‹œì‘ ë¼ì¸ë“¤ ìˆ˜ì§‘
        note_lines = []
        for line in text.split('\n'):
            if line.strip().startswith('*'):
                note_lines.append(line.strip())
        
        if note_lines:
            note_content = '\n'.join(note_lines)
            
            chunks.append(SubChunk(
                section_id=f"annex_{annex_no}_notes",
                section_type="note",
                content=note_content,
                metadata={
                    "note_count": len(note_lines)
                },
                char_count=len(note_content),
                order=order
            ))
        
        return chunks
    
    def _estimate_row_count(self, text: str) -> int:
        """í–‰ ê°œìˆ˜ ì¶”ì •"""
        
        lines = [line for line in text.split('\n') if line.strip()]
        
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ê°œìˆ˜
        numbered_lines = [
            line for line in lines 
            if re.match(r'^\d+\s', line.strip())
        ]
        
        return len(numbered_lines)
    
    def _log_chunk_types(self, chunks: List[SubChunk]):
        """ì²­í¬ íƒ€ì…ë³„ í†µê³„"""
        
        type_counts = {}
        for chunk in chunks:
            ctype = chunk.section_type
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")


def validate_subchunks(chunks: List[SubChunk], original_len: int) -> Dict[str, Any]:
    """ì„œë¸Œì²­í¬ ê²€ì¦"""
    
    total_chars = sum(c.char_count for c in chunks)
    loss_rate = abs(total_chars - original_len) / original_len if original_len > 0 else 0
    
    is_valid = loss_rate <= 0.05
    
    return {
        'is_valid': is_valid,
        'chunk_count': len(chunks),
        'total_chars': total_chars,
        'original_len': original_len,
        'loss_rate': loss_rate
    }