"""
core/annex_subchunker.py - Phase 0.9.3 Critical Hotfix

Phase 0.9.3 ìˆ˜ì •ì‚¬í•­:
- âœ… ë³„í‘œ ë¶„ë¦¬ ë¡œì§ ì¶”ê°€ (re.finditer â†’ ëª¨ë“  ë³„í‘œ ì°¾ê¸°)
- âœ… ê° ë³„í‘œë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì„œë¸Œì²­í‚¹
- âœ… ë…¸ì´ì¦ˆ ì œê±° ìˆœì„œ ê°œì„  (ë³„í‘œ ì¸ì‹ ë¨¼ì €)
- âœ… QA ê°•í™” (ë³„í‘œ ê°œìˆ˜ ê²€ì¦)

ê·¼ë³¸ ì›ì¸ í•´ê²°:
- BEFORE: ë‹¨ì¼ ë³„í‘œ ê°€ì • (ì²« ë²ˆì§¸ë§Œ ì²˜ë¦¬)
- AFTER:  ë‹¤ì¤‘ ë³„í‘œ ì§€ì› (Nê°œ ëª¨ë‘ ì²˜ë¦¬)

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-21
Version: Phase 0.9.3
"""

import re
import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class SubChunk:
    """ì„œë¸Œì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    section_id: str
    section_type: str
    content: str
    metadata: Dict[str, Any]
    char_count: int
    order: int


class AnnexSubChunker:
    """
    Annex ì„œë¸Œì²­í‚¹ (Phase 0.9.3 Critical Hotfix)
    
    ë‹¤ì¤‘ ë³„í‘œ ì§€ì›:
    - âœ… [ë³„í‘œ1], [ë³„í‘œ2], [ë³„í‘œ3] ... ëª¨ë‘ ì²˜ë¦¬
    - âœ… ê° ë³„í‘œë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²­í‚¹
    - âœ… ë³„í‘œ ê°œìˆ˜ ê²€ì¦
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        
        self.patterns = {
            'annex_header': r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)',
            'related_article': r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>',
            'amendment': r'\[(.*?(\d{4}\.\d{1,2}\.\d{1,2}).*?)\]',
            'table_separator': r'^([â‘ -â‘³â– â–ªâ—â—†â—‡]?\s*[ê°€-í£\s]+(?:ëª…ì¹­|ê¸‰ë³„|ë²”ìœ„|ëŒ€ìƒ|êµ¬ë¶„))',
            'note': r'^\*\s*(.+)$'
        }
        
        logger.info("âœ… AnnexSubChunker v0.9.3 ì´ˆê¸°í™” (Multi-Annex Support)")
    
    def chunk(self, annex_text: str, annex_no: str = "1") -> List[SubChunk]:
        """
        Annex í…ìŠ¤íŠ¸ â†’ ì„œë¸Œì²­í¬ ë³€í™˜ (Phase 0.9.3)
        
        âœ… ë‹¤ì¤‘ ë³„í‘œ ì§€ì›:
        1. ëª¨ë“  [ë³„í‘œN] ìœ„ì¹˜ ì°¾ê¸°
        2. ê° ë³„í‘œë¥¼ ë…ë¦½ ì˜ì—­ìœ¼ë¡œ ë¶„ë¦¬
        3. ê° ì˜ì—­ë§ˆë‹¤ ì„œë¸Œì²­í‚¹ ìˆ˜í–‰
        """
        logger.info(f"ğŸ”§ Annex ì„œë¸Œì²­í‚¹ ì‹œì‘: {len(annex_text)}ì")
        
        # Phase 0.9.3: ë³„í‘œ ë¶„ë¦¬
        annex_sections = self._split_by_annex(annex_text)
        
        if not annex_sections:
            logger.warning("âš ï¸ ë³„í‘œ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - fallback ì²˜ë¦¬")
            return self._fallback_chunk(annex_text, annex_no)
        
        logger.info(f"âœ… ë³„í‘œ ë¶„ë¦¬ ì™„ë£Œ: {len(annex_sections)}ê°œ")
        
        # ê° ë³„í‘œë§ˆë‹¤ ì²­í‚¹
        all_chunks = []
        global_order = 0
        
        for annex_sec in annex_sections:
            annex_num = annex_sec['annex_no']
            annex_content = annex_sec['content']
            
            logger.info(f"   ğŸ”¹ ë³„í‘œ{annex_num} ì²˜ë¦¬ ì¤‘... ({len(annex_content)}ì)")
            
            # ë…¸ì´ì¦ˆ ì œê±° (ë³„í‘œ ë¶„ë¦¬ í›„)
            cleaned_content = self._clean_annex_text(annex_content)
            
            # ì„œë¸Œì²­í‚¹
            section_chunks = self._process_single_annex(
                cleaned_content,
                annex_num,
                global_order
            )
            
            all_chunks.extend(section_chunks)
            global_order += len(section_chunks)
            
            logger.info(f"   âœ… ë³„í‘œ{annex_num}: {len(section_chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # Phase 0.9.3: QA ê²€ì¦
        self._validate_multi_annex(annex_sections, all_chunks, annex_text)
        
        logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì™„ë£Œ: ì´ {len(all_chunks)}ê°œ")
        self._log_chunk_types(all_chunks)
        
        return all_chunks
    
    def _split_by_annex(self, text: str) -> List[Dict[str, Any]]:
        """
        Phase 0.9.3: ë³„í‘œë³„ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬
        
        Returns:
            [
                {
                    'annex_no': '1',
                    'title': '...',
                    'content': '...',
                    'start_pos': 0,
                    'end_pos': 100
                },
                ...
            ]
        """
        # ëª¨ë“  [ë³„í‘œN] ìœ„ì¹˜ ì°¾ê¸°
        matches = list(re.finditer(self.patterns['annex_header'], text))
        
        if not matches:
            return []
        
        sections = []
        
        for i, match in enumerate(matches):
            annex_no = match.group(1)
            annex_title = match.group(2).strip()
            start_pos = match.start()
            
            # ë‹¤ìŒ ë³„í‘œê¹Œì§€ or ëê¹Œì§€
            if i + 1 < len(matches):
                end_pos = matches[i + 1].start()
            else:
                end_pos = len(text)
            
            content = text[start_pos:end_pos]
            
            sections.append({
                'annex_no': annex_no,
                'title': annex_title,
                'content': content,
                'start_pos': start_pos,
                'end_pos': end_pos
            })
        
        return sections
    
    def _process_single_annex(
        self,
        content: str,
        annex_no: str,
        start_order: int
    ) -> List[SubChunk]:
        """
        ë‹¨ì¼ ë³„í‘œ ì²˜ë¦¬ (Phase 0.9.3)
        
        Args:
            content: ë³„í‘œ ì˜ì—­ í…ìŠ¤íŠ¸
            annex_no: ë³„í‘œ ë²ˆí˜¸
            start_order: ì‹œì‘ ìˆœì„œ
        
        Returns:
            ì„œë¸Œì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        order = start_order
        
        # 1. Header ì²­í¬
        header_chunk = self._extract_header_from_content(content, annex_no, order)
        if header_chunk:
            chunks.append(header_chunk)
            order += 1
        
        # 2. Table ì˜ì—­ ë¶„ë¦¬
        table_chunks = self._extract_table_sections(content, annex_no, order)
        chunks.extend(table_chunks)
        order += len(table_chunks)
        
        # 3. Note ì²­í¬
        note_chunks = self._extract_notes(content, annex_no, order)
        chunks.extend(note_chunks)
        
        return chunks
    
    def _extract_header_from_content(
        self,
        content: str,
        annex_no: str,
        order: int
    ) -> Optional[SubChunk]:
        """ë³„í‘œ í—¤ë” ì²­í¬ ìƒì„±"""
        
        match = re.search(self.patterns['annex_header'], content)
        if not match:
            return None
        
        annex_num = match.group(1)
        title = match.group(2).strip()
        
        # ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
        related_article = ""
        article_match = re.search(self.patterns['related_article'], content)
        if article_match:
            related_article = article_match.group(1)
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendments = []
        for m in re.finditer(self.patterns['amendment'], content):
            amendments.append(m.group(1).strip())
        
        # í—¤ë” ì˜ì—­ í…ìŠ¤íŠ¸ (ì²« 200ì ì •ë„)
        header_text = content[:min(200, len(content))]
        header_text = re.sub(r'\n+', '\n', header_text)
        
        return SubChunk(
            section_id=f"annex_{annex_no}_header",
            section_type="header",
            content=header_text,
            metadata={
                "annex_no": annex_num,
                "title": title,
                "related_article": related_article,
                "amendments": amendments
            },
            char_count=len(header_text),
            order=order
        )
    
    def _clean_annex_text(self, text: str) -> str:
        """
        Annex í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°
        
        âœ… Phase 0.9.3: ë³„í‘œ ë¶„ë¦¬ í›„ ì‹¤í–‰
        """
        
        # 1. Private Use Area (U+F000 ~ U+F8FF) ì œê±°
        text = re.sub(r'[\uF000-\uF8FF]', '', text)
        
        # 2. Box drawing characters ì œê±°
        box_chars = 'â”€â”â”‚â”ƒâ”Œâ”â””â”˜â”œâ”¤â”¬â”´â”¼â•‹â•â•‘â•”â•—â•šâ•â• â•£â•¦â•©â•¬â– â–¡â–ªâ–«'
        for char in box_chars:
            text = text.replace(char, '')
        
        # 3. ê¸°íƒ€ íŠ¹ìˆ˜ ê³µë°± ë¬¸ì ì •ë¦¬
        text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)
        
        # 4. ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 5. ì—°ì† ê°œí–‰ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 6. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        return text.strip()
    
    def _extract_table_sections(
        self,
        text: str,
        annex_no: str,
        start_order: int
    ) -> List[SubChunk]:
        """Table ì„¹ì…˜ ë¶„ë¦¬"""
        chunks = []
        order = start_order
        
        separators = list(re.finditer(
            self.patterns['table_separator'],
            text,
            re.MULTILINE
        ))
        
        if not separators:
            # êµ¬ë¶„ì ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ
            table_text = self._extract_table_body(text)
            if table_text and len(table_text.strip()) > 20:
                chunks.append(SubChunk(
                    section_id=f"annex_{annex_no}_table_main",
                    section_type="table_rows",
                    content=table_text,
                    metadata={
                        "table_title": "main",
                        "row_count_estimate": self._estimate_row_count(table_text)
                    },
                    char_count=len(table_text),
                    order=order
                ))
            return chunks
        
        # êµ¬ë¶„ì ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¦¬
        for i, sep_match in enumerate(separators):
            section_title = sep_match.group(1)
            start_pos = sep_match.start()
            end_pos = (separators[i+1].start()
                      if i+1 < len(separators)
                      else len(text))
            
            section_text = text[start_pos:end_pos]
            
            # Note ì œì™¸
            section_text_no_notes = re.sub(
                r'^\*.*$',
                '',
                section_text,
                flags=re.MULTILINE
            )
            
            if len(section_text_no_notes.strip()) > 20:
                chunks.append(SubChunk(
                    section_id=f"annex_{annex_no}_table_{i+1}",
                    section_type="table_rows",
                    content=section_text_no_notes.strip(),
                    metadata={
                        "table_title": section_title,
                        "row_count_estimate": self._estimate_row_count(section_text_no_notes)
                    },
                    char_count=len(section_text_no_notes.strip()),
                    order=order
                ))
                order += 1
        
        return chunks
    
    def _extract_table_body(self, text: str) -> str:
        """í‘œ ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ"""
        lines = text.split('\n')
        table_start = 0
        
        for i, line in enumerate(lines):
            if re.match(r'^\d+\s', line):
                table_start = i
                break
        
        if table_start == 0:
            return ""
        
        table_lines = []
        for line in lines[table_start:]:
            if line.startswith('*'):
                break
            if line.strip():
                table_lines.append(line)
        
        return '\n'.join(table_lines)
    
    def _extract_notes(
        self,
        text: str,
        annex_no: str,
        start_order: int
    ) -> List[SubChunk]:
        """Note ì²­í¬ ì¶”ì¶œ"""
        chunks = []
        order = start_order
        
        note_lines = []
        for line in text.split('\n'):
            if line.strip().startswith('*'):
                note_lines.append(line.strip())
        
        if note_lines:
            note_content = '\n'.join(note_lines)
            chunks.append(SubChunk(
                section_id=f"annex_{annex_no}_notes",
                section_type="note",
                content=note_content,
                metadata={"note_count": len(note_lines)},
                char_count=len(note_content),
                order=order
            ))
        
        return chunks
    
    def _estimate_row_count(self, text: str) -> int:
        """í–‰ ê°œìˆ˜ ì¶”ì •"""
        lines = [l for l in text.split('\n') if l.strip()]
        digit_lines = [l for l in lines if re.match(r'^\d+\s', l)]
        return len(digit_lines)
    
    def _fallback_chunk(self, text: str, annex_no: str) -> List[SubChunk]:
        """
        Fallback: ë³„í‘œ íŒ¨í„´ ì—†ì„ ë•Œ
        """
        logger.warning("   âš ï¸ Fallback ëª¨ë“œ: ì „ì²´ë¥¼ ë‹¨ì¼ ì²­í¬ë¡œ ì²˜ë¦¬")
        
        cleaned = self._clean_annex_text(text)
        
        return [SubChunk(
            section_id=f"annex_{annex_no}_fallback",
            section_type="unknown",
            content=cleaned,
            metadata={"fallback": True},
            char_count=len(cleaned),
            order=0
        )]
    
    def _validate_multi_annex(
        self,
        annex_sections: List[Dict],
        chunks: List[SubChunk],
        original_text: str
    ):
        """
        Phase 0.9.3: ë‹¤ì¤‘ ë³„í‘œ QA ê²€ì¦
        
        1. ë³„í‘œ ê°œìˆ˜ vs í—¤ë” ì²­í¬ ê°œìˆ˜
        2. ê° ë³„í‘œì˜ ìµœì†Œ ë¬¸ì ìˆ˜
        3. ì „ì²´ í…ìŠ¤íŠ¸ ì†ì‹¤ë¥ 
        """
        # 1. ë³„í‘œ ê°œìˆ˜ ê²€ì¦
        header_chunks = [c for c in chunks if 'header' in c.section_type]
        annex_count = len(annex_sections)
        header_count = len(header_chunks)
        
        if header_count != annex_count:
            logger.error(
                f"âŒ ë³„í‘œ ê°œìˆ˜ ë¶ˆì¼ì¹˜! "
                f"ì…ë ¥: {annex_count}ê°œ, ì¶œë ¥: {header_count}ê°œ"
            )
        else:
            logger.info(f"âœ… ë³„í‘œ ê°œìˆ˜ ì¼ì¹˜: {annex_count}ê°œ")
        
        # 2. ê° ë³„í‘œ ìµœì†Œ ë¬¸ì ìˆ˜ ê²€ì¦
        for annex_sec in annex_sections:
            annex_no = annex_sec['annex_no']
            annex_chunks = [
                c for c in chunks
                if c.section_id.startswith(f"annex_{annex_no}")
            ]
            
            total_chars = sum(c.char_count for c in annex_chunks)
            
            if total_chars < 50:
                logger.warning(
                    f"âš ï¸ ë³„í‘œ{annex_no} ë¬¸ì ìˆ˜ ë¶€ì¡±: {total_chars}ì"
                )
            else:
                logger.info(f"   ë³„í‘œ{annex_no}: {total_chars}ì, {len(annex_chunks)}ê°œ ì²­í¬")
        
        # 3. ì „ì²´ í…ìŠ¤íŠ¸ ì†ì‹¤ë¥ 
        total_output = sum(c.char_count for c in chunks)
        loss_rate = abs(total_output - len(original_text)) / len(original_text)
        
        if loss_rate > 0.10:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì†ì‹¤ë¥  {loss_rate:.1%}")
        else:
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ì†ì‹¤ë¥  {loss_rate:.1%} (í—ˆìš© ë²”ìœ„)")
    
    def _log_chunk_types(self, chunks: List[SubChunk]):
        """ì²­í¬ íƒ€ì… í†µê³„"""
        type_counts = {}
        for chunk in chunks:
            ctype = chunk.section_type
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        logger.info(f"   íƒ€ì… ë¶„í¬: {dict(type_counts)}")


def validate_subchunks(chunks: List[SubChunk], original_length: int) -> Dict[str, Any]:
    """
    ì„œë¸Œì²­í¬ ê²€ì¦
    
    Phase 0.9.3: ë‹¤ì¤‘ ë³„í‘œ ê²€ì¦ ì¶”ê°€
    """
    if not chunks:
        return {
            'is_valid': False,
            'reason': 'ì²­í¬ ì—†ìŒ',
            'chunk_count': 0,
            'type_counts': {},
            'loss_rate': 1.0,
            'has_header': False
        }
    
    type_counts = {}
    for chunk in chunks:
        ctype = chunk.section_type
        type_counts[ctype] = type_counts.get(ctype, 0) + 1
    
    total_chars = sum(c.char_count for c in chunks)
    loss_rate = abs(total_chars - original_length) / original_length if original_length > 0 else 0
    
    has_header = any('header' in c.section_type for c in chunks)
    
    is_valid = (
        len(chunks) >= 1 and
        loss_rate < 0.15 and
        has_header
    )
    
    return {
        'is_valid': is_valid,
        'reason': 'OK' if is_valid else 'ê²€ì¦ ì‹¤íŒ¨',
        'chunk_count': len(chunks),
        'type_counts': type_counts,
        'loss_rate': loss_rate,
        'has_header': has_header
    }