"""
core/annex_subchunker.py - Phase 0.9.7.2 Header/Body Split Fix + Table Candidate Merge

GPT ë¯¸ì†¡ë‹˜ ìµœì¢… ì§„ë‹¨:
Phase 0.9.7.1 ì‹¤íŒ¨ ì›ì¸: í—¤ë”ê°€ í‘œ ë¸”ë¡ ê·¼ì²˜ì— ì•„ì˜ˆ ì—†ìŒ (í™•ì¥: â†‘0 â†“0)

Phase 0.9.7.2 í•µì‹¬ ê°œì„ :
1. âœ… header/body ë¶„ë¦¬ ì¬ì •ì˜ (cleaned_contentì—ì„œ ë‹¤ì‹œ í—¤ë” ì°¾ê¸°)
2. âœ… table_candidate ìƒíƒœ + Merge (ì•½í•œ í‘œ í›„ë³´ 0.45~0.6 ë³´ì¡´)
3. âœ… ê²½ê³„ ë³´ì • ê°•í™” (ì¡°ê±´ ê¸°ë°˜, ê¸´ ë¬¸ì¥ ê°ì§€)

ëª©í‘œ: TableParser ì„±ê³µë¥  1/3 â†’ 3/3 (í—¤ë”+í‘œ ì´ˆë°˜ë¶€ â†’ í‘œ ë¸”ë¡)

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT ë¯¸ì†¡ë‹˜
Date: 2025-11-25
Version: Phase 0.9.7.2 Header Split Fix + Table Candidate Merge
"""

import re
import logging
import statistics
from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# âœ… Phase 0.9.7.1: TableParser ì¹œí™”ì  ìƒìˆ˜ (GPT ë¯¸ì†¡ë‹˜)
HEADER_KEYWORDS = re.compile(
    r"(ì„ìš©í•˜ê³ ì|ì„œì—´ëª…ë¶€|ì§ê¸‰|ì‘ì‹œìê²©|ìŠ¹ì§„|ì œì™¸|êµ¬ë¶„|ë¹„ê³ |ì¸ì›ìˆ˜|ì ìˆ˜|í‰ì •|í‰ê°€)",
    re.IGNORECASE
)

SEPARATOR_LINE = re.compile(r"^[-â”€â€”]{3,}$")

DIGITISH_LINE = re.compile(r"^\s*(\d+[\.\)]?|\([0-9]+\)|[ê°€-í£]\)|[A-Za-z]\))\s+")


@dataclass
class SubChunk:
    """ì„œë¸Œì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    section_id: str
    section_type: str
    content: str
    metadata: Dict[str, Any]
    char_count: int
    order: int


class AnnexSubChunker:
    """
    Annex ì„œë¸Œì²­í‚¹ (Phase 0.9.7 Table Block Segmentation)
    
    GPT ë¯¸ì†¡ë‹˜ ì„¤ê³„:
    - ë¼ì¸ ìœ ì§€ â†’ í‘œ ë¸”ë¡ ê°ì§€ â†’ ì²­í¬ ë³€í™˜
    - Table Block Segmentation (5ê°œ íŠ¹ì§•)
    - Explainable Metadata (Block-Level)
    - P0 ì•ˆì •ì„± ìœ ì§€
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        
        self.patterns = {
            'annex_header': r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)',
            'related_article': r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>',
            'amendment': r'\[(.*?(\d{4}\.\d{1,2}\.\d{1,2}).*?)\]',
            'note_marker': r'^\*\s*(.+)$',
            # âœ… Phase 0.9.7: Digit Regex êµì²´ (GPT ë¯¸ì†¡ë‹˜)
            'digit_line': r'^\d+(\s*\S+)?$',  # "1", "1 5ë²ˆê¹Œì§€" ëª¨ë‘ ë§¤ì¹­
            'header_keywords': r'(ì§ê¸‰|ì‘ì‹œìê²©|ë¹„ê³ |ì¸ì›ìˆ˜|ì„œì—´ëª…ë¶€|ìˆœìœ„)',
        }
        
        logger.info("âœ… AnnexSubChunker v0.9.7.2 ì´ˆê¸°í™” (Header Split Fix + Table Candidate Merge)")
    
    def chunk(self, annex_text: str, annex_no: str = "1") -> List[SubChunk]:
        """
        Annex í…ìŠ¤íŠ¸ â†’ ì„œë¸Œì²­í‚¹ (Phase 0.9.7)
        
        GPT ë¯¸ì†¡ë‹˜ ë‹¨ê³„:
        1. Annex ì™„ì „ ë¶„ë¦¬
        2. ë¼ì¸ ìœ ì§€ (ë¬¸ë‹¨ ë¶„ë¦¬ ê¸ˆì§€!)
        3. Table Block Segmentation
        4. Block â†’ Chunk ë³€í™˜
        5. Loss Check
        """
        logger.info(f"ğŸ”§ Phase 0.9.7: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘: {len(annex_text)}ì")
        
        # Step 1: Annex ì™„ì „ ë¶„ë¦¬ (raw ë‹¨ìœ„)
        annex_sections = self._split_by_annex(annex_text)
        
        if not annex_sections:
            logger.warning("âš ï¸ ë³„í‘œ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - fallback ì²˜ë¦¬")
            return self._fallback_chunk(annex_text, annex_no)
        
        logger.info(f"âœ… Step 1: ë³„í‘œ ë¶„ë¦¬ ì™„ë£Œ: {len(annex_sections)}ê°œ")
        
        # Canonical text ìƒì„± (Loss Check ê¸°ì¤€)
        canonical_text = self._clean_annex_text(annex_text)
        
        # Step 2-4: ê° ë³„í‘œë§ˆë‹¤ Table Block Segmentation
        all_chunks = []
        global_order = 0
        
        for annex_sec in annex_sections:
            annex_num = annex_sec['annex_no']
            raw_content = annex_sec['content']
            header_end_pos = annex_sec['header_end_pos']
            
            logger.info(f"   ğŸ”¹ ë³„í‘œ{annex_num} ì²˜ë¦¬ ì¤‘... ({len(raw_content)}ì)")
            
            # ë…¸ì´ì¦ˆ ì œê±° (ë³„í‘œ ë¶„ë¦¬ í›„)
            cleaned_content = self._clean_annex_text(raw_content)
            
            # âœ… Phase 0.9.7: Table Block Segmentation
            section_chunks = self._process_single_annex_v097(
                cleaned_content,
                annex_num,
                global_order,
                header_end_pos
            )
            
            all_chunks.extend(section_chunks)
            global_order += len(section_chunks)
            
            logger.info(f"   âœ… ë³„í‘œ{annex_num}: {len(section_chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # Step 5: Annex Loss Check
        self._check_annex_loss(canonical_text, all_chunks)
        
        logger.info(f"âœ… Phase 0.9.7: Annex ì„œë¸Œì²­í‚¹ ì™„ë£Œ: ì´ {len(all_chunks)}ê°œ")
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for chunk in all_chunks:
            ctype = chunk.section_type
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")
        
        return all_chunks
    
    def _split_by_annex(self, annex_text: str) -> List[Dict[str, Any]]:
        """Step 1: ë³„í‘œ ë‹¨ìœ„ë¡œ ì™„ì „ ë¶„ë¦¬"""
        pattern = r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)'
        
        matches = list(re.finditer(pattern, annex_text))
        
        if not matches:
            return []
        
        sections = []
        
        for i, match in enumerate(matches):
            annex_no = match.group(1)
            start_pos = match.start()
            header_end_pos = match.end()
            
            # ë‹¤ìŒ ë³„í‘œê¹Œì§€ ë˜ëŠ” ëê¹Œì§€
            if i + 1 < len(matches):
                end_pos = matches[i + 1].start()
            else:
                end_pos = len(annex_text)
            
            content = annex_text[start_pos:end_pos].strip()
            
            # Header ë ìœ„ì¹˜ë¥¼ ìƒëŒ€ ìœ„ì¹˜ë¡œ ë³€í™˜
            relative_header_end = header_end_pos - start_pos
            
            sections.append({
                'annex_no': annex_no,
                'content': content,
                'start_pos': start_pos,
                'end_pos': end_pos,
                'header_end_pos': relative_header_end
            })
        
        return sections
    
    def _process_single_annex_v097(
        self,
        content: str,
        annex_no: str,
        start_order: int,
        header_end_pos: int
    ) -> List[SubChunk]:
        """
        âœ… Phase 0.9.7.2: Header/Body Split Fix (GPT ë¯¸ì†¡ë‹˜ ì§„ë‹¨)
        
        í•µì‹¬ ê°œì„ :
        1. cleaned_contentì—ì„œ ë‹¤ì‹œ í—¤ë” ì°¾ê¸° (pos ë²„ê·¸ í•´ê²°)
        2. Table Candidate Merge (ì•½í•œ í‘œ í›„ë³´ ë³´ì¡´)
        3. ê²½ê³„ ë³´ì • ê°•í™”
        """
        chunks = []
        order = start_order
        
        # âœ… Phase 0.9.7.2: Header/Body ì¬ì •ì˜ (GPT ë¯¸ì†¡ë‹˜)
        # ë¬¸ì œ: raw ìœ„ì¹˜ë¡œ cleaned_content ìë¦„ â†’ ê²½ê³„ ì˜¤ì—¼
        # í•´ê²°: cleaned_contentì—ì„œ ë‹¤ì‹œ í—¤ë” ì°¾ê¸°
        header_text, body_text = self._split_header_body(content)
        
        # Step 1: Header ì²­í¬
        if header_text:
            header_chunk = self._extract_header_chunk(header_text, annex_no, order)
            if header_chunk:
                chunks.append(header_chunk)
                order += 1
        
        # âœ… Phase 0.9.7: ë¼ì¸ ìœ ì§€ (ë¬¸ë‹¨ ë¶„ë¦¬ ê¸ˆì§€!)
        lines = body_text.split('\n')
        lines = [l for l in lines if l.strip()]  # ë¹ˆ ì¤„ë§Œ ì œê±°
        
        logger.info(f"      ë¼ì¸ ìœ ì§€: {len(lines)}ê°œ")
        
        # Step 2: Table Block Segmentation
        blocks = self._segment_blocks(lines)
        
        # âœ… Phase 0.9.7.2: Table Candidate Merge (GPT ë¯¸ì†¡ë‹˜)
        blocks = self._merge_table_candidates(blocks)
        
        logger.info(f"      ë¸”ë¡ ë¶„ë¦¬: {len(blocks)}ê°œ")
        
        # Step 3: Block â†’ Chunk ë³€í™˜
        for i, block in enumerate(blocks):
            block_lines = block['lines']
            block_type = block['type']
            block_metadata = block['metadata']
            
            is_last_block = (i == len(blocks) - 1)
            
            # Note ì¶”ì¶œ (ë§ˆì§€ë§‰ ë¸”ë¡ì—ì„œë§Œ)
            if is_last_block:
                note_chunk, remaining_lines = self._extract_note_from_lines(
                    block_lines, annex_no, order
                )
                
                if remaining_lines:
                    block_chunk = self._create_block_chunk(
                        remaining_lines, annex_no, order, i, block_type, block_metadata
                    )
                    if block_chunk:
                        chunks.append(block_chunk)
                        order += 1
                
                if note_chunk:
                    chunks.append(note_chunk)
                    order += 1
            else:
                # ì¤‘ê°„ ë¸”ë¡
                block_chunk = self._create_block_chunk(
                    block_lines, annex_no, order, i, block_type, block_metadata
                )
                if block_chunk:
                    chunks.append(block_chunk)
                    order += 1
        
        return chunks
    
    def _segment_blocks(self, lines: List[str]) -> List[Dict[str, Any]]:
        """
        âœ… Phase 0.9.7.2: Table Block Segmentation + Candidate (GPT ë¯¸ì†¡ë‹˜)
        
        ê°œì„ ì‚¬í•­:
        1. í‘œ ë¸”ë¡ ê°ì§€ (0.9.7)
        2. TableParser ì¹œí™”ì  ê²½ê³„ ë³´ì • (0.9.7.1)
        3. table_candidate ìƒíƒœ ì¶”ê°€ (0.9.7.2) â† í•µì‹¬!
           - ì•½í•œ í‘œ í›„ë³´ (0.45~0.6) ë³´ì¡´
           - ë‹¤ìŒ ë¸”ë¡ì´ table_rowsë©´ merge
        """
        blocks = []
        
        if not lines:
            return blocks
        
        # ìœˆë„ìš° ìŠ¬ë¼ì´ë”© (5~8 ë¼ì¸)
        window_size = min(8, max(5, len(lines) // 3))
        
        i = 0
        while i < len(lines):
            # ìœˆë„ìš° ì„¤ì •
            window_end = min(i + window_size, len(lines))
            window_lines = lines[i:window_end]
            
            # 5ê°œ íŠ¹ì§• ê³„ì‚°
            features = self._calculate_block_features(window_lines)
            
            # Table Score ê³„ì‚°
            table_score = self._calculate_table_score(features)
            
            # âœ… Phase 0.9.7.2: Block íƒ€ì… ê²°ì • (table_candidate ì¶”ê°€)
            if table_score >= 0.6:
                # ê°•í™•ì‹ : table_rows
                block_type = "table_rows"
                
                # í‘œ ë¸”ë¡ í™•ì¥ (ì—°ì†ëœ í‘œ ë¼ì¸ ëª¨ë‘ í¬í•¨)
                extended_end = self._extend_table_block(lines, i, window_end, features)
                
                # âœ… Phase 0.9.7.1: TableParser ì¹œí™”ì  ê²½ê³„ ë³´ì •
                refined_start, refined_end, expand_meta = self._refine_table_block_boundaries(
                    lines, i, extended_end
                )
                
                block_lines = lines[refined_start:refined_end]
                
                logger.info(
                    f"         í‘œ ë¸”ë¡ ê°ì§€: {refined_start}~{refined_end} ë¼ì¸ "
                    f"(ì ìˆ˜: {table_score:.2f}, í™•ì¥: â†‘{expand_meta['expanded_up']} â†“{expand_meta['expanded_down']})"
                )
                
                i = refined_end
                
                features_with_expand = {
                    **features,
                    'table_score': round(table_score, 3),
                    **expand_meta
                }
            
            elif 0.45 <= table_score < 0.6 and (
                features['short_line_ratio'] > 0.8 or
                features['digit_density'] > 0.25 or
                features['header_hint']
            ):
                # âœ… Phase 0.9.7.2: ì•½í™•ì‹  â†’ table_candidate (GPT ë¯¸ì†¡ë‹˜)
                block_type = "table_candidate"
                
                # í‘œ ë¸”ë¡ í™•ì¥
                extended_end = self._extend_table_block(lines, i, window_end, features)
                block_lines = lines[i:extended_end]
                
                logger.info(
                    f"         í‘œ í›„ë³´ ê°ì§€: {i}~{extended_end} ë¼ì¸ "
                    f"(ì ìˆ˜: {table_score:.2f}, ë³´ë¥˜)"
                )
                
                i = extended_end
                
                features_with_expand = {
                    **features,
                    'table_score': round(table_score, 3)
                }
            
            else:
                # ì•½í™•ì‹ /í™•ì‹  ì—†ìŒ: paragraph
                block_type = "paragraph"
                
                # ë‹¤ìŒ í‘œ ë¸”ë¡ê¹Œì§€ ë˜ëŠ” ìœˆë„ìš° ëê¹Œì§€
                block_lines = window_lines
                
                i = window_end
                
                features_with_expand = {
                    **features,
                    'table_score': round(table_score, 3)
                }
            
            # Block ìƒì„±
            blocks.append({
                'type': block_type,
                'lines': block_lines,
                'metadata': features_with_expand
            })
        
        return blocks
    
    def _calculate_block_features(self, lines: List[str]) -> Dict[str, Any]:
        """
        âœ… Phase 0.9.7: 5ê°œ íŠ¹ì§• ê³„ì‚°
        """
        if not lines:
            return {
                'digit_density': 0.0,
                'short_line_ratio': 0.0,
                'column_gap_consistency': 0.0,
                'header_hint': False,
                'avg_line_length': 0.0
            }
        
        # 1. digit_density (GPT ë¯¸ì†¡ë‹˜: ì¦‰ì‹œ íš¨ê³¼)
        digit_lines = [l for l in lines if re.match(self.patterns['digit_line'], l)]
        digit_density = len(digit_lines) / len(lines)
        
        # 2. short_line_ratio
        line_lengths = [len(l) for l in lines]
        avg_line_length = sum(line_lengths) / len(lines)
        short_lines = [l for l in line_lengths if l < 50]  # 50ì ë¯¸ë§Œ
        short_line_ratio = len(short_lines) / len(lines)
        
        # 3. column_gap_consistency (ê³µë°± ì •ë ¬)
        space_positions = []
        for line in lines:
            positions = [i for i, c in enumerate(line) if c == ' ']
            if positions:
                space_positions.append(positions[0] if positions else -1)
        
        if len(space_positions) > 1:
            variance = statistics.variance([p for p in space_positions if p >= 0])
            column_gap_consistency = 1.0 / (1.0 + variance)
        else:
            column_gap_consistency = 0.0
        
        # 4. header_hint
        first_line = lines[0] if lines else ""
        header_hint = bool(re.search(self.patterns['header_keywords'], first_line))
        
        return {
            'digit_density': round(digit_density, 3),
            'short_line_ratio': round(short_line_ratio, 3),
            'column_gap_consistency': round(column_gap_consistency, 3),
            'header_hint': header_hint,
            'avg_line_length': round(avg_line_length, 1)
        }
    
    def _calculate_table_score(self, features: Dict[str, Any]) -> float:
        """
        âœ… Phase 0.9.7: Table Score ê³„ì‚°
        
        GPT ë¯¸ì†¡ë‹˜ ê°€ì¤‘ì¹˜:
        - digit_density: 40%
        - short_line_ratio: 30%
        - column_gap_consistency: 20%
        - header_hint: 10%
        """
        score = 0.0
        
        # Digit Density (40%)
        score += features['digit_density'] * 0.4
        
        # Short Line Ratio (30%)
        score += features['short_line_ratio'] * 0.3
        
        # Column Gap Consistency (20%)
        score += features['column_gap_consistency'] * 0.2
        
        # Header Hint (10%)
        if features['header_hint']:
            score += 0.1
        
        return score
    
    def _extend_table_block(
        self,
        lines: List[str],
        start: int,
        end: int,
        features: Dict[str, Any]
    ) -> int:
        """í‘œ ë¸”ë¡ í™•ì¥ (ì—°ì†ëœ í‘œ ë¼ì¸ í¬í•¨)"""
        extended_end = end
        
        # ë‹¤ìŒ ë¼ì¸ë“¤ë„ í‘œ íŒ¨í„´ì´ë©´ í¬í•¨
        while extended_end < len(lines):
            next_line = lines[extended_end]
            
            # Digit ë¼ì¸ì´ë©´ í¬í•¨
            if re.match(self.patterns['digit_line'], next_line):
                extended_end += 1
            # ì§§ì€ ë¼ì¸ì´ë©´ í¬í•¨
            elif len(next_line) < 50:
                extended_end += 1
            # ì•„ë‹ˆë©´ ì¢…ë£Œ
            else:
                break
        
        return extended_end
    
    def _refine_table_block_boundaries(
        self,
        lines: List[str],
        start: int,
        end: int
    ) -> Tuple[int, int, Dict[str, Any]]:
        """
        âœ… Phase 0.9.7.2: TableParser ì¹œí™”ì  ê²½ê³„ ë³´ì • ê°•í™” (GPT ë¯¸ì†¡ë‹˜)
        
        ê°œì„ ì‚¬í•­:
        - ê¸°ì¡´ (0.9.7.1): MAX_EXPAND_UP = 5 (ë„ˆë¬´ ì§§ìŒ)
        - ê°œì„  (0.9.7.2): ì¡°ê±´ ê¸°ë°˜ í™•ì¥ (ê¸´ ë¬¸ì¥ ë§Œë‚˜ê¸° ì „ê¹Œì§€)
        """
        refined_start = start
        refined_end = end
        
        # ---- ì—­ë°©í–¥ í™•ì¥ (ì¡°ê±´ ê¸°ë°˜) ----
        i = start - 1
        expanded_up = 0
        while i >= 0:
            prev_line = lines[i].strip()
            
            if not prev_line:
                i -= 1
                expanded_up += 1
                continue
            
            # âœ… Phase 0.9.7.2: ê¸´ ë¬¸ì¥ ë§Œë‚˜ë©´ ì¢…ë£Œ (GPT ë¯¸ì†¡ë‹˜)
            if len(prev_line) > 80:
                break  # í‘œ í—¤ë” ëìœ¼ë¡œ ê°„ì£¼
            
            # í—¤ë” í‚¤ì›Œë“œ/êµ¬ë¶„ì„ ì´ë©´ ë¸”ë¡ì— í¬í•¨
            if HEADER_KEYWORDS.search(prev_line) or SEPARATOR_LINE.match(prev_line):
                refined_start = i
                i -= 1
                expanded_up += 1
                continue
            
            # í‘œ í–‰ íŒ¨í„´ì´ë©´ ë¶™ì—¬ë„ ë¨
            if DIGITISH_LINE.match(prev_line):
                refined_start = i
                i -= 1
                expanded_up += 1
                continue
            
            break  # ê·¸ ì™¸ëŠ” í™•ì¥ ì¢…ë£Œ
        
        # ---- ì •ë°©í–¥ í™•ì¥ (ì¡°ê±´ ê¸°ë°˜) ----
        j = end
        expanded_down = 0
        max_down = 8  # ì •ë°©í–¥ì€ ì œí•œ ìœ ì§€
        
        while j < len(lines) and expanded_down < max_down:
            next_line = lines[j].strip()
            
            if not next_line:
                j += 1
                expanded_down += 1
                continue
            
            # ê¸´ ë¬¸ì¥ ë§Œë‚˜ë©´ ì¢…ë£Œ
            if len(next_line) > 80:
                break
            
            if HEADER_KEYWORDS.search(next_line) or SEPARATOR_LINE.match(next_line):
                refined_end = j + 1
                j += 1
                expanded_down += 1
                continue
            
            if DIGITISH_LINE.match(next_line):
                refined_end = j + 1
                j += 1
                expanded_down += 1
                continue
            
            break
        
        # ë©”íƒ€ë°ì´í„°
        expand_meta = {
            "refined_start": refined_start,
            "refined_end": refined_end,
            "expanded_up": expanded_up,
            "expanded_down": expanded_down,
        }
        
        return refined_start, refined_end, expand_meta
    
    def _split_header_body(self, content: str) -> Tuple[str, str]:
        """
        âœ… Phase 0.9.7.2: Header/Body ì¬ì •ì˜ (GPT ë¯¸ì†¡ë‹˜)
        
        ë¬¸ì œ: raw ìœ„ì¹˜ë¡œ cleaned_content ìë¦„ â†’ ê²½ê³„ ì˜¤ì—¼
        í•´ê²°: cleaned_contentì—ì„œ ë‹¤ì‹œ í—¤ë” ì°¾ê¸°
        """
        # ë³„í‘œ í—¤ë” íŒ¨í„´ ì°¾ê¸°
        match = re.search(self.patterns['annex_header'], content)
        if not match:
            return "", content
        
        header_end = match.end()
        
        # í—¤ë” ë ì´í›„ ì²« ë¹ˆì¤„ ë˜ëŠ” í‘œ ì‹œì‘ê¹Œì§€ë¥¼ í—¤ë”ë¡œ í™•ì¥
        lines = content.split('\n')
        
        # header_end ìœ„ì¹˜ë¥¼ ë¼ì¸ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        char_count = 0
        header_line_idx = 0
        for idx, line in enumerate(lines):
            char_count += len(line) + 1  # +1 for \n
            if char_count >= header_end:
                header_line_idx = idx + 1  # ë‹¤ìŒ ë¼ì¸ë¶€í„° body
                break
        
        # í—¤ë” í™•ì¥: ê´€ë ¨ ì¡°ë¬¸, ê°œì •ì´ë ¥ í¬í•¨
        while header_line_idx < len(lines):
            line = lines[header_line_idx].strip()
            
            if not line:
                header_line_idx += 1
                break
            
            # ê´€ë ¨ ì¡°ë¬¸/ê°œì •ì´ë ¥ íŒ¨í„´
            if re.search(self.patterns['related_article'], line):
                header_line_idx += 1
                continue
            
            if re.search(self.patterns['amendment'], line):
                header_line_idx += 1
                continue
            
            # ìˆ«ìë¡œ ì‹œì‘í•˜ê±°ë‚˜ í‘œ íŒ¨í„´ì´ë©´ body ì‹œì‘
            if re.match(r'^\d+', line) or len(line) < 50:
                break
            
            # ê·¸ ì™¸ëŠ” í—¤ë”ì— í¬í•¨
            header_line_idx += 1
        
        header_text = '\n'.join(lines[:header_line_idx]).strip()
        body_text = '\n'.join(lines[header_line_idx:]).strip()
        
        return header_text, body_text
    
    def _merge_table_candidates(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        âœ… Phase 0.9.7.2: Table Candidate Merge (GPT ë¯¸ì†¡ë‹˜)
        
        ì „ëµ:
        - table_candidate + table_rows â†’ table_rows (í•©ì¹¨)
        - í—¤ë”+í‘œ ì´ˆë°˜ë¶€ë¥¼ í‘œ ë¸”ë¡ìœ¼ë¡œ ìŠ¹ê²©
        """
        merged = []
        
        for block in blocks:
            if merged and merged[-1]['type'] == "table_candidate" and block['type'] == "table_rows":
                # ì´ì „ ë¸”ë¡(table_candidate) + í˜„ì¬ ë¸”ë¡(table_rows) â†’ í•©ì¹¨
                prev_block = merged.pop()
                
                # ë¼ì¸ í•©ì¹˜ê¸°
                block['lines'] = prev_block['lines'] + block['lines']
                
                # Metadata ì—…ë°ì´íŠ¸
                if 'expanded_up' in block['metadata']:
                    block['metadata']['expanded_up'] += len(prev_block['lines'])
                else:
                    block['metadata']['expanded_up'] = len(prev_block['lines'])
                
                logger.info(
                    f"         í‘œ í›„ë³´ ìŠ¹ê²©: {len(prev_block['lines'])}ì¤„ ì¶”ê°€ "
                    f"(ì ìˆ˜: {prev_block['metadata']['table_score']:.2f} â†’ table_rows)"
                )
            
            merged.append(block)
        
        # ë‚¨ì€ table_candidateëŠ” paragraphë¡œ ê°•ë“±
        for block in merged:
            if block['type'] == "table_candidate":
                block['type'] = "paragraph"
                logger.info(
                    f"         í‘œ í›„ë³´ ê°•ë“±: paragraphë¡œ ì²˜ë¦¬ "
                    f"(ì ìˆ˜: {block['metadata']['table_score']:.2f})"
                )
        
        return merged
    
    def _create_block_chunk(
        self,
        lines: List[str],
        annex_no: str,
        order: int,
        block_index: int,
        block_type: str,
        metadata: Dict[str, Any]
    ) -> Optional[SubChunk]:
        """Block â†’ Chunk ë³€í™˜"""
        if not lines:
            return None
        
        content = '\n'.join(lines).strip()
        
        if len(content) < 10:
            return None
        
        # íŒì • ê·¼ê±°
        if block_type == "table_rows":
            íŒì •_ê·¼ê±° = (
                f"digit {metadata['digit_density']:.0%} + "
                f"ì§§ì€ë¼ì¸ {metadata['short_line_ratio']:.0%} + "
                f"ì •ë ¬ {metadata['column_gap_consistency']:.2f}"
            )
            if metadata['header_hint']:
                íŒì •_ê·¼ê±° += " + í—¤ë”"
        else:
            íŒì •_ê·¼ê±° = f"ì¼ë°˜ ë¬¸ë‹¨ (ì ìˆ˜: {metadata['table_score']:.2f})"
        
        return SubChunk(
            section_id=f"annex_{annex_no}_{block_type}_{block_index+1}",
            section_type=block_type,
            content=content,
            metadata={
                'block_index': block_index,
                **metadata,
                'íŒì •_ê·¼ê±°': íŒì •_ê·¼ê±°,
                'has_table': block_type == "table_rows"
            },
            char_count=len(content),
            order=order
        )
    
    def _extract_header_chunk(
        self,
        header_text: str,
        annex_no: str,
        order: int
    ) -> Optional[SubChunk]:
        """Header ì²­í¬ ìƒì„±"""
        match = re.search(self.patterns['annex_header'], header_text)
        if not match:
            return None
        
        annex_num = match.group(1)
        title = match.group(2).strip()
        
        # ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
        related_article = ""
        article_match = re.search(self.patterns['related_article'], header_text)
        if article_match:
            related_article = article_match.group(1)
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendments = []
        for m in re.finditer(self.patterns['amendment'], header_text):
            amendments.append(m.group(1).strip())
        
        return SubChunk(
            section_id=f"annex_{annex_no}_header",
            section_type="header",
            content=header_text,
            metadata={
                "annex_no": annex_num,
                "title": title,
                "related_article": related_article,
                "amendments": amendments
            },
            char_count=len(header_text),
            order=order
        )
    
    def _extract_note_from_lines(
        self,
        lines: List[str],
        annex_no: str,
        order: int
    ) -> Tuple[Optional[SubChunk], List[str]]:
        """ë§ˆì§€ë§‰ ë¸”ë¡ì—ì„œ Note ì¶”ì¶œ"""
        note_lines = []
        regular_lines = []
        
        in_note = False
        
        for line in lines:
            if re.match(self.patterns['note_marker'], line):
                in_note = True
                note_lines.append(line)
            elif in_note:
                note_lines.append(line)
            else:
                regular_lines.append(line)
        
        if not note_lines:
            return None, lines
        
        note_content = '\n'.join(note_lines).strip()
        
        note_chunk = SubChunk(
            section_id=f"annex_{annex_no}_note",
            section_type="note",
            content=note_content,
            metadata={
                "is_note": True
            },
            char_count=len(note_content),
            order=order
        )
        
        return note_chunk, regular_lines
    
    def _check_annex_loss(self, original_text: str, chunks: List[SubChunk]) -> None:
        """Annex Loss Check"""
        original_len = len(original_text)
        chunks_len = sum(chunk.char_count for chunk in chunks)
        
        loss_rate = abs(original_len - chunks_len) / original_len if original_len > 0 else 0
        
        logger.info(f"   ğŸ“Š Loss Check:")
        logger.info(f"      ì›ë³¸: {original_len}ì")
        logger.info(f"      ì²­í¬: {chunks_len}ì")
        logger.info(f"      ì†ì‹¤ë¥ : {loss_rate*100:.1f}%")
        
        if loss_rate > 0.03:
            logger.warning(f"   âš ï¸ ì†ì‹¤ë¥  {loss_rate*100:.1f}% > 3% (í—ˆìš©ì¹˜ ì´ˆê³¼)")
        else:
            logger.info(f"   âœ… ì†ì‹¤ë¥  {loss_rate*100:.1f}% â‰¤ 3% (í†µê³¼)")
    
    def _clean_annex_text(self, text: str) -> str:
        """ë…¸ì´ì¦ˆ ì œê±°"""
        cleaned = text
        
        # 1. í˜ì´ì§€ ë²ˆí˜¸ ì œê±°
        cleaned = re.sub(r'^\d+[-â€”â€“_]\d+\s*$', '', cleaned, flags=re.MULTILINE)
        cleaned = re.sub(r'^Page\s+\d+\s*$', '', cleaned, flags=re.MULTILINE | re.IGNORECASE)
        
        # 2. HTML íƒœê·¸ ì œê±°
        cleaned = re.sub(r'<[^>]+>', '', cleaned)
        
        # 3. ì—°ì† ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'[ \t]+', ' ', cleaned)
        
        # 4. ì—°ì† ê°œí–‰ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        
        return cleaned.strip()
    
    def _fallback_chunk(self, annex_text: str, annex_no: str) -> List[SubChunk]:
        """Fallback: ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ"""
        logger.warning("âš ï¸ Fallback ëª¨ë“œ: ì „ì²´ë¥¼ ë‹¨ì¼ ì²­í¬ë¡œ ì²˜ë¦¬")
        
        return [
            SubChunk(
                section_id=f"annex_{annex_no}_fallback",
                section_type="paragraph",
                content=annex_text.strip(),
                metadata={
                    "fallback": True
                },
                char_count=len(annex_text.strip()),
                order=0
            )
        ]


def validate_subchunks(chunks: List[SubChunk], original_length: int) -> dict:
    """
    âœ… Phase 0.9.7: ì‹œê·¸ë‹ˆì²˜ ìœ ì§€ (2-arg)
    
    ì„œë¸Œì²­í‚¹ ê²°ê³¼ ê²€ì¦
    """
    if not chunks:
        return {
            'is_valid': False,
            'reason': 'ì²­í¬ ì—†ìŒ',
            'chunk_count': 0,
            'loss_rate': 1.0
        }
    
    # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
    type_counts = {}
    for chunk in chunks:
        chunk_type = chunk.section_type
        type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
    
    # ì´ ë¬¸ì ìˆ˜
    total_chars = sum(c.char_count for c in chunks)
    
    # ì†ì‹¤ë¥ 
    loss_rate = abs(original_length - total_chars) / original_length if original_length > 0 else 0
    
    # ê²€ì¦ ê¸°ì¤€
    has_header = 'header' in type_counts
    has_content = 'table_rows' in type_counts or 'paragraph' in type_counts
    
    is_valid = has_header and has_content and loss_rate < 0.05
    
    return {
        'is_valid': is_valid,
        'chunk_count': len(chunks),
        'type_counts': type_counts,
        'total_chars': total_chars,
        'loss_rate': loss_rate,
        'has_header': has_header,
        'has_content': has_content
    }