"""
core/annex_subchunker.py - Phase 0.9.5.2.1 ê¸´ê¸‰ Hotfix

GPT ë¯¸ì†¡ë‹˜ í”¼ë“œë°± ë°˜ì˜:
1. âœ… validate_subchunks ì‹œê·¸ë‹ˆì²˜ ë³µêµ¬ (2-arg)
2. âœ… Phase 0.9.5.1 ì•ˆì •ì„± 100% ë³µêµ¬
3. â¸ï¸ í‘œ íŒì • ê°•í™” ë³´ë¥˜ (Phase 0.9.5.1 íŒ¨í„´ ìœ ì§€)

ìˆ˜ì • ì‚¬í•­:
- validate_subchunks(chunks, original_length) ì‹œê·¸ë‹ˆì²˜ ë³µêµ¬
- Phase 0.9.5.1 í‘œ íŒì • ë¡œì§ ìœ ì§€
- LawParser í˜¸ì¶œ ê³„ì•½ ì™„ì „ ë³µêµ¬

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT ë¯¸ì†¡ë‹˜ ê¸´ê¸‰ ê°€ì´ë“œ
Date: 2025-11-24
Version: Phase 0.9.5.2.1 Hotfix
"""

import re
import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class SubChunk:
    """ì„œë¸Œì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    section_id: str
    section_type: str
    content: str
    metadata: Dict[str, Any]
    char_count: int
    order: int


class AnnexSubChunker:
    """
    Annex ì„œë¸Œì²­í‚¹ (Phase 0.9.5.2.1 Hotfix)
    
    GPT ë¯¸ì†¡ë‹˜ ì›ì¹™:
    - í‘œ ì—†ëŠ” ë³„í‘œ: header + paragraphÃ—N + note
    - í‘œ ìˆëŠ” ë³„í‘œ: header + paragraph + table_rows + paragraph + note
    - í…ìŠ¤íŠ¸ ì†ì‹¤ Â±3% ì´í•˜ ë³´ì¥
    - validate_subchunks 2-arg ì‹œê·¸ë‹ˆì²˜ ì—„ìˆ˜
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        
        self.patterns = {
            'annex_header': r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)',
            'related_article': r'<(ì œ\d+ì¡°[^>]*)ê´€ë ¨>',
            'amendment': r'\[(.*?(\d{4}\.\d{1,2}\.\d{1,2}).*?)\]',
            'note_marker': r'^\*\s*(.+)$',
            'table_start': r'^\d+\s+',  # âœ… Phase 0.9.5.1 íŒ¨í„´ ìœ ì§€
        }
        
        logger.info("âœ… AnnexSubChunker v0.9.5.2.1 ì´ˆê¸°í™” (Hotfix - Rollback to 0.9.5.1)")
    
    def chunk(self, annex_text: str, annex_no: str = "1") -> List[SubChunk]:
        """
        Annex í…ìŠ¤íŠ¸ â†’ ì„œë¸Œì²­í‚¹ (Phase 0.9.5.2.1 Hotfix)
        
        GPT ë¯¸ì†¡ë‹˜ 6ë‹¨ê³„:
        1. Annex ì™„ì „ ë¶„ë¦¬
        2. ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™”
        3. í‘œ ìœ„/ì•„ë˜ ë¬¸ë‹¨ ë³´ì¡´
        4. noteëŠ” ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œë§Œ
        5. Loss Check ìµœì¢… ë‹¨ê³„
        6. DualQA ì˜í–¥ ì—†ìŒ
        """
        logger.info(f"ğŸ”§ Phase 0.9.5.2.1 Hotfix: Annex ì„œë¸Œì²­í‚¹ ì‹œì‘: {len(annex_text)}ì")
        
        # Step 1: Annex ì™„ì „ ë¶„ë¦¬ (raw ë‹¨ìœ„)
        annex_sections = self._split_by_annex(annex_text)
        
        if not annex_sections:
            logger.warning("âš ï¸ ë³„í‘œ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - fallback ì²˜ë¦¬")
            return self._fallback_chunk(annex_text, annex_no)
        
        logger.info(f"âœ… Step 1: ë³„í‘œ ë¶„ë¦¬ ì™„ë£Œ: {len(annex_sections)}ê°œ")
        
        # Canonical text ìƒì„± (Loss Check ê¸°ì¤€)
        canonical_text = self._clean_annex_text(annex_text)
        
        # Step 2-4: ê° ë³„í‘œë§ˆë‹¤ ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™”
        all_chunks = []
        global_order = 0
        
        for annex_sec in annex_sections:
            annex_num = annex_sec['annex_no']
            raw_content = annex_sec['content']
            header_end_pos = annex_sec['header_end_pos']
            
            logger.info(f"   ğŸ”¹ ë³„í‘œ{annex_num} ì²˜ë¦¬ ì¤‘... ({len(raw_content)}ì)")
            
            # ë…¸ì´ì¦ˆ ì œê±° (ë³„í‘œ ë¶„ë¦¬ í›„)
            cleaned_content = self._clean_annex_text(raw_content)
            
            # ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™”
            section_chunks = self._process_single_annex_v095(
                cleaned_content,
                annex_num,
                global_order,
                header_end_pos
            )
            
            all_chunks.extend(section_chunks)
            global_order += len(section_chunks)
            
            logger.info(f"   âœ… ë³„í‘œ{annex_num}: {len(section_chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # Step 5: Annex Loss Check (ìµœì¢… ë‹¨ê³„ë§Œ!)
        self._check_annex_loss(canonical_text, all_chunks)
        
        # Step 6: DualQAëŠ” ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ!
        logger.info(f"âœ… Phase 0.9.5.2.1 Hotfix: Annex ì„œë¸Œì²­í‚¹ ì™„ë£Œ: ì´ {len(all_chunks)}ê°œ")
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for chunk in all_chunks:
            ctype = chunk.section_type
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"   - {ctype}: {count}ê°œ")
        
        return all_chunks
    
    def _split_by_annex(self, annex_text: str) -> List[Dict[str, Any]]:
        """
        Step 1: ë³„í‘œ ë‹¨ìœ„ë¡œ ì™„ì „ ë¶„ë¦¬
        """
        pattern = r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n<]+)'
        
        matches = list(re.finditer(pattern, annex_text))
        
        if not matches:
            return []
        
        sections = []
        
        for i, match in enumerate(matches):
            annex_no = match.group(1)
            start_pos = match.start()
            header_end_pos = match.end()
            
            # ë‹¤ìŒ ë³„í‘œê¹Œì§€ ë˜ëŠ” ëê¹Œì§€
            if i + 1 < len(matches):
                end_pos = matches[i + 1].start()
            else:
                end_pos = len(annex_text)
            
            content = annex_text[start_pos:end_pos].strip()
            
            # Header ë ìœ„ì¹˜ë¥¼ ìƒëŒ€ ìœ„ì¹˜ë¡œ ë³€í™˜
            relative_header_end = header_end_pos - start_pos
            
            sections.append({
                'annex_no': annex_no,
                'content': content,
                'start_pos': start_pos,
                'end_pos': end_pos,
                'header_end_pos': relative_header_end
            })
        
        return sections
    
    def _process_single_annex_v095(
        self,
        content: str,
        annex_no: str,
        start_order: int,
        header_end_pos: int
    ) -> List[SubChunk]:
        """
        Step 2-4: ë‹¨ì¼ ë³„í‘œ ë¬¸ë‹¨ ë‹¨ìœ„ êµ¬ì¡°í™” (Phase 0.9.5.2.1 Hotfix)
        
        âœ… Phase 0.9.5.1 ë¡œì§ ì™„ì „ ë³µêµ¬
        
        GPT ë¯¸ì†¡ë‹˜ ì›ì¹™:
        - ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ìë¦„
        - í‘œê°€ ìˆëŠ” ë¬¸ë‹¨ë§Œ table_rowsë¡œ ë³€í™˜
        - ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ paragraph
        - noteëŠ” ê°€ì¥ ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œë§Œ
        """
        chunks = []
        order = start_order
        
        # Header/Body ì •í™•íˆ ë¶„ë¦¬
        header_text = content[:header_end_pos].strip()
        body_text = content[header_end_pos:].strip()
        
        # Step 2-1: Header ì²­í¬
        if header_text:
            header_chunk = self._extract_header_chunk(header_text, annex_no, order)
            if header_chunk:
                chunks.append(header_chunk)
                order += 1
        
        # Step 3: ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ë¶„ë¦¬ (ë¹ˆ ì¤„ ê¸°ì¤€)
        paragraphs = self._split_into_paragraphs(body_text)
        
        logger.info(f"      ë¬¸ë‹¨ ë¶„ë¦¬: {len(paragraphs)}ê°œ")
        
        # Step 3-4: ê° ë¬¸ë‹¨ ì²˜ë¦¬
        for i, para_text in enumerate(paragraphs):
            is_last_paragraph = (i == len(paragraphs) - 1)
            
            # Step 4: ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œë§Œ note ì¶”ì¶œ
            if is_last_paragraph:
                note_chunk, remaining_text = self._extract_note_from_paragraph(
                    para_text, annex_no, order
                )
                
                if remaining_text and remaining_text.strip():
                    para_chunk = self._create_paragraph_or_table(
                        remaining_text, annex_no, order, i
                    )
                    if para_chunk:
                        chunks.append(para_chunk)
                        order += 1
                
                if note_chunk:
                    chunks.append(note_chunk)
                    order += 1
            else:
                # ì¤‘ê°„ ë¬¸ë‹¨: paragraph ë˜ëŠ” table_rowsë¡œ ë³€í™˜
                para_chunk = self._create_paragraph_or_table(
                    para_text, annex_no, order, i
                )
                if para_chunk:
                    chunks.append(para_chunk)
                    order += 1
        
        return chunks
    
    def _extract_header_chunk(
        self,
        header_text: str,
        annex_no: str,
        order: int
    ) -> Optional[SubChunk]:
        """Header ì²­í¬ ìƒì„±"""
        match = re.search(self.patterns['annex_header'], header_text)
        if not match:
            return None
        
        annex_num = match.group(1)
        title = match.group(2).strip()
        
        # ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
        related_article = ""
        article_match = re.search(self.patterns['related_article'], header_text)
        if article_match:
            related_article = article_match.group(1)
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendments = []
        for m in re.finditer(self.patterns['amendment'], header_text):
            amendments.append(m.group(1).strip())
        
        return SubChunk(
            section_id=f"annex_{annex_no}_header",
            section_type="header",
            content=header_text,
            metadata={
                "annex_no": annex_num,
                "title": title,
                "related_article": related_article,
                "amendments": amendments
            },
            char_count=len(header_text),
            order=order
        )
    
    def _split_into_paragraphs(self, text: str) -> List[str]:
        """ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬ (ë¹ˆ ì¤„ ê¸°ì¤€)"""
        paragraphs = re.split(r'\n\s*\n', text)
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        return paragraphs
    
    def _create_paragraph_or_table(
        self,
        para_text: str,
        annex_no: str,
        order: int,
        para_index: int
    ) -> Optional[SubChunk]:
        """
        âœ… Phase 0.9.5.2.1 Hotfix: Phase 0.9.5.1 í‘œ íŒì • ë¡œì§ ë³µêµ¬
        
        GPT ë¯¸ì†¡ë‹˜ ì›ì¹™:
        - í‘œ íŒ¨í„´ì´ ìˆìœ¼ë©´ table_rows
        - ì—†ìœ¼ë©´ paragraph
        - ë‘˜ ë‹¤ ë³´ì¡´ (ì†ì‹¤ ì—†ìŒ)
        """
        if not para_text or len(para_text.strip()) < 10:
            return None
        
        lines = para_text.split('\n')
        
        # âœ… Phase 0.9.5.1 í‘œ íŒì • ë¡œì§ (ê²€ì¦ëœ ë²„ì „)
        digit_lines = [l for l in lines if re.match(self.patterns['table_start'], l)]
        
        has_table = len(digit_lines) >= 3  # Phase 0.9.5.1 ê¸°ì¤€
        
        if has_table:
            # table_rows ì²­í¬
            return SubChunk(
                section_id=f"annex_{annex_no}_table_{para_index+1}",
                section_type="table_rows",
                content=para_text.strip(),
                metadata={
                    "para_index": para_index,
                    "row_count_estimate": len(digit_lines),
                    "has_table": True
                },
                char_count=len(para_text.strip()),
                order=order
            )
        else:
            # paragraph ì²­í¬
            return SubChunk(
                section_id=f"annex_{annex_no}_para_{para_index+1}",
                section_type="paragraph",
                content=para_text.strip(),
                metadata={
                    "para_index": para_index,
                    "has_table": False
                },
                char_count=len(para_text.strip()),
                order=order
            )
    
    def _extract_note_from_paragraph(
        self,
        para_text: str,
        annex_no: str,
        order: int
    ) -> tuple[Optional[SubChunk], str]:
        """ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œ Note ì¶”ì¶œ"""
        lines = para_text.split('\n')
        
        note_lines = []
        regular_lines = []
        
        in_note = False
        
        for line in lines:
            if re.match(self.patterns['note_marker'], line):
                in_note = True
                note_lines.append(line)
            elif in_note:
                note_lines.append(line)
            else:
                regular_lines.append(line)
        
        if not note_lines:
            return None, para_text
        
        note_content = '\n'.join(note_lines).strip()
        remaining_content = '\n'.join(regular_lines).strip()
        
        note_chunk = SubChunk(
            section_id=f"annex_{annex_no}_note",
            section_type="note",
            content=note_content,
            metadata={
                "is_note": True
            },
            char_count=len(note_content),
            order=order
        )
        
        return note_chunk, remaining_content
    
    def _check_annex_loss(self, original_text: str, chunks: List[SubChunk]) -> None:
        """Annex Loss Check (ë§ˆì§€ë§‰ ë‹¨ê³„ë§Œ)"""
        original_len = len(original_text)
        chunks_len = sum(chunk.char_count for chunk in chunks)
        
        loss_rate = abs(original_len - chunks_len) / original_len if original_len > 0 else 0
        
        logger.info(f"   ğŸ“Š Loss Check:")
        logger.info(f"      ì›ë³¸: {original_len}ì")
        logger.info(f"      ì²­í¬: {chunks_len}ì")
        logger.info(f"      ì†ì‹¤ë¥ : {loss_rate*100:.1f}%")
        
        if loss_rate > 0.03:
            logger.warning(f"   âš ï¸ ì†ì‹¤ë¥  {loss_rate*100:.1f}% > 3% (í—ˆìš©ì¹˜ ì´ˆê³¼)")
        else:
            logger.info(f"   âœ… ì†ì‹¤ë¥  {loss_rate*100:.1f}% â‰¤ 3% (í†µê³¼)")
    
    def _clean_annex_text(self, text: str) -> str:
        """ë…¸ì´ì¦ˆ ì œê±° (ë‹¨ì¼ ë²„ì „)"""
        cleaned = text
        
        # 1. í˜ì´ì§€ ë²ˆí˜¸ ì œê±°
        cleaned = re.sub(r'^\d+[-â€”â€“_]\d+\s*$', '', cleaned, flags=re.MULTILINE)
        cleaned = re.sub(r'^Page\s+\d+\s*$', '', cleaned, flags=re.MULTILINE | re.IGNORECASE)
        
        # 2. HTML íƒœê·¸ ì œê±°
        cleaned = re.sub(r'<[^>]+>', '', cleaned)
        
        # 3. ì—°ì† ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'[ \t]+', ' ', cleaned)
        
        # 4. ì—°ì† ê°œí–‰ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        
        return cleaned.strip()
    
    def _fallback_chunk(self, annex_text: str, annex_no: str) -> List[SubChunk]:
        """Fallback: ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ"""
        logger.warning("âš ï¸ Fallback ëª¨ë“œ: ì „ì²´ë¥¼ ë‹¨ì¼ ì²­í¬ë¡œ ì²˜ë¦¬")
        
        return [
            SubChunk(
                section_id=f"annex_{annex_no}_fallback",
                section_type="paragraph",
                content=annex_text.strip(),
                metadata={
                    "fallback": True
                },
                char_count=len(annex_text.strip()),
                order=0
            )
        ]


def validate_subchunks(chunks: List[SubChunk], original_length: int) -> dict:
    """
    âœ… Phase 0.9.5.2.1 Hotfix: ì‹œê·¸ë‹ˆì²˜ ë³µêµ¬ (2-arg)
    
    ì„œë¸Œì²­í‚¹ ê²°ê³¼ ê²€ì¦
    
    Args:
        chunks: ì„œë¸Œì²­í¬ ë¦¬ìŠ¤íŠ¸
        original_length: ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´ (Loss Check ê¸°ì¤€)
    
    Returns:
        ê²€ì¦ ê²°ê³¼ dict
    """
    if not chunks:
        return {
            'is_valid': False,
            'reason': 'ì²­í¬ ì—†ìŒ',
            'chunk_count': 0,
            'loss_rate': 1.0
        }
    
    # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
    type_counts = {}
    for chunk in chunks:
        chunk_type = chunk.section_type
        type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
    
    # ì´ ë¬¸ì ìˆ˜
    total_chars = sum(c.char_count for c in chunks)
    
    # ì†ì‹¤ë¥ 
    loss_rate = abs(original_length - total_chars) / original_length if original_length > 0 else 0
    
    # ê²€ì¦ ê¸°ì¤€
    has_header = 'header' in type_counts
    has_content = 'table_rows' in type_counts or 'paragraph' in type_counts
    
    is_valid = has_header and has_content and loss_rate < 0.05
    
    return {
        'is_valid': is_valid,
        'chunk_count': len(chunks),
        'type_counts': type_counts,
        'total_chars': total_chars,
        'loss_rate': loss_rate,
        'has_header': has_header,
        'has_content': has_content
    }