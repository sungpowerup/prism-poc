"""
core/annex_subchunker.py - PRISM Phase 0.8
Annex ì„œë¸Œì²­í‚¹ ì—”ì§„

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + ì´ì„œì˜ (Backend Lead)
Date: 2025-11-17
Version: Phase 0.8
"""

import re
import logging
from dataclasses import dataclass
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)


@dataclass
class SubChunk:
    """ì„œë¸Œì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    section_id: str
    section_type: str  # 'header' | 'table_rows' | 'note' | 'exception'
    content: str
    metadata: Dict
    char_count: int
    order: int  # GPT í”¼ë“œë°±: ìˆœì„œ ë©”íƒ€ë°ì´í„°


class AnnexSubChunker:
    """
    Annex ì„œë¸Œì²­í‚¹ ì—”ì§„
    
    GPT í•µì‹¬ í”¼ë“œë°± ë°˜ì˜:
    1. Row ì™„ë²½ íŒŒì‹± ëª©í‘œ ì œê±° (â†’ Phase 0.9)
    2. ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ì— ì§‘ì¤‘
    3. section_type ëª…í™•í™”
    4. order ë©”íƒ€ë°ì´í„° ì¶”ê°€
    """
    
    def __init__(self):
        self.patterns = {
            'header': r'\[ë³„í‘œ\s*(\d+)\]\s*([^\n]+)',
            'related_article': r'<(ì œ\d+ì¡°.*?)ê´€ë ¨>',
            'amendment': r'\(ê°œì •([0-9.,\s]+)\)',
            'note_marker': r'^\*',
            'table_separator': r'^ì„ìš©í•˜ê³ ìí•˜ëŠ”ì¸ì›ìˆ˜ì—ëŒ€í•œìŠ¹ì§„í›„ë³´ìë²”ìœ„\((\d+ê¸‰.*?)\)'
        }
        logger.info("âœ… AnnexSubChunker ì´ˆê¸°í™” (Phase 0.8)")
    
    def chunk(self, annex_content: str, annex_no: str = "1") -> List[SubChunk]:
        """
        Annex í…ìŠ¤íŠ¸ â†’ ì„œë¸Œì²­í¬ ë¦¬ìŠ¤íŠ¸
        
        Args:
            annex_content: Annex ì›ë¬¸
            annex_no: ë³„í‘œ ë²ˆí˜¸
        
        Returns:
            ì„œë¸Œì²­í¬ ë¦¬ìŠ¤íŠ¸ (ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬)
        """
        if not annex_content or len(annex_content) < 50:
            logger.warning("âš ï¸ Annex ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ - ì„œë¸Œì²­í‚¹ ìŠ¤í‚µ")
            return []
        
        logger.info(f"ğŸ“‹ Annex ì„œë¸Œì²­í‚¹ ì‹œì‘: {len(annex_content)}ì")
        
        chunks = []
        order = 0
        
        # 1. Header ì²­í¬
        header_chunk = self._extract_header(annex_content, annex_no, order)
        if header_chunk:
            chunks.append(header_chunk)
            order += 1
        
        # 2. Table ì˜ì—­ ë¶„ë¦¬ (3ê¸‰ì œì™¸ / 3ê¸‰ìŠ¹ì§„ ë“±)
        table_chunks = self._extract_table_sections(annex_content, annex_no, order)
        chunks.extend(table_chunks)
        order += len(table_chunks)
        
        # 3. Note ì²­í¬
        note_chunks = self._extract_notes(annex_content, annex_no, order)
        chunks.extend(note_chunks)
        
        # 4. í…ìŠ¤íŠ¸ ì†ì‹¤ ê²€ì¦ (GPT í•„ìˆ˜ ìš”êµ¬ì‚¬í•­)
        total_chars = sum(c.char_count for c in chunks)
        loss_rate = abs(total_chars - len(annex_content)) / len(annex_content)
        
        if loss_rate > 0.05:  # 5% ì´ˆê³¼ ì†ì‹¤
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì†ì‹¤ {loss_rate:.1%} - ê¸°ì¤€ ì´ˆê³¼!")
        else:
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ì†ì‹¤ {loss_rate:.1%} (í—ˆìš© ë²”ìœ„)")
        
        logger.info(f"âœ… Annex ì„œë¸Œì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        self._log_chunk_types(chunks)
        
        return chunks
    
    def _extract_header(
        self, 
        text: str, 
        annex_no: str, 
        order: int
    ) -> Optional[SubChunk]:
        """í—¤ë” ì²­í¬ ì¶”ì¶œ"""
        
        # [ë³„í‘œN] íŒ¨í„´ ì°¾ê¸°
        match = re.search(self.patterns['header'], text)
        if not match:
            return None
        
        annex_num = match.group(1)
        title = match.group(2).strip()
        
        # ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
        related_article = ""
        article_match = re.search(self.patterns['related_article'], text)
        if article_match:
            related_article = article_match.group(1)
        
        # ê°œì •ì´ë ¥ ì¶”ì¶œ
        amendments = []
        for m in re.finditer(self.patterns['amendment'], text):
            amendments.append(m.group(1).strip())
        
        # í—¤ë” ì˜ì—­ í…ìŠ¤íŠ¸ (ì²« 100ì ì •ë„)
        header_text = text[:min(200, len(text))]
        header_text = re.sub(r'\n+', '\n', header_text)
        
        return SubChunk(
            section_id=f"annex_{annex_no}_header",
            section_type="header",
            content=header_text,
            metadata={
                "annex_no": annex_num,
                "title": title,
                "related_article": related_article,
                "amendments": amendments
            },
            char_count=len(header_text),
            order=order
        )
    
    def _extract_table_sections(
        self, 
        text: str, 
        annex_no: str, 
        start_order: int
    ) -> List[SubChunk]:
        """
        Table ì„¹ì…˜ ë¶„ë¦¬
        
        GPT í”¼ë“œë°±: Rowë³„ íŒŒì‹±ì´ ì•„ë‹Œ "ì„¹ì…˜ ë¶„ë¦¬"
        ì˜ˆ: 3ê¸‰ì œì™¸, 3ê¸‰ìŠ¹ì§„
        """
        chunks = []
        order = start_order
        
        # í‘œ êµ¬ë¶„ íŒ¨í„´ ì°¾ê¸°
        separators = list(re.finditer(
            self.patterns['table_separator'], 
            text, 
            re.MULTILINE
        ))
        
        if not separators:
            # êµ¬ë¶„ì ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ
            table_text = self._extract_table_body(text)
            if table_text:
                chunks.append(SubChunk(
                    section_id=f"annex_{annex_no}_table_main",
                    section_type="table_rows",
                    content=table_text,
                    metadata={
                        "table_title": "main",
                        "row_count_estimate": self._estimate_row_count(table_text)
                    },
                    char_count=len(table_text),
                    order=order
                ))
            return chunks
        
        # êµ¬ë¶„ì ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¦¬
        for i, sep_match in enumerate(separators):
            section_title = sep_match.group(1)
            
            # ì„¹ì…˜ ì‹œì‘/ë ìœ„ì¹˜
            start_pos = sep_match.start()
            end_pos = (separators[i+1].start() 
                      if i+1 < len(separators) 
                      else len(text))
            
            section_text = text[start_pos:end_pos]
            
            # Note ë¶„ë¦¬
            section_text_no_notes = re.sub(
                r'^\*.*$', 
                '', 
                section_text, 
                flags=re.MULTILINE
            )
            
            if len(section_text_no_notes.strip()) > 20:
                chunks.append(SubChunk(
                    section_id=f"annex_{annex_no}_table_{i+1}",
                    section_type="table_rows",
                    content=section_text_no_notes.strip(),
                    metadata={
                        "table_title": section_title,
                        "row_count_estimate": self._estimate_row_count(section_text_no_notes)
                    },
                    char_count=len(section_text_no_notes.strip()),
                    order=order
                ))
                order += 1
        
        return chunks
    
    def _extract_table_body(self, text: str) -> str:
        """í‘œ ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ (í—¤ë” ì œì™¸)"""
        
        # ì²« ë²ˆì§¸ ìˆ«ì í–‰ì´ ì‹œì‘ë˜ëŠ” ì§€ì ë¶€í„°
        lines = text.split('\n')
        table_start = 0
        
        for i, line in enumerate(lines):
            if re.match(r'^\d+\s', line):
                table_start = i
                break
        
        if table_start == 0:
            return ""
        
        # Note ì‹œì‘ ì „ê¹Œì§€
        table_lines = []
        for line in lines[table_start:]:
            if line.startswith('*'):
                break
            if line.strip():
                table_lines.append(line)
        
        return '\n'.join(table_lines)
    
    def _extract_notes(
        self, 
        text: str, 
        annex_no: str, 
        start_order: int
    ) -> List[SubChunk]:
        """Note ì²­í¬ ì¶”ì¶œ (* ì‹œì‘)"""
        
        chunks = []
        order = start_order
        
        # * ì‹œì‘í•˜ëŠ” ëª¨ë“  ì¤„
        note_lines = []
        for line in text.split('\n'):
            if line.strip().startswith('*'):
                note_lines.append(line.strip())
        
        # ê° Noteë¥¼ ê°œë³„ ì²­í¬ë¡œ
        for i, note in enumerate(note_lines):
            # ë…¸ì´ì¦ˆ ì œê±° (GPT ìš”êµ¬ì‚¬í•­)
            note_clean = self._clean_note(note)
            
            chunks.append(SubChunk(
                section_id=f"annex_{annex_no}_note_{i+1}",
                section_type="note",
                content=note_clean,
                metadata={
                    "note_type": self._classify_note(note_clean)
                },
                char_count=len(note_clean),
                order=order
            ))
            order += 1
        
        return chunks
    
    def _clean_note(self, note: str) -> str:
        """
        Note ë…¸ì´ì¦ˆ ì œê±°
        
        GPT ìš”êµ¬ì‚¬í•­: ê¾¸ë°ˆì„ , ì¥ì‹ ë¬¸ì ì œê±°
        """
        # ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ì ì œê±° (â”€, â•, â” ë“±)
        note = re.sub(r'[â”€â•â”]{2,}', '', note)
        
        # ì—°ì† ê³µë°± ì •ë¦¬
        note = re.sub(r'\s{2,}', ' ', note)
        
        return note.strip()
    
    def _classify_note(self, note: str) -> str:
        """Note ìœ í˜• ë¶„ë¥˜"""
        note_lower = note.lower()
        
        if 'ì˜ˆì™¸' in note or 'ë‹¨,' in note or 'ë‹¤ë§Œ' in note:
            return "exception"
        elif 'í¬í•¨' in note or 'ë²”ìœ„' in note:
            return "rule"
        else:
            return "general"
    
    def _estimate_row_count(self, text: str) -> int:
        """Row ê°œìˆ˜ ì¶”ì • (ì™„ë²½í•œ íŒŒì‹± ì•„ë‹˜)"""
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ê°œìˆ˜
        lines = text.split('\n')
        count = sum(1 for line in lines if re.match(r'^\d+\s', line))
        return count
    
    def _log_chunk_types(self, chunks: List[SubChunk]):
        """ì²­í¬ íƒ€ì… í†µê³„ ë¡œê¹…"""
        type_counts = {}
        for chunk in chunks:
            type_counts[chunk.section_type] = type_counts.get(chunk.section_type, 0) + 1
        
        logger.info(f"   ğŸ“Š ì²­í¬ íƒ€ì… ë¶„í¬:")
        for ctype, count in sorted(type_counts.items()):
            logger.info(f"      - {ctype}: {count}ê°œ")


# ============================================
# ìœ í‹¸ë¦¬í‹°
# ============================================

def validate_subchunks(chunks: List[SubChunk], original_length: int) -> Dict:
    """
    ì„œë¸Œì²­í¬ ê²€ì¦ (GPT í•„ìˆ˜ ìš”êµ¬ì‚¬í•­)
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    total_chars = sum(c.char_count for c in chunks)
    loss_rate = abs(total_chars - original_length) / original_length
    
    type_counts = {}
    for chunk in chunks:
        type_counts[chunk.section_type] = type_counts.get(chunk.section_type, 0) + 1
    
    # GPT ìš”êµ¬: ì˜ë¯¸ ë‹¨ìœ„ ê²€ì¦
    has_header = 'header' in type_counts
    has_table = 'table_rows' in type_counts
    has_note = 'note' in type_counts
    has_multiple_types = len(type_counts) >= 2
    
    is_valid = (
        loss_rate < 0.05 and
        has_multiple_types and
        len(chunks) >= 3
    )
    
    return {
        'is_valid': is_valid,
        'loss_rate': loss_rate,
        'chunk_count': len(chunks),
        'type_counts': type_counts,
        'has_header': has_header,
        'has_table': has_table,
        'has_note': has_note
    }


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    
    # ê°„ë‹¨ í…ŒìŠ¤íŠ¸
    sample = """
[ë³„í‘œ 1] ìŠ¹ì§„í›„ë³´ìë²”ìœ„(3ê¸‰ìŠ¹ì§„ì œì™¸)
<ì œ20ì¡°ì œ2í•­ê´€ë ¨>(ê°œì •2003.3.29)

1 5ë²ˆê¹Œì§€
2 10ë²ˆê¹Œì§€
*ì„ìš©í•˜ê³ ìí•˜ëŠ”ì¸ì›ìˆ˜ê°€5ëª…ê¹Œì§€ëŠ”ì„œì—´ëª…ë¶€ìˆœìœ„ì˜5ë°°ìˆ˜
"""
    
    chunker = AnnexSubChunker()
    chunks = chunker.chunk(sample)
    
    print(f"\nâœ… ì²­í¬ {len(chunks)}ê°œ ìƒì„±")
    for c in chunks:
        print(f"   - {c.section_type}: {c.char_count}ì")
