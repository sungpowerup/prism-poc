"""
core/semantic_chunker.py
PRISM Phase 0.4.0 P0-2 ê¸´ê¸‰ íŒ¨ì¹˜ (ê¸°ë³¸ì •ì‹  í—¤ë” ì¸ì‹)

âœ… í•µì‹¬ ê°œì„ :
1. ê¸°ë³¸ì •ì‹  í—¤ë” ìš°ì„  íƒì§€ (priority=1)
2. JSON/ë¦¬ë·°ìš© ëª¨ë‘ì—ì„œ ì²« ì²­í¬ë¡œ ë³´ì¡´
3. íƒ€ì… ë¶„í¬ì—ì„œ basic íƒ€ì… ì •ìƒ í‘œì‹œ

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ (ë°•ì¤€í˜¸ AI/ML Lead) + GPT ë³´ì •
Date: 2025-11-13
Version: Phase 0.4.0 P0-2 (Emergency Patch)
"""

import re
import logging
from typing import List, Dict, Any, Tuple, Set

logger = logging.getLogger(__name__)


class SemanticChunker:
    """Phase 0.4.0 P0-2 ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (ê¸°ë³¸ì •ì‹  ë³´ì¡´)"""
    
    # ìœ ì—°í•œ ìˆ«ì íŒ¨í„´
    NUM = r'(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)'
    
    # âœ… Dual-Rail íŒ¨í„´
    ARTICLE_STRICT = re.compile(
        r'(?m)^(?:#{0,6}\s*)ì œ\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)\s*ì¡°'
        r'(?:\s*ì˜\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+))?\s*\(',
        re.IGNORECASE
    )
    
    ARTICLE_LOOSE = re.compile(
        r'(?m)^(?:#{0,6}\s*)ì œ\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)\s*ì¡°'
        r'(?:\s*ì˜\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+))?'
        r'(?!\s*\d)',
        re.IGNORECASE
    )
    
    INLINE_REF = re.compile(r'^.{0,30}ì œ\s*\d+\s*ì¡°\s*ì œ\s*\d+\s*(?:í•­|í˜¸)', re.MULTILINE)
    
    CHAPTER = re.compile(
        r'(?m)^(?:#{0,6}\s*)ì œ\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)\s*ì¥',
        re.IGNORECASE
    )
    
    # âœ… P0-2: ê¸°ë³¸ì •ì‹  í—¤ë” íŒ¨í„´ ê°•í™”
    BASIC = re.compile(
        r'(?m)^(?:#{0,6}\s*)?ê¸°\s*ë³¸\s*ì •\s*ì‹ |^ê¸°ë³¸ì •ì‹ |^ê¸°ë³¸\s*ì •ì‹ ',
        re.IGNORECASE
    )
    
    SUPPLEMENT = re.compile(
        r'(?m)^(?:#{0,6}\s*)ë¶€\s*ì¹™',
        re.IGNORECASE
    )
    
    ROMAN = re.compile(r'(?m)^[â… -â…¨]+\.', re.IGNORECASE)
    
    # âœ… P0-2: ìš°ì„ ìˆœìœ„ ì¡°ì • (basicì´ ìµœìš°ì„ )
    PRIORITY = {
        'basic': 1,        # ìµœìš°ì„ 
        'chapter': 2,
        'supplement': 3,
        'article': 4,
        'article_loose': 4,
        'roman': 5
    }
    
    def __init__(self):
        logger.info("âœ… SemanticChunker Phase 0.4.0 P0-2 ì´ˆê¸°í™” (ê¸°ë³¸ì •ì‹  ë³´ì¡´)")
        logger.info("   ğŸ¯ ì œ4ì¡° ëˆ„ë½ ë°©ì§€ + ê¸°ë³¸ì •ì‹  í—¤ë” ì¸ì‹")
        logger.info("   ğŸ”§ ê°•í™”ëœ ë¼ì¸ë¸Œë ˆì´í¬ + ì ì‘í˜• ë³‘í•© + ìë™ QA")
    
    def _pre_normalize(self, text: str) -> str:
        """ê°•í™”ëœ ë¼ì¸ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬"""
        # 1. í•œ ì¤„ì— ë¶™ì€ ì¥+ì¡° ë¶„ë¦¬
        text = re.sub(r'(ì¥[^\n]{1,40})\s+(ì œ\s*\d+\s*ì¡°)', r'\1\n\2', text)
        
        # 2. ì¡° í—¤ë” ì•ì— ê°•ì œ ê°œí–‰ (ê´„í˜¸ ìˆëŠ” ê²½ìš°)
        text = re.sub(r'([^\n])(\s*ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?\s*\()', r'\1\n\2', text)
        
        # 3. ì¡° í—¤ë” ì•ì— ê°•ì œ ê°œí–‰ (ê´„í˜¸ ì—†ëŠ” ê²½ìš°)
        text = re.sub(r'([^\n])(\s*ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?(?!\d))', r'\1\n\2', text)
        
        # 4. í—¤ë”© ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬
        text = re.sub(r'([^\n])\s*(#{1,6}\s*ì œ\s*\d+\s*[ì¥ì¡°][^\n]*)', r'\1\n\2', text)
        
        # 5. ë³¼ë“œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        text = re.sub(r'([^\n])\s*(\*\*\s*ì œ\s*\d+\s*ì¡°[^*]*\*\*)', r'\1\n\2', text)
        
        # 6. ì¥ ì•ì— ì¤„ë°”ê¿ˆ
        text = re.sub(r'([^\n])\s*(ì œ\s*\d+\s*ì¥)', r'\1\n\2', text)
        
        # 7. ë¶€ì¹™ ì•ì— ì¤„ë°”ê¿ˆ
        text = re.sub(r'([^\n])\s*(ë¶€\s*ì¹™)', r'\1\n\2', text)
        
        # âœ… P0-2: ê¸°ë³¸ì •ì‹  ì•ì— ì¤„ë°”ê¿ˆ
        text = re.sub(r'([^\n])\s*(ê¸°\s*ë³¸\s*ì •\s*ì‹ )', r'\1\n\2', text)
        
        # 8. ë¶ˆê·œì¹™ ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 9. ì—°ì† ê°œí–‰ ì •ë¦¬ (ìµœëŒ€ 2ê°œ)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text
    
    def _is_inline_reference(self, pos: int, matched: str, text: str) -> bool:
        """ì¸ë¼ì¸ ì°¸ì¡° íŒì • (ê°™ì€ ì¤„ë§Œ ê²€ì‚¬)"""
        # 1. ê²½ê³„ê°€ ì†í•œ ì¤„ ì¶”ì¶œ
        line_start = text.rfind('\n', 0, pos) + 1
        line_end = text.find('\n', pos)
        if line_end == -1:
            line_end = len(text)
        
        line = text[line_start:line_end]
        
        # 2. ê°™ì€ ì¤„ì—ì„œ ì¸ë¼ì¸ ì°¸ì¡° íŒ¨í„´ ì²´í¬
        if self.INLINE_REF.match(line):
            logger.debug(f"      ğŸ” ì¸ë¼ì¸ ì°¸ì¡° ê°ì§€ (ê°™ì€ ì¤„): {matched}")
            return True
        
        # 3. ê´„í˜¸+ì œëª© ì²´í¬
        if '(' not in matched and ')' not in matched:
            logger.debug(f"      ğŸ” ê´„í˜¸ ì—†ìŒ: {matched}")
            return True
        
        return False
    
    def _filter_inline_references(
        self,
        boundaries: List[Tuple[int, str, str]],
        text: str
    ) -> List[Tuple[int, str, str]]:
        """article_loose ì‚¬í›„ í•„í„°ë§ (ì •êµí™”)"""
        filtered = []
        removed_count = 0
        
        for pos, btype, matched in boundaries:
            if btype == 'article_loose':
                if self._is_inline_reference(pos, matched, text):
                    removed_count += 1
                    continue
            
            filtered.append((pos, btype, matched))
        
        if removed_count > 0:
            logger.info(f"   ğŸ—‘ï¸ ì¸ë¼ì¸ ì°¸ì¡° ì œê±°: {removed_count}ê°œ")
        
        return filtered
    
    def _find_boundaries(self, text: str) -> List[Tuple[int, str, str]]:
        """
        âœ… Dual-Rail ê²½ê³„ íƒì§€ + ê¸°ë³¸ì •ì‹  ìš°ì„  íƒì§€
        
        ìš°ì„ ìˆœìœ„:
        1. ê¸°ë³¸ì •ì‹  (ìµœìš°ì„ )
        2. ì¥
        3. ì¡°(Strict)
        4. ë¶€ì¹™
        5. ì¡°(Loose, í•„ìš”ì‹œ)
        """
        boundaries = []
        
        # âœ… P0-2: 1ë‹¨ê³„ - ê¸°ë³¸ì •ì‹  ìš°ì„  íƒì§€
        for match in self.BASIC.finditer(text):
            boundaries.append((
                match.start(),
                'basic',
                match.group(0).strip()
            ))
            logger.info(f"   ğŸ“– ê¸°ë³¸ì •ì‹  ê°ì§€: {match.group(0).strip()}")
        
        # 2ë‹¨ê³„: ì¥, ì¡°(Strict), ë¶€ì¹™
        for pattern, pattern_type in [
            (self.CHAPTER, 'chapter'),
            (self.ARTICLE_STRICT, 'article'),
            (self.SUPPLEMENT, 'supplement'),
            (self.ROMAN, 'roman')
        ]:
            for match in pattern.finditer(text):
                boundaries.append((
                    match.start(),
                    pattern_type,
                    match.group(0).strip()
                ))
        
        # ì¡°ë¬¸ ê°œìˆ˜ í™•ì¸
        article_count = sum(1 for _, t, _ in boundaries if t == 'article')
        logger.info(f"   ğŸ” 1ë‹¨ê³„ (Strict): ì¡°ë¬¸ {article_count}ê°œ")
        
        # 3ë‹¨ê³„: ì¡°ë¬¸ < 12ê°œë©´ Loose ë³´ê°•
        if article_count < 12:
            logger.info(f"   ğŸ” ì¡°ë¬¸ ë¶€ì¡± â†’ Loose íŒ¨í„´ ë³´ê°•")
            
            for match in self.ARTICLE_LOOSE.finditer(text):
                pos = match.start()
                matched = match.group(0).strip()
                
                # ì¤‘ë³µ ì²´í¬
                if not any(b[0] == pos for b in boundaries):
                    boundaries.append((pos, 'article_loose', matched))
            
            article_count = sum(1 for _, t, _ in boundaries if t in ('article', 'article_loose'))
            logger.info(f"   âœ… 2ë‹¨ê³„ (Loose): ì¡°ë¬¸ {article_count}ê°œ")
        
        # ìœ„ì¹˜ ê¸°ì¤€ ì •ë ¬
        boundaries.sort(key=lambda x: (x[0], self.PRIORITY[x[1]]))
        
        # ì¤‘ë³µ ì œê±°
        unique_boundaries = []
        last_pos = -1
        for b in boundaries:
            if b[0] != last_pos:
                unique_boundaries.append(b)
                last_pos = b[0]
        
        # 4ë‹¨ê³„: ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§
        unique_boundaries = self._filter_inline_references(unique_boundaries, text)
        
        return unique_boundaries
    
    def _log_boundary_preview(self, text: str, boundaries: List[Tuple[int, str, str]], window: int = 30):
        """ê²½ê³„ ë¯¸ë¦¬ë³´ê¸° ë¡œê¹…"""
        if not boundaries:
            return
        
        previews = []
        for i, (pos, btype, matched) in enumerate(boundaries[:20], 1):
            start = max(0, pos - window)
            end = min(len(text), pos + window)
            
            before = text[start:pos].replace('\n', 'â†µ')
            after = text[pos:end].replace('\n', 'â†µ')
            
            previews.append(f"   [{i:02d}:{btype:8s}] ...{before}âŸ¨{matched}âŸ©{after}...")
        
        logger.info("   ğŸ” ê²½ê³„ ë¯¸ë¦¬ë³´ê¸°:")
        for preview in previews:
            logger.info(preview)
    
    def _is_header_chunk(self, content: str, btype: str) -> bool:
        """í—¤ë” ì²­í¬ íŒì • ê°•í™”"""
        # âœ… P0-2: basic íƒ€ì… í—¤ë” ì¸ì‹
        if btype in ('basic', 'chapter', 'supplement'):
            return True
        
        s = content.lstrip()
        if s.startswith(('ì œ', 'ë¶€ì¹™', 'â… ', 'â…¡', 'â…¢', '#', '**', 'ê¸°ë³¸ì •ì‹ ', 'ê¸°ë³¸ ì •ì‹ ')):
            return True
        
        if len(content) < 50 and btype == 'article':
            return True
        
        return False
    
    def _post_merge_small_fragments(
        self,
        chunks: List[Dict],
        target_size: int = 800,
        min_len: int = 150
    ) -> List[Dict]:
        """ì ì‘í˜• ë³‘í•© (í—¤ë” ì ˆëŒ€ ë³´í˜¸)"""
        if not chunks:
            return chunks
        
        merged = []
        fragment_count = 0
        
        for chunk in chunks:
            char_count = chunk['metadata']['char_count']
            content = chunk['content']
            btype = chunk['metadata']['type']
            
            is_curr_header = self._is_header_chunk(content, btype)
            
            should_merge = False
            if merged:
                prev_content = merged[-1]['content']
                prev_btype = merged[-1]['metadata']['type']
                is_prev_header = self._is_header_chunk(prev_content, prev_btype)
                
                should_merge = (
                    char_count < min_len
                    and not is_curr_header
                    and not is_prev_header
                    and (merged[-1]['metadata']['char_count'] + char_count <= int(target_size * 1.2))
                    and (merged[-1]['metadata'].get('merged_count', 0) < 1)
                )
            
            if should_merge:
                merged[-1]['content'] += '\n' + chunk['content']
                merged[-1]['metadata']['char_count'] += char_count
                merged[-1]['metadata']['merged_count'] = merged[-1]['metadata'].get('merged_count', 0) + 1
                fragment_count += 1
                logger.debug(f"      ğŸ§© íŒŒí¸ ë³‘í•©: {chunk['metadata']['boundary']} ({char_count}ì)")
            else:
                merged.append(chunk)
        
        if fragment_count > 0:
            logger.info(f"   ğŸ§© íŒŒí¸ ë³‘í•©: {fragment_count}ê°œ")
        
        for i, chunk in enumerate(merged, 1):
            chunk['metadata']['chunk_index'] = i
        
        return merged
    
    def _extract_headers_from_md(self, text: str) -> Set[str]:
        """Markdownì—ì„œ ì¡°ë¬¸ í—¤ë” ì¶”ì¶œ"""
        headers = set()
        
        patterns = [
            re.compile(r'ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?'),
            re.compile(r'ì œ\s*\d+\s*ì¥'),
            # âœ… P0-2: ê¸°ë³¸ì •ì‹ ë„ í—¤ë”ë¡œ ì¸ì‹
            re.compile(r'ê¸°ë³¸ì •ì‹ |ê¸°ë³¸\s*ì •ì‹ '),
        ]
        
        for pattern in patterns:
            for match in pattern.finditer(text):
                headers.add(match.group(0).strip())
        
        return headers
    
    def _qa_check_missing_headers(self, text: str, chunks: List[Dict]) -> None:
        """ìë™ QA ê²Œì´íŠ¸ - ëˆ„ë½ ì¡°ë¬¸ ê°ì§€"""
        md_headers = self._extract_headers_from_md(text)
        
        chunk_headers = set()
        for chunk in chunks:
            boundary = chunk['metadata'].get('boundary', '')
            btype = chunk['metadata'].get('type', '')
            
            # âœ… P0-2: basic íƒ€ì…ë„ í—¤ë”ë¡œ ì¶”ê°€
            if btype == 'basic':
                chunk_headers.add('ê¸°ë³¸ì •ì‹ ')
            
            if boundary:
                match = re.match(r'(ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?)', boundary)
                if match:
                    chunk_headers.add(match.group(1).strip())
                
                match = re.match(r'(ì œ\s*\d+\s*ì¥)', boundary)
                if match:
                    chunk_headers.add(match.group(1).strip())
        
        missing = md_headers - chunk_headers
        
        if missing:
            logger.warning(f"   âš ï¸ ëˆ„ë½ëœ ì¡°ë¬¸ ê°ì§€: {sorted(missing)}")
            logger.warning(f"   â†’ JSON ì²­í¬ì—ì„œ ë‹¤ìŒ ì¡°ë¬¸ì´ ë¹ ì¡ŒìŠµë‹ˆë‹¤!")
        else:
            logger.info(f"   âœ… QA í†µê³¼: ëª¨ë“  ì¡°ë¬¸ì´ JSONì— í¬í•¨ë¨")
    
    def chunk(
        self,
        text: str,
        target_size: int = 800,
        min_size: int = 50,
        max_size: int = 1500
    ) -> List[Dict[str, Any]]:
        """ì²­í‚¹ ì‹¤í–‰ + ìë™ QA"""
        if not text or len(text) < min_size:
            raise ValueError(f"ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(text)}ì)")
        
        logger.info(f"âœ‚ï¸ ì²­í‚¹ ì‹œì‘: {len(text)}ì")
        
        # 1. ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬
        text = self._pre_normalize(text)
        logger.info("   ğŸ”§ ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬ ì™„ë£Œ")
        
        # 2. Dual-Rail ê²½ê³„ íƒì§€ + ê¸°ë³¸ì •ì‹  ìš°ì„ 
        boundaries = self._find_boundaries(text)
        
        if not boundaries:
            logger.warning("   âš ï¸ íŒ¨í„´ ë¯¸ê²€ì¶œ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        logger.info(f"   ğŸ“‹ ìœ íš¨ ê²½ê³„: {len(boundaries)}ê°œ")
        
        if logger.level <= logging.INFO:
            self._log_boundary_preview(text, boundaries)
        
        # 3. ì²­í¬ ìƒì„±
        chunks = []
        for i, (start, btype, matched) in enumerate(boundaries):
            end = boundaries[i + 1][0] if i + 1 < len(boundaries) else len(text)
            
            content = text[start:end].strip()
            
            if len(content) < min_size and not self._is_header_chunk(content, btype):
                continue
            
            # ì œëª© ì¶”ì¶œ
            if btype == 'basic':
                title = 'ê¸°ë³¸ì •ì‹ '
            else:
                title_match = re.search(r'ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?\s*\(([^)]+)\)', content)
                title = title_match.group(1) if title_match else None
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': btype,
                    'boundary': matched,
                    'title': title,
                    'char_count': len(content),
                    'chunk_index': len(chunks) + 1
                }
            })
        
        if not chunks:
            logger.warning("   âš ï¸ ë¹ˆ ê²°ê³¼ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # 4. ì ì‘í˜• ë³‘í•©
        chunks = self._post_merge_small_fragments(chunks, target_size=target_size, min_len=150)
        
        logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ ë¶„í¬
        type_counts = {}
        for chunk in chunks:
            chunk_type = chunk['metadata']['type']
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        logger.info(f"   ğŸ“Š íƒ€ì… ë¶„í¬: {dict(type_counts)}")
        
        # âœ… P0-2: basic íƒ€ì… ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if 'basic' in type_counts:
            logger.info(f"   âœ… ê¸°ë³¸ì •ì‹  ì²­í¬ ë³´ì¡´: {type_counts['basic']}ê°œ")
        else:
            logger.warning(f"   âš ï¸ ê¸°ë³¸ì •ì‹  ì²­í¬ ì—†ìŒ (MDì— ì—†ê±°ë‚˜ ì¸ì‹ ì‹¤íŒ¨)")
        
        loose_count = type_counts.get('article_loose', 0)
        loose_ratio = loose_count / len(chunks) if chunks else 0
        
        if loose_ratio > 0.3:
            logger.warning(f"   âš ï¸ article_loose ë¹„ìœ¨ ë†’ìŒ: {loose_ratio:.1%}")
        else:
            logger.info(f"   âœ… article_loose ë¹„ìœ¨ ì–‘í˜¸: {loose_ratio:.1%}")
        
        # 5. ìë™ QA
        self._qa_check_missing_headers(text, chunks)
        
        return chunks
    
    def _fallback_chunk(
        self,
        text: str,
        target: int,
        min_len: int,
        max_len: int
    ) -> List[Dict[str, Any]]:
        """ê¸¸ì´ ê¸°ë°˜ í˜ì¼ì„¸ì´í”„ ì²­í‚¹"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + target, len(text))
            
            if end < len(text):
                for sep in ['\n\n', '\n', '. ', 'ã€‚']:
                    last_sep = text.rfind(sep, start, end)
                    if last_sep > start:
                        end = last_sep + len(sep)
                        break
            
            content = text[start:end].strip()
            
            if len(content) >= min_len:
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'fallback',
                        'boundary': 'length-based',
                        'title': None,
                        'char_count': len(content),
                        'chunk_index': len(chunks) + 1
                    }
                })
            
            start = end
        
        logger.info(f"   âš ï¸ Fallback ì²­í‚¹: {len(chunks)}ê°œ")
        return chunks