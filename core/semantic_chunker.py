"""
core/semantic_chunker.py
PRISM Phase 5.2 - Semantic Chunker (ì§€ëŠ¥í˜• ì²­í‚¹)
"""

import re
import logging
from typing import List, Dict, Tuple

logger = logging.getLogger(__name__)


class SemanticChunker:
    """
    Phase 5.2: ì˜ë¯¸ ë‹¨ìœ„ ê¸°ë°˜ ì§€ëŠ¥í˜• ì²­í‚¹
    
    ëª©í‘œ:
    - ì²­í¬ í¬ê¸°: 800~1,000ì (ìµœì )
    - ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´: 98%
    - í‘œ ì™„ê²°ì„±: 100%
    """
    
    def __init__(self, 
                 min_chunk_size: int = 600,
                 max_chunk_size: int = 1200,
                 target_chunk_size: int = 900):
        """
        Args:
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸° (ê¸°ë³¸ 600ì)
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ê¸°ë³¸ 1200ì)
            target_chunk_size: ëª©í‘œ ì²­í¬ í¬ê¸° (ê¸°ë³¸ 900ì)
        """
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.target_chunk_size = target_chunk_size
        logger.info(f"âœ… SemanticChunker ì´ˆê¸°í™”: {min_chunk_size}~{max_chunk_size}ì")
    
    def chunk_markdown(self, markdown: str, page_num: int = None) -> List[Dict]:
        """
        Markdownì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
        
        Args:
            markdown: ì²­í‚¹í•  Markdown í…ìŠ¤íŠ¸
            page_num: í˜ì´ì§€ ë²ˆí˜¸ (ì„ íƒ)
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        """
        logger.info(f"ğŸ¯ ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹ ì‹œì‘: {len(markdown)}ì")
        
        # Step 1: í—¤ë” ê¸°ì¤€ ì„¹ì…˜ ë¶„í• 
        sections = self._split_by_headers(markdown)
        logger.info(f"  ğŸ“„ ì„¹ì…˜ ë¶„í• : {len(sections)}ê°œ")
        
        # Step 2: í‘œ ë³´í˜¸ (ë¶„ë¦¬ë˜ì§€ ì•Šë„ë¡)
        sections = self._protect_tables(sections)
        logger.info(f"  ğŸ“Š í‘œ ë³´í˜¸ ì™„ë£Œ")
        
        # Step 3: ì²­í¬ í¬ê¸° ìµœì í™”
        chunks = self._optimize_chunk_size(sections)
        logger.info(f"  ğŸ“¦ ì²­í¬ ìµœì í™”: {len(chunks)}ê°œ")
        
        # Step 4: ë©”íƒ€ë°ì´í„° ì¶”ê°€
        chunked_data = self._add_metadata(chunks, page_num)
        
        # í†µê³„ ì¶œë ¥
        avg_size = sum(c['size'] for c in chunked_data) / len(chunked_data) if chunked_data else 0
        logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunked_data)}ê°œ ì²­í¬, í‰ê·  {avg_size:.0f}ì")
        
        return chunked_data
    
    def _split_by_headers(self, markdown: str) -> List[str]:
        """
        í—¤ë”(##) ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
        
        í—¤ë” ìš°ì„ ìˆœìœ„:
        1. ## (H2) - ì£¼ìš” ì„¹ì…˜
        2. ### (H3) - í•˜ìœ„ ì„¹ì…˜
        3. #### (H4) - ì„¸ë¶€ ì„¹ì…˜
        """
        sections = []
        current_section = []
        
        lines = markdown.split('\n')
        
        for line in lines:
            # H2 í—¤ë” ê°ì§€ (ì£¼ìš” ì„¹ì…˜ êµ¬ë¶„)
            if re.match(r'^##\s+', line):
                if current_section:
                    sections.append('\n'.join(current_section))
                current_section = [line]
            else:
                current_section.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€
        if current_section:
            sections.append('\n'.join(current_section))
        
        return sections
    
    def _protect_tables(self, sections: List[str]) -> List[str]:
        """
        í‘œê°€ ì²­í¬ ê²½ê³„ì—ì„œ ë¶„ë¦¬ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸
        
        í‘œ ê°ì§€:
        - Markdown í‘œ: | ë¡œ ì‹œì‘í•˜ëŠ” ì—°ì†ëœ ë¼ì¸
        - í‘œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ì·¨ê¸‰
        """
        protected_sections = []
        
        for section in sections:
            # í‘œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            has_table = '|' in section and section.count('|') > 3
            
            if has_table:
                # í‘œê°€ ìˆëŠ” ì„¹ì…˜ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ
                protected_sections.append(section)
            else:
                # í‘œê°€ ì—†ëŠ” ì„¹ì…˜ì€ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í•  ê°€ëŠ¥
                protected_sections.append(section)
        
        return protected_sections
    
    def _optimize_chunk_size(self, sections: List[str]) -> List[str]:
        """
        ì²­í¬ í¬ê¸°ë¥¼ 800~1,000ìë¡œ ìµœì í™”
        
        ì „ëµ:
        1. ë„ˆë¬´ ì‘ì€ ì„¹ì…˜ (<600ì): ë‹¤ìŒ ì„¹ì…˜ê³¼ ë³‘í•©
        2. ì ì • í¬ê¸° (600~1,200ì): ê·¸ëŒ€ë¡œ ìœ ì§€
        3. ë„ˆë¬´ í° ì„¹ì…˜ (>1,200ì): ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• 
        """
        optimized = []
        buffer = ""
        
        for section in sections:
            section_size = len(section)
            
            if not section.strip():
                continue
            
            # Case 1: ë„ˆë¬´ ì‘ì€ ì„¹ì…˜ - ë²„í¼ì— ëˆ„ì 
            if section_size < self.min_chunk_size:
                if not buffer:
                    buffer = section
                else:
                    # ë²„í¼ì™€ ë³‘í•©í–ˆì„ ë•Œ maxë¥¼ ë„˜ì§€ ì•Šìœ¼ë©´ ë³‘í•©
                    if len(buffer) + len(section) + 2 <= self.max_chunk_size:
                        buffer += "\n\n" + section
                    else:
                        # ë²„í¼ flush
                        optimized.append(buffer)
                        buffer = section
            
            # Case 2: ì ì • í¬ê¸° - ê·¸ëŒ€ë¡œ ì¶”ê°€
            elif section_size <= self.max_chunk_size:
                # ë²„í¼ê°€ ìˆìœ¼ë©´ ë¨¼ì € flush
                if buffer:
                    optimized.append(buffer)
                    buffer = ""
                optimized.append(section)
            
            # Case 3: ë„ˆë¬´ í° ì„¹ì…˜ - ë¶„í• 
            else:
                # ë²„í¼ê°€ ìˆìœ¼ë©´ ë¨¼ì € flush
                if buffer:
                    optimized.append(buffer)
                    buffer = ""
                
                # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
                sub_chunks = self._split_large_section(section)
                optimized.extend(sub_chunks)
        
        # ë§ˆì§€ë§‰ ë²„í¼ flush
        if buffer:
            optimized.append(buffer)
        
        return optimized
    
    def _split_large_section(self, section: str) -> List[str]:
        """
        í° ì„¹ì…˜ì„ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
        
        ë¶„í•  ê¸°ì¤€:
        1. ë¹ˆ ì¤„ (\n\n)
        2. ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ
        3. í‘œ
        """
        chunks = []
        current_chunk = []
        current_size = 0
        
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
        paragraphs = section.split('\n\n')
        
        for para in paragraphs:
            para_size = len(para)
            
            if current_size + para_size + 2 <= self.max_chunk_size:
                current_chunk.append(para)
                current_size += para_size + 2
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = [para]
                current_size = para_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    def _add_metadata(self, chunks: List[str], page_num: int = None) -> List[Dict]:
        """
        ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        
        ë©”íƒ€ë°ì´í„°:
        - chunk_id: ì²­í¬ ë²ˆí˜¸
        - content: ì²­í¬ ë‚´ìš©
        - size: ì²­í¬ í¬ê¸°
        - section: ì„¹ì…˜ ì´ë¦„
        - page_num: í˜ì´ì§€ ë²ˆí˜¸ (ìˆëŠ” ê²½ìš°)
        - prev_chunk_id: ì´ì „ ì²­í¬ ID
        - next_chunk_id: ë‹¤ìŒ ì²­í¬ ID
        """
        chunked_data = []
        
        for i, chunk in enumerate(chunks):
            # ì„¹ì…˜ ì´ë¦„ ì¶”ì¶œ (ì²« ë²ˆì§¸ í—¤ë”)
            section_name = self._extract_section_name(chunk)
            
            metadata = {
                'chunk_id': i,
                'content': chunk,
                'size': len(chunk),
                'section': section_name,
                'page_num': page_num,
                'prev_chunk_id': i - 1 if i > 0 else None,
                'next_chunk_id': i + 1 if i < len(chunks) - 1 else None
            }
            
            chunked_data.append(metadata)
        
        return chunked_data
    
    def _extract_section_name(self, chunk: str) -> str:
        """ì²­í¬ì—ì„œ ì²« ë²ˆì§¸ í—¤ë” ì¶”ì¶œ"""
        lines = chunk.split('\n')
        for line in lines:
            if line.startswith('#'):
                # í—¤ë” ê¸°í˜¸ ì œê±°
                return line.lstrip('#').strip()
        return "Untitled Section"
    
    def get_statistics(self, chunked_data: List[Dict]) -> Dict:
        """
        ì²­í‚¹ í†µê³„ ê³„ì‚°
        
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if not chunked_data:
            return {}
        
        sizes = [c['size'] for c in chunked_data]
        
        stats = {
            'total_chunks': len(chunked_data),
            'avg_chunk_size': sum(sizes) / len(sizes),
            'min_chunk_size': min(sizes),
            'max_chunk_size': max(sizes),
            'target_achievement': sum(
                1 for s in sizes 
                if self.min_chunk_size <= s <= self.max_chunk_size
            ) / len(sizes) * 100
        }
        
        return stats