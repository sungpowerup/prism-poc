"""
core/semantic_chunker.py
PRISM Phase 0.3.4 P2 - Semantic Chunker (ì¡°ë¬¸ íŒ¨í„´ ê°•í™”)

âœ… Phase 0.3.4 P2 ê¸´ê¸‰ ìˆ˜ì •:
1. ì¡°ë¬¸ íŒ¨í„´ ê°•í™” (ì œ1~200ì¡° ëª¨ë‘ ë§¤ì¹­)
2. ë‹¤ì–‘í•œ í—¤ë”© ë ˆë²¨ ì§€ì› (# ## ### ####)
3. ì¡°ë¬¸ ëˆ„ë½ ë°©ì§€ ë¡œì§
4. ì²­í¬=0 í•˜ë“œ ì‹¤íŒ¨ ìœ ì§€
âš ï¸ GPT í”¼ë“œë°± í•µì‹¬:
"ì œ1~6ì¡°, ì œ8ì¡° í†µì§¸ë¡œ ì²­í¬ ëˆ„ë½ â†’ ì¡°ë¬¸ íŒ¨í„´ ë¯¸ìŠ¤ë§¤ì¹˜"

Author: ë°•ì¤€í˜¸ (AI/ML Lead) + ë§ˆì°½ìˆ˜ì‚° íŒ€
Date: 2025-11-08
Version: Phase 0.3.4 P2
"""

import re
import logging
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)


class SemanticChunker:
    """
    Phase 0.3.4 P2 ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì—”ì§„
    
    âœ… Phase 0.3.4 P2 ê°œì„ :
    - ì¡°ë¬¸ íŒ¨í„´ ê°•í™” (ëˆ„ë½ ë°©ì§€)
    - ì²­í¬=0 í•˜ë“œ ì‹¤íŒ¨ ìœ ì§€
    - Fallback ê¸¸ì´ ê¸°ë°˜ ì²­í‚¹ ì˜ë¬´í™”
    """
    
    VERSION = "Phase 0.3.4 P2"
    
    # âœ… P2: ê°•í™”ëœ ì¡°ë¬¸ íŒ¨í„´
    ARTICLE_PATTERNS = [
        # íŒ¨í„´ 1: ### ì œ1ì¡°(ëª©ì )
        r'(#{1,4}\s*ì œ\d+ì¡°[^#]*?)(?=#{1,4}\s*ì œ\d+ì¡°|$)',
        # íŒ¨í„´ 2: **ì œ1ì¡°(ëª©ì )**
        r'(\*\*ì œ\d+ì¡°[^*]*?\*\*[^*]*?)(?=\*\*ì œ\d+ì¡°|$)',
        # íŒ¨í„´ 3: ì œ1ì¡°(ëª©ì ) (í—¤ë”© ì—†ìŒ)
        r'(^ì œ\d+ì¡°[^\n]*?\n[\s\S]*?)(?=^ì œ\d+ì¡°|\Z)',
    ]
    
    # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
    KOREAN_SENTENCE_ENDINGS = [
        r'ë‹¤\.',  # ~í•˜ë‹¤.
        r'ë‹¤\)',  # ~í•œë‹¤)
        r'ë‹¤\s*$',  # ~í•œë‹¤
        r'í•¨\.',  # ~í•¨.
        r'ë¨\.',  # ~ë¨.
        r'ì„\.',  # ~ì„.
        r'<\d{4}\.\s*\d{1,2}\.\s*\d{1,2}>',  # <2024.1.1>
        r'\d+\.\)',  # 1.)
        r'í˜¸\s*$',  # ~í˜¸
        r'í•œë‹¤\.',  # ~í•œë‹¤.
    ]
    
    # ë¶ˆì™„ì „ ì¢…ê²° íŒ¨í„´
    INCOMPLETE_ENDINGS = [
        r'ì˜\s*$', r'ë¥¼\s*$', r'ì„\s*$', r'ê°€\s*$',
        r'ì—\s*$', r'ì™€\s*$', r'ë¡œ\s*$', r'ì±„\s*$',
    ]
    
    def __init__(self, target_size: int = 800, min_size: int = 300):
        """ì´ˆê¸°í™”"""
        self.target_size = target_size
        self.min_size = min_size
        
        self.sentence_patterns = [re.compile(p) for p in self.KOREAN_SENTENCE_ENDINGS]
        self.incomplete_patterns = [re.compile(p) for p in self.INCOMPLETE_ENDINGS]
        
        # âœ… P2: ì¡°ë¬¸ íŒ¨í„´ ì»´íŒŒì¼
        self.article_patterns = [
            re.compile(p, re.MULTILINE | re.DOTALL) 
            for p in self.ARTICLE_PATTERNS
        ]
        
        logger.info(f"âœ… SemanticChunker {self.VERSION} ì´ˆê¸°í™”")
        logger.info(f"   ğŸ¯ ëª©í‘œ: {target_size}ì, ìµœì†Œ: {min_size}ì")
        logger.info(f"   ğŸ“‹ ì¡°ë¬¸ íŒ¨í„´: {len(self.article_patterns)}ê°œ")
        logger.info(f"   ğŸš« ì²­í¬=0 â†’ ì˜ˆì™¸ ë°œìƒ (í•˜ë“œ ì‹¤íŒ¨)")
    
    def chunk(self, text: str, doc_type: str = 'statute') -> List[Dict[str, Any]]:
        """
        âœ… P2: í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            doc_type: ë¬¸ì„œ íƒ€ì…
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        
        Raises:
            RuntimeError: ì²­í¬=0ì¸ ê²½ìš°
        """
        logger.info(f"   âœ‚ï¸ SemanticChunker {self.VERSION} ì‹œì‘")
        
        # ì…ë ¥ ê²€ì¦
        if not text or len(text.strip()) < 10:
            error_msg = f"âŒ ì²­í‚¹ ì‹¤íŒ¨: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(text)}ì)"
            logger.error(f"      {error_msg}")
            raise RuntimeError(error_msg)
        
        # 1. ê¸°ë³¸ ì²­í‚¹
        chunks = self._basic_chunk(text, doc_type)
        logger.info(f"      ê¸°ë³¸ ì²­í‚¹: {len(chunks)}ê°œ")
        
        # âœ… P2: ì²­í¬=0 Fallback
        if not chunks:
            logger.warning(f"      âš ï¸ ê¸°ë³¸ ì²­í‚¹ ì‹¤íŒ¨ â†’ Fallback ê¸¸ì´ ê¸°ë°˜ ì²­í‚¹")
            chunks = self._fallback_length_based_chunk(text)
            logger.info(f"      Fallback ì²­í‚¹: {len(chunks)}ê°œ")
        
        # 2. ê²½ê³„ ê²€ì¦ + ë³‘í•©
        validated = self._validate_boundaries(chunks)
        logger.info(f"      ê²½ê³„ ê²€ì¦: {len(validated)}ê°œ")
        
        # 3. ì¡°ë¬¸ ì •ë ¬ (statuteë§Œ)
        if doc_type == 'statute' and validated:
            sorted_chunks = self._sort_by_article(validated)
        else:
            sorted_chunks = validated
        
        # âœ… P2: ìµœì¢… ì²­í¬=0 í•˜ë“œ ì‹¤íŒ¨
        if not sorted_chunks:
            error_msg = "âŒ ì²­í‚¹ í•˜ë“œ ì‹¤íŒ¨: 0ê°œ ì²­í¬ ìƒì„± (Fallbackë„ ì‹¤íŒ¨)"
            logger.error(f"      {error_msg}")
            logger.error(f"      ì…ë ¥ ê¸¸ì´: {len(text)}ì")
            logger.error(f"      ë¬¸ì„œ íƒ€ì…: {doc_type}")
            raise RuntimeError(error_msg)
        
        logger.info(f"   âœ… ì²­í‚¹ ì™„ë£Œ: {len(sorted_chunks)}ê°œ")
        
        return sorted_chunks
    
    def _basic_chunk(self, text: str, doc_type: str) -> List[Dict[str, Any]]:
        """
        âœ… P2: ê¸°ë³¸ ì²­í‚¹ (ê°•í™”ëœ ì¡°ë¬¸ íŒ¨í„´)
        """
        chunks = []
        
        if doc_type == 'statute':
            # âœ… P2: ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„
            matches = []
            
            for pattern in self.article_patterns:
                found = pattern.findall(text)
                if found:
                    matches.extend(found)
                    logger.info(f"      ğŸ“‹ íŒ¨í„´ ë§¤ì¹­: {len(found)}ê°œ")
            
            # ì¤‘ë³µ ì œê±° (ê°™ì€ ì¡°ë¬¸ì„ ì—¬ëŸ¬ íŒ¨í„´ì´ ì¡ì„ ìˆ˜ ìˆìŒ)
            seen_articles = set()
            unique_matches = []
            
            for match in matches:
                # ì¡°ë¬¸ ë²ˆí˜¸ ì¶”ì¶œ
                article_match = re.search(r'ì œ(\d+)ì¡°', match)
                if article_match:
                    article_num = article_match.group(1)
                    
                    if article_num not in seen_articles:
                        seen_articles.add(article_num)
                        unique_matches.append(match)
            
            logger.info(f"      ğŸ“‹ ê³ ìœ  ì¡°ë¬¸: {len(unique_matches)}ê°œ")
            
            # ì²­í¬ ìƒì„±
            for match in unique_matches:
                article_match = re.search(r'ì œ(\d+)ì¡°(?:ì˜(\d+))?', match)
                article_no = article_match.group(0) if article_match else ''
                
                title_match = re.search(r'ì œ\d+ì¡°(?:ì˜\d+)?\s*\(([^)]+)\)', match)
                article_title = title_match.group(1) if title_match else ''
                
                # ì •ë¦¬
                content = match.strip()
                
                # í—¤ë”© ë§ˆì»¤ ì œê±° (ì¤‘ë³µ ë°©ì§€)
                content = re.sub(r'^#{1,4}\s*', '', content, flags=re.MULTILINE)
                content = re.sub(r'\*\*', '', content)
                
                chunks.append({
                    'content': content,
                    'metadata': {
                        'article_no': article_no,
                        'article_title': article_title,
                        'char_count': len(content),
                        'chunk_index': len(chunks) + 1
                    }
                })
        
        else:
            # ì¼ë°˜ ë¬¸ì„œ: ë¬¸ì¥ ë‹¨ìœ„
            sentences = re.split(r'(?<=[.!?])\s+', text)
            current_chunk = []
            current_size = 0
            
            for sentence in sentences:
                if current_size + len(sentence) > self.target_size and current_chunk:
                    chunk_text = ' '.join(current_chunk)
                    chunks.append({
                        'content': chunk_text,
                        'metadata': {
                            'article_no': '',
                            'article_title': '',
                            'char_count': len(chunk_text),
                            'chunk_index': len(chunks) + 1
                        }
                    })
                    current_chunk = []
                    current_size = 0
                
                current_chunk.append(sentence)
                current_size += len(sentence)
            
            if current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunks.append({
                    'content': chunk_text,
                    'metadata': {
                        'article_no': '',
                        'article_title': '',
                        'char_count': len(chunk_text),
                        'chunk_index': len(chunks) + 1
                    }
                })
        
        return chunks
    
    def _fallback_length_based_chunk(self, text: str) -> List[Dict[str, Any]]:
        """
        âœ… P2: Fallback ê¸¸ì´ ê¸°ë°˜ ì²­í‚¹ (ì˜ë¬´)
        """
        logger.warning("      ğŸ”§ Fallback ê¸¸ì´ ê¸°ë°˜ ì²­í‚¹ ì‹œì‘")
        
        chunks = []
        text_length = len(text)
        
        start = 0
        chunk_index = 1
        
        while start < text_length:
            end = min(start + self.target_size, text_length)
            
            # ë¬¸ì¥ ì¤‘ê°„ì´ë©´ ë‹¤ìŒ ë§ˆì¹¨í‘œê¹Œì§€ í™•ì¥
            if end < text_length:
                search_end = min(end + 200, text_length)
                text_segment = text[end:search_end]
                
                period_match = re.search(r'[.!?]\s', text_segment)
                if period_match:
                    end += period_match.end()
            
            chunk_text = text[start:end].strip()
            
            # ìµœì†Œ í¬ê¸° ì²´í¬
            if len(chunk_text) >= self.min_size or start == 0:
                chunks.append({
                    'content': chunk_text,
                    'metadata': {
                        'article_no': '',
                        'article_title': '',
                        'char_count': len(chunk_text),
                        'chunk_index': chunk_index,
                        'fallback': True
                    }
                })
                chunk_index += 1
            
            start = end
        
        logger.info(f"      âœ… Fallback ì²­í‚¹: {len(chunks)}ê°œ ìƒì„±")
        
        return chunks
    
    def _validate_boundaries(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê²½ê³„ ê²€ì¦ + ìë™ ë³‘í•©"""
        validated = []
        i = 0
        
        while i < len(chunks):
            chunk = chunks[i]
            content = chunk['content']
            
            is_complete = self._is_sentence_boundary(content)
            
            if not is_complete and i + 1 < len(chunks):
                # ë¶ˆì™„ì „ â†’ ë³‘í•©
                logger.warning(f"      âš ï¸ ë¶ˆì™„ì „ ì²­í¬: '{content[-50:]}'")
                next_chunk = chunks[i + 1]
                
                merged_content = content + '\n\n' + next_chunk['content']
                merged_chunk = {
                    'content': merged_content,
                    'metadata': {
                        'article_no': chunk['metadata']['article_no'] or next_chunk['metadata']['article_no'],
                        'article_title': chunk['metadata']['article_title'] or next_chunk['metadata']['article_title'],
                        'char_count': len(merged_content),
                        'chunk_index': len(validated) + 1,
                        'merged': True
                    }
                }
                
                validated.append(merged_chunk)
                logger.info(f"      âœ… ìë™ ë³‘í•©: Chunk {i+1} + {i+2}")
                i += 2
            else:
                validated.append(chunk)
                i += 1
        
        return validated
    
    def _is_sentence_boundary(self, text: str) -> bool:
        """ë¬¸ì¥ ê²½ê³„ ê²€ì‚¬"""
        text_end = text.strip()[-100:]
        
        # ì™„ì „í•œ ì¢…ê²° í™•ì¸
        for pattern in self.sentence_patterns:
            if pattern.search(text_end):
                return True
        
        # ë¶ˆì™„ì „ ì¢…ê²° í™•ì¸
        for pattern in self.incomplete_patterns:
            if pattern.search(text_end):
                return False
        
        return True
    
    def _sort_by_article(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¡°ë¬¸ ë²ˆí˜¸ ê¸°ì¤€ ì •ë ¬"""
        def extract_article_number(chunk: Dict[str, Any]) -> Tuple[int, int]:
            article_no = chunk['metadata'].get('article_no', '')
            
            if not article_no:
                return (999, 0)
            
            main_match = re.search(r'ì œ(\d+)ì¡°', article_no)
            sub_match = re.search(r'ì œ\d+ì¡°ì˜(\d+)', article_no)
            
            main_num = int(main_match.group(1)) if main_match else 999
            sub_num = int(sub_match.group(1)) if sub_match else 0
            
            return (main_num, sub_num)
        
        sorted_chunks = sorted(chunks, key=extract_article_number)
        
        # chunk_index ì¬ë¶€ì—¬
        for i, chunk in enumerate(sorted_chunks, 1):
            chunk['metadata']['chunk_index'] = i
        
        return sorted_chunks