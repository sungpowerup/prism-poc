"""
core/semantic_chunker.py
PRISM Phase 0.3.4 P2.5.1 - Dual-Rail Hotfix

âœ… ë³€ê²½ì‚¬í•­ (ë§ˆì°½ìˆ˜ì‚°íŒ€ ì£¼ë„ ì„¤ê³„):
1. ë“€ì–¼ íŒ¨í„´ (Strict + Loose)
2. ê°•í™”ëœ ë¼ì¸ë¸Œë ˆì´í¬ (ëª¨ë“  ì¼€ì´ìŠ¤ ì»¤ë²„)
3. ì ì‘í˜• ë³‘í•© (í—¤ë” ì ˆëŒ€ ë³´í˜¸)
4. ìƒì„¸ ë¡œê¹… (ë””ë²„ê¹… ìš©ì´)

ëª©í‘œ: ì²­í¬ 10~14ê°œ, ì¡°ë¬¸ ê°ì§€ 15~20ê°œ
Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ (ë°•ì¤€í˜¸ AI/ML Lead)
Date: 2025-11-12
Version: Phase 0.3.4 P2.5.1
"""

import re
import logging
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)


class SemanticChunker:
    """Phase 0.3.4 P2.5.1 ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (Dual-Rail Hotfix)"""
    
    # ìœ ì—°í•œ ìˆ«ì íŒ¨í„´ (raw string)
    NUM = r'(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)'
    
    # âœ… Dual-Rail íŒ¨í„´
    ARTICLE_STRICT = re.compile(
        r'(?m)^(?:#{0,6}\s*)ì œ\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)\s*ì¡°'
        r'(?:\s*ì˜\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+))?\s*\(',
        re.IGNORECASE
    )
    
    ARTICLE_LOOSE = re.compile(
        r'(?m)^(?:#{0,6}\s*)ì œ\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)\s*ì¡°'
        r'(?:\s*ì˜\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+))?'
        r'(?!\s*\d)',  # ë’¤ì— ìˆ«ì ê¸ˆì§€ (ì œ28ì¡°ì œ2í•­ ì œì™¸)
        re.IGNORECASE
    )
    
    CHAPTER = re.compile(
        r'(?m)^(?:#{0,6}\s*)ì œ\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)\s*ì¥',
        re.IGNORECASE
    )
    
    BASIC = re.compile(
        r'(?m)^(?:#{0,6}\s*)ê¸°\s*ë³¸\s*ì •\s*ì‹ |^ê¸°ë³¸ì •ì‹ ',
        re.IGNORECASE
    )
    
    SUPPLEMENT = re.compile(
        r'(?m)^(?:#{0,6}\s*)ë¶€\s*ì¹™',
        re.IGNORECASE
    )
    
    ROMAN = re.compile(r'(?m)^[â… -â…¨]+\.', re.IGNORECASE)
    
    PRIORITY = {
        'basic': 1,
        'chapter': 2,
        'supplement': 3,
        'article': 4,
        'article_loose': 4,  # ê°™ì€ ìš°ì„ ìˆœìœ„
        'roman': 5
    }
    
    def __init__(self):
        logger.info("âœ… SemanticChunker Phase 0.3.4 P2.5.1 ì´ˆê¸°í™” (Dual-Rail)")
        logger.info("   ğŸ¯ ë“€ì–¼ íŒ¨í„´: Strict + Loose")
        logger.info("   ğŸ”§ ê°•í™”ëœ ë¼ì¸ë¸Œë ˆì´í¬ + ì ì‘í˜• ë³‘í•©")
    
    def _pre_normalize(self, text: str) -> str:
        """
        âœ… ê°•í™”ëœ ë¼ì¸ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬ (ì •ê·œì‹ ì˜¤ë¥˜ ìˆ˜ì •)
        
        ì²˜ë¦¬ ìˆœì„œ:
        1. í•œ ì¤„ì— ë¶™ì€ ì¥+ì¡° ë¶„ë¦¬
        2. ì¡° í—¤ë” ì•ì— ê°•ì œ ê°œí–‰
        3. ë³¼ë“œ/í—¤ë”© í˜•íƒœ ì²˜ë¦¬
        4. ë¶€ì¹™ ì²˜ë¦¬
        """
        # 1. í•œ ì¤„ì— ë¶™ì€ ì¥+ì¡° ë¶„ë¦¬
        # "ì œ1ì¥ ì´ì¹™ ì œ1ì¡°(ëª©ì )" â†’ "ì œ1ì¥ ì´ì¹™\nì œ1ì¡°(ëª©ì )"
        # âœ… look-behind ëŒ€ì‹  look-ahead ì‚¬ìš©
        text = re.sub(
            r'(ì¥[^\n]{1,40})\s+(ì œ\s*\d+\s*ì¡°)',
            r'\1\n\2',
            text
        )
        
        # 2. ì¡° í—¤ë” ì•ì— ê°•ì œ ê°œí–‰ (ê´„í˜¸ ìˆëŠ” ê²½ìš°)
        # "...í•œë‹¤. ì œ1ì¡°(ëª©ì ) ì´ ê·œì •ì€" â†’ "...í•œë‹¤.\nì œ1ì¡°(ëª©ì ) ì´ ê·œì •ì€"
        # âœ… look-behind ì œê±°
        text = re.sub(
            r'([^\n])(\s*ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?\s*\()',
            r'\1\n\2',
            text
        )
        
        # 3. ì¡° í—¤ë” ì•ì— ê°•ì œ ê°œí–‰ (ê´„í˜¸ ì—†ëŠ” ê²½ìš°ë„)
        # âœ… look-behind ì œê±°
        text = re.sub(
            r'([^\n])(\s*ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?(?!\d))',
            r'\1\n\2',
            text
        )
        
        # 4. í—¤ë”© ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬
        # âœ… look-behind ì œê±°
        text = re.sub(
            r'([^\n])\s*(#{1,6}\s*ì œ\s*\d+\s*[ì¥ì¡°][^\n]*)',
            r'\1\n\2',
            text
        )
        
        # 5. ë³¼ë“œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        # âœ… look-behind ì œê±°
        text = re.sub(
            r'([^\n])\s*(\*\*\s*ì œ\s*\d+\s*ì¡°[^*]*\*\*)',
            r'\1\n\2',
            text
        )
        
        # 6. ì¥ ì•ì— ì¤„ë°”ê¿ˆ
        # âœ… look-behind ì œê±°
        text = re.sub(
            r'([^\n])\s*(ì œ\s*\d+\s*ì¥)',
            r'\1\n\2',
            text
        )
        
        # 7. ë¶€ì¹™ ì•ì— ì¤„ë°”ê¿ˆ
        # âœ… look-behind ì œê±°
        text = re.sub(
            r'([^\n])\s*(ë¶€\s*ì¹™)',
            r'\1\n\2',
            text
        )
        
        # 8. ë¶ˆê·œì¹™ ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 9. ì—°ì† ê°œí–‰ ì •ë¦¬ (ìµœëŒ€ 2ê°œ)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text
    
    def _find_boundaries(self, text: str) -> List[Tuple[int, str, str]]:
        """
        âœ… Dual-Rail ê²½ê³„ íƒì§€
        
        1ë‹¨ê³„: Strict íŒ¨í„´ (ê´„í˜¸ í•„ìˆ˜)
        2ë‹¨ê³„: ì¡°ë¬¸ < 12ê°œë©´ Loose ë³´ê°•
        
        Returns:
            List of (position, type, matched_text)
        """
        boundaries = []
        
        # 1ë‹¨ê³„: ê¸°ë³¸ì •ì‹ , ì¥, ì¡°(Strict), ë¶€ì¹™
        for pattern, pattern_type in [
            (self.BASIC, 'basic'),
            (self.CHAPTER, 'chapter'),
            (self.ARTICLE_STRICT, 'article'),
            (self.SUPPLEMENT, 'supplement'),
            (self.ROMAN, 'roman')
        ]:
            for match in pattern.finditer(text):
                boundaries.append((
                    match.start(),
                    pattern_type,
                    match.group(0).strip()
                ))
        
        # ì¡°ë¬¸ ê°œìˆ˜ í™•ì¸
        article_count = sum(1 for _, t, _ in boundaries if t == 'article')
        
        logger.info(f"   ğŸ” 1ë‹¨ê³„ (Strict): ì¡°ë¬¸ {article_count}ê°œ")
        
        # 2ë‹¨ê³„: ì¡°ë¬¸ < 12ê°œë©´ Loose ë³´ê°•
        if article_count < 12:
            logger.info(f"   ğŸ” ì¡°ë¬¸ ë¶€ì¡± â†’ Loose íŒ¨í„´ ë³´ê°•")
            
            for match in self.ARTICLE_LOOSE.finditer(text):
                pos = match.start()
                matched = match.group(0).strip()
                
                # ì¤‘ë³µ ì²´í¬ (ê°™ì€ ìœ„ì¹˜ì— ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
                if not any(b[0] == pos for b in boundaries):
                    boundaries.append((pos, 'article_loose', matched))
            
            article_count = sum(1 for _, t, _ in boundaries if t in ('article', 'article_loose'))
            logger.info(f"   âœ… 2ë‹¨ê³„ (Loose): ì¡°ë¬¸ {article_count}ê°œ")
        
        # ìœ„ì¹˜ ê¸°ì¤€ ì •ë ¬
        boundaries.sort(key=lambda x: (x[0], self.PRIORITY[x[1]]))
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ ìœ„ì¹˜ëŠ” ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒë§Œ)
        unique_boundaries = []
        last_pos = -1
        for b in boundaries:
            if b[0] != last_pos:
                unique_boundaries.append(b)
                last_pos = b[0]
        
        return unique_boundaries
    
    def _log_boundary_preview(self, text: str, boundaries: List[Tuple[int, str, str]], window: int = 30):
        """ê²½ê³„ ë¯¸ë¦¬ë³´ê¸° ë¡œê¹… (ë””ë²„ê¹…ìš©)"""
        if not boundaries:
            return
        
        previews = []
        for i, (pos, btype, matched) in enumerate(boundaries[:20], 1):  # ìµœëŒ€ 20ê°œ
            start = max(0, pos - window)
            end = min(len(text), pos + window)
            
            before = text[start:pos].replace('\n', 'â†µ')
            after = text[pos:end].replace('\n', 'â†µ')
            
            previews.append(f"   [{i:02d}:{btype:8s}] ...{before}âŸ¨{matched}âŸ©{after}...")
        
        logger.info("   ğŸ” ê²½ê³„ ë¯¸ë¦¬ë³´ê¸°:")
        for preview in previews:
            logger.info(preview)
    
    def _post_merge_small_fragments(
        self,
        chunks: List[Dict],
        target_size: int = 800,
        min_len: int = 150
    ) -> List[Dict]:
        """
        âœ… ì ì‘í˜• ë³‘í•© (í—¤ë” ì ˆëŒ€ ë³´í˜¸)
        
        ê·œì¹™:
        1. í—¤ë” ì²­í¬ëŠ” ì ˆëŒ€ ë³‘í•© ì•ˆ í•¨
        2. ì—°ì† 1íšŒê¹Œì§€ë§Œ ë³‘í•©
        3. í•©ì‚° í¬ê¸° ì œí•œ (target_size * 1.2)
        """
        if not chunks:
            return chunks
        
        merged = []
        fragment_count = 0
        
        for chunk in chunks:
            char_count = chunk['metadata']['char_count']
            content = chunk['content'].lstrip()
            
            # í—¤ë” íŒì • (ë” ë„“ì€ ë²”ìœ„)
            is_header = (
                content.startswith(('ì œ', 'ë¶€ì¹™', 'â… ', 'â…¡', 'â…¢', '#', '**', 'ê¸°ë³¸ì •ì‹ '))
                or chunk['metadata']['type'] in ('basic', 'chapter', 'supplement')
            )
            
            # ë³‘í•© ì¡°ê±´ ì²´í¬
            should_merge = (
                merged  # ì• ì²­í¬ ì¡´ì¬
                and char_count < min_len  # ì§§ì€ ì²­í¬
                and not is_header  # í—¤ë” ì•„ë‹˜
                and not merged[-1]['content'].lstrip().startswith(('ì œ', 'ë¶€ì¹™', 'â… ', 'â…¡', 'â…¢', '#', '**', 'ê¸°ë³¸ì •ì‹ '))  # ì•ë„ í—¤ë” ì•„ë‹˜
                and (merged[-1]['metadata']['char_count'] + char_count <= int(target_size * 1.2))  # í¬ê¸° ì œí•œ
                and (merged[-1]['metadata'].get('merged_count', 0) < 1)  # ì—°ì† 1íšŒê¹Œì§€
            )
            
            if should_merge:
                # ì• ì²­í¬ì— ë³‘í•©
                merged[-1]['content'] += '\n' + chunk['content']
                merged[-1]['metadata']['char_count'] += char_count
                merged[-1]['metadata']['merged_count'] = merged[-1]['metadata'].get('merged_count', 0) + 1
                fragment_count += 1
                logger.debug(f"      ğŸ§© íŒŒí¸ ë³‘í•©: {chunk['metadata']['boundary']} ({char_count}ì)")
            else:
                merged.append(chunk)
        
        if fragment_count > 0:
            logger.info(f"   ğŸ§© íŒŒí¸ ë³‘í•©: {fragment_count}ê°œ")
        
        # ì¸ë±ìŠ¤ ì¬ì •ë ¬
        for i, chunk in enumerate(merged, 1):
            chunk['metadata']['chunk_index'] = i
        
        return merged
    
    def chunk(
        self,
        text: str,
        target_size: int = 800,
        min_size: int = 50,
        max_size: int = 1500
    ) -> List[Dict[str, Any]]:
        """ì²­í‚¹ ì‹¤í–‰"""
        if not text or len(text) < min_size:
            raise ValueError(f"ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(text)}ì)")
        
        logger.info(f"âœ‚ï¸ ì²­í‚¹ ì‹œì‘: {len(text)}ì")
        
        # 1. ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬
        text = self._pre_normalize(text)
        logger.info("   ğŸ”§ ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬ ì™„ë£Œ")
        
        # 2. Dual-Rail ê²½ê³„ íƒì§€
        boundaries = self._find_boundaries(text)
        
        if not boundaries:
            logger.warning("   âš ï¸ íŒ¨í„´ ë¯¸ê²€ì¶œ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        logger.info(f"   ğŸ“‹ ìœ íš¨ ê²½ê³„: {len(boundaries)}ê°œ")
        
        # ë””ë²„ê¹…: ê²½ê³„ ë¯¸ë¦¬ë³´ê¸° (ì²« 20ê°œ)
        if logger.level <= logging.INFO:
            self._log_boundary_preview(text, boundaries)
        
        # 3. ì²­í¬ ìƒì„±
        chunks = []
        for i, (start, btype, matched) in enumerate(boundaries):
            end = boundaries[i + 1][0] if i + 1 < len(boundaries) else len(text)
            
            content = text[start:end].strip()
            
            if len(content) < min_size:
                continue
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(r'ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?\s*\(([^)]+)\)', content)
            title = title_match.group(1) if title_match else None
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': btype,
                    'boundary': matched,
                    'title': title,
                    'char_count': len(content),
                    'chunk_index': len(chunks) + 1
                }
            })
        
        if not chunks:
            logger.warning("   âš ï¸ ë¹ˆ ê²°ê³¼ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # 4. ì ì‘í˜• ë³‘í•©
        chunks = self._post_merge_small_fragments(chunks, target_size=target_size, min_len=150)
        
        logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ ë¶„í¬ ë¡œê¹…
        type_counts = {}
        for chunk in chunks:
            chunk_type = chunk['metadata']['type']
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        logger.info(f"   ğŸ“Š íƒ€ì… ë¶„í¬: {dict(type_counts)}")
        
        return chunks
    
    def _fallback_chunk(
        self,
        text: str,
        target: int,
        min_len: int,
        max_len: int
    ) -> List[Dict[str, Any]]:
        """ê¸¸ì´ ê¸°ë°˜ í˜ì¼ì„¸ì´í”„ ì²­í‚¹"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + target, len(text))
            
            # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
            if end < len(text):
                for sep in ['\n\n', '\n', '. ', 'ã€‚']:
                    last_sep = text.rfind(sep, start, end)
                    if last_sep > start:
                        end = last_sep + len(sep)
                        break
            
            content = text[start:end].strip()
            
            if len(content) >= min_len:
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'fallback',
                        'boundary': 'length-based',
                        'title': None,
                        'char_count': len(content),
                        'chunk_index': len(chunks) + 1
                    }
                })
            
            start = end
        
        logger.info(f"   âš ï¸ Fallback ì²­í‚¹: {len(chunks)}ê°œ")
        return chunks