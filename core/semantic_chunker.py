"""
core/semantic_chunker.py
PRISM Phase 0.4.0 P0-3.1 - Hotfix (í—¤ë” íŒ¨í„´ í†µí•© + ê¸°ë³¸ì •ì‹  ê°•í™”)

âœ… P0-3.1 ê¸´ê¸‰ ìˆ˜ì •:
1. DualQAì™€ ë™ì¼í•œ í—¤ë” íŒ¨í„´ ì‚¬ìš© (9ê°œ ì¡°ë¬¸ â†’ 9ê°œ ì²­í¬)
2. ê¸°ë³¸ì •ì‹  íŒ¨í„´ ê°•í™” (ëª¨ë“  ë³€í˜• ì»¤ë²„)
3. ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§ ìœ ì§€

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT í”¼ë“œë°± ë°˜ì˜
Date: 2025-11-13
Version: Phase 0.4.0 P0-3.1
"""

import re
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


class SemanticChunker:
    """Phase 0.4.0 P0-3.1 ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (DualQA íŒ¨í„´ í†µí•©)"""
    
    # ============================================
    # âœ… P0-3.1: DualQAì™€ ì™„ì „íˆ ë™ì¼í•œ íŒ¨í„´ ì‚¬ìš©
    # ============================================
    NUM = r'\d+(?:ì˜\d+)?'
    
    # Strict: ì œNì¡°( í˜•ì‹ (ì‹¤ì œ í—¤ë”)
    # âœ… DualQAì™€ ë™ì¼: ì•ì— ê³µë°±/íŠ¹ìˆ˜ë¬¸ì í—ˆìš©
    ARTICLE_STRICT = re.compile(
        rf'^[\sâŸ¨<\[]*(ì œ\s*{NUM}\s*ì¡°)\s*\(',
        re.MULTILINE
    )
    
    # Loose: ì œNì¡° ë‹¨ë… (ë°±ì—…)
    # âœ… DualQAì™€ ë™ì¼: ì•ì— ê³µë°±/íŠ¹ìˆ˜ë¬¸ì í—ˆìš©
    ARTICLE_LOOSE = re.compile(
        rf'^[\sâŸ¨<\[]*(ì œ\s*{NUM}\s*ì¡°)(?=\s|$)',
        re.MULTILINE
    )
    
    # ì¥ íŒ¨í„´
    CHAPTER = re.compile(r'^[\sâŸ¨<\[]*(ì œ\s*\d+\s*ì¥)', re.MULTILINE)
    
    # âœ… P0-3.1: ê¸°ë³¸ì •ì‹  íŒ¨í„´ ëŒ€í­ ê°•í™”
    # "ê¸°ë³¸ì •ì‹ ", "ê¸° ë³¸ ì • ì‹ ", "âŸ¨ê¸°ë³¸ì •ì‹ âŸ©", "âŸ¨ê¸° ë³¸ ì • ì‹ âŸ©" ëª¨ë‘ ì»¤ë²„
    BASIC_SPIRIT = re.compile(
        r'[\sâŸ¨<\[]*(ê¸°\s*ë³¸\s*ì •\s*ì‹ )[\sâŸ©>\]]*',
        re.MULTILINE | re.IGNORECASE
    )
    
    def __init__(self, target_size: int = 512, min_size: int = 100, max_size: int = 2048):
        self.target_size = target_size
        self.min_size = min_size
        self.max_size = max_size
        
        logger.info("âœ… SemanticChunker Phase 0.4.0 P0-3.1 ì´ˆê¸°í™” (Hotfix)")
        logger.info("   ğŸ¯ DualQA íŒ¨í„´ í†µí•© + ê¸°ë³¸ì •ì‹  ê°•í™”")
    
    def chunk(self, text: str, target_size: int = None, min_size: int = None, max_size: int = None) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì‹¤í–‰"""
        if not text or not text.strip():
            return []
        
        target_size = target_size or self.target_size
        min_size = min_size or self.min_size
        max_size = max_size or self.max_size
        
        logger.info(f"âœ‚ï¸ ì²­í‚¹ ì‹œì‘: {len(text)}ì")
        
        # ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬
        text = self._preprocess_linebreaks(text)
        logger.info("   ğŸ”§ ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬ ì™„ë£Œ")
        
        # âœ… P0-3.1: ê¸°ë³¸ì •ì‹  ìš°ì„  ê°ì§€ (ê°•í™”ëœ íŒ¨í„´)
        basic_match = self.BASIC_SPIRIT.search(text)
        if basic_match:
            logger.info(f"   ğŸ“– ê¸°ë³¸ì •ì‹  ê°ì§€: {basic_match.group(1)}")
        else:
            logger.warning("   âš ï¸ ê¸°ë³¸ì •ì‹  ë¯¸ê°ì§€ (VLM ì¶”ì¶œ ì‹¤íŒ¨ ê°€ëŠ¥ì„±)")
        
        # ê²½ê³„ ì°¾ê¸°
        boundaries = self._find_boundaries(text)
        
        if not boundaries:
            logger.warning("   âš ï¸ ê²½ê³„ ë¯¸ë°œê²¬ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # âœ… P0-3.1: ê¸°ë³¸ì •ì‹  ê²½ê³„ ì¶”ê°€ (ìµœìš°ì„ )
        if basic_match:
            basic_pos = basic_match.start()
            # ê¸°ë³¸ì •ì‹ ì´ ê²½ê³„ ëª©ë¡ì— ì—†ìœ¼ë©´ ì¶”ê°€
            if not any(b[0] == basic_pos for b in boundaries):
                boundaries.insert(0, (basic_pos, 'basic', 'ê¸°ë³¸ì •ì‹ ', None))
                logger.info("   âœ… ê¸°ë³¸ì •ì‹  ê²½ê³„ ì¶”ê°€")
        
        # ê²½ê³„ ê¸°ë°˜ ì²­í‚¹
        chunks = []
        for i, (pos, btype, matched, title) in enumerate(boundaries):
            next_pos = boundaries[i + 1][0] if i + 1 < len(boundaries) else len(text)
            content = text[pos:next_pos].strip()
            
            if len(content) >= min_size:
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': btype,
                        'boundary': matched,
                        'title': title,
                        'char_count': len(content),
                        'chunk_index': len(chunks) + 1
                    }
                })
        
        if not chunks:
            logger.warning("   âš ï¸ ë¹ˆ ê²°ê³¼ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # íŒŒí¸ ë³‘í•© (200ì ë¯¸ë§Œ)
        chunks = self._post_merge_small_fragments(chunks, target_size=target_size, min_len=200)
        
        logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # íƒ€ì… ë¶„í¬
        type_counts = {}
        for chunk in chunks:
            chunk_type = chunk['metadata']['type']
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        logger.info(f"   ğŸ“Š íƒ€ì… ë¶„í¬: {dict(type_counts)}")
        
        # âœ… P0-3.1: ê¸°ë³¸ì •ì‹  ì²­í¬ ê²€ì¦
        if basic_match and type_counts.get('basic', 0) == 0:
            logger.error("   âŒ ê¸°ë³¸ì •ì‹  ê°ì§€í–ˆìœ¼ë‚˜ ì²­í¬ ìƒì„± ì‹¤íŒ¨!")
        elif type_counts.get('basic', 0) > 0:
            logger.info(f"   âœ… ê¸°ë³¸ì •ì‹  ì²­í¬ ë³´ì¡´: {type_counts['basic']}ê°œ")
        
        # article_loose ë¹„ìœ¨ ëª¨ë‹ˆí„°ë§
        loose_count = type_counts.get('article_loose', 0)
        loose_ratio = loose_count / len(chunks) if chunks else 0
        
        if loose_ratio > 0.3:
            logger.warning(f"   âš ï¸ article_loose ë¹„ìœ¨ ë†’ìŒ: {loose_ratio:.1%}")
        else:
            logger.info(f"   âœ… article_loose ë¹„ìœ¨ ì–‘í˜¸: {loose_ratio:.1%}")
        
        # âœ… P0-3.1: QA ê²€ì¦ (MD vs JSON)
        md_headers = self._extract_headers_for_qa(text)
        json_headers = [c['metadata']['boundary'] for c in chunks if c['metadata']['type'] in ['article', 'article_loose']]
        
        missing_headers = set(md_headers) - set(json_headers)
        
        logger.info(f"   ğŸ“Š QA ê²€ì¦:")
        logger.info(f"      MD í—¤ë”: {len(md_headers)}ê°œ")
        logger.info(f"      JSON í—¤ë”: {len(json_headers)}ê°œ")
        
        if missing_headers:
            logger.error(f"      âŒ ëˆ„ë½: {len(missing_headers)}ê°œ - {list(missing_headers)[:5]}")
        else:
            logger.info(f"      âœ… ëˆ„ë½: 0ê°œ")
        
        return chunks
    
    def _preprocess_linebreaks(self, text: str) -> str:
        """ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬"""
        # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r' {2,}', ' ', text)
        # ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text
    
    def _find_boundaries(self, text: str) -> List[tuple]:
        """ê²½ê³„ ì°¾ê¸° (DualQA íŒ¨í„´ í†µí•©)"""
        boundaries = []
        
        # 1. ê¸°ë³¸ì •ì‹  (ìµœìš°ì„ )
        basic_match = self.BASIC_SPIRIT.search(text)
        if basic_match:
            boundaries.append((basic_match.start(), 'basic', 'ê¸°ë³¸ì •ì‹ ', None))
        
        # 2. Strict ì¡°ë¬¸ (ì œNì¡°( í˜•ì‹)
        strict_articles = set()
        for m in self.ARTICLE_STRICT.finditer(text):
            pos = m.start()
            matched = m.group(1).strip()
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(rf'{re.escape(matched)}\s*\(([^)]+)\)', text[pos:pos+50])
            title = title_match.group(1) if title_match else None
            
            boundaries.append((pos, 'article', matched, title))
            strict_articles.add(matched)
        
        logger.info(f"   ğŸ” 1ë‹¨ê³„ (Strict): ì¡°ë¬¸ {len(strict_articles)}ê°œ")
        
        # 3. Loose ì¡°ë¬¸ ë³´ê°• (Strictì— ì—†ëŠ” ê²ƒë§Œ)
        if len(strict_articles) < 5:
            logger.info("   ğŸ” ì¡°ë¬¸ ë¶€ì¡± â†’ Loose íŒ¨í„´ ë³´ê°•")
            
            loose_candidates = []
            for m in self.ARTICLE_LOOSE.finditer(text):
                matched = m.group(1).strip()
                if matched not in strict_articles:
                    loose_candidates.append((m.start(), matched))
            
            # âœ… ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§
            loose_candidates = self._filter_inline_references(text, loose_candidates)
            
            for pos, matched in loose_candidates:
                boundaries.append((pos, 'article_loose', matched, None))
            
            logger.info(f"   âœ… 2ë‹¨ê³„ (Loose): ì¡°ë¬¸ {len(strict_articles) + len(loose_candidates)}ê°œ")
            logger.info(f"   ğŸ—‘ï¸ ì¸ë¼ì¸ ì°¸ì¡° ì œê±°: {len(list(self.ARTICLE_LOOSE.finditer(text))) - len(loose_candidates)}ê°œ")
        
        # 4. ì¥
        for m in self.CHAPTER.finditer(text):
            boundaries.append((m.start(), 'chapter', m.group(1).strip(), None))
        
        # ì •ë ¬
        boundaries.sort(key=lambda x: x[0])
        
        # ìœ íš¨ì„± ê²€ì¦
        boundaries = [b for b in boundaries if b[0] < len(text)]
        
        logger.info(f"   ğŸ“‹ ìœ íš¨ ê²½ê³„: {len(boundaries)}ê°œ")
        
        # ê²½ê³„ ë¯¸ë¦¬ë³´ê¸°
        if boundaries:
            logger.info("   ğŸ” ê²½ê³„ ë¯¸ë¦¬ë³´ê¸°:")
            for i, (pos, btype, matched, title) in enumerate(boundaries[:10]):
                preview = text[max(0, pos-20):pos+80].replace('\n', 'â†µ')
                logger.info(f"   [{i+1:02d}:{btype:8s}] ...{preview}...")
        
        return boundaries
    
    def _filter_inline_references(self, text: str, candidates: List[tuple]) -> List[tuple]:
        """ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§"""
        filtered = []
        
        for pos, matched in candidates:
            # ì•ë’¤ 50ì ì»¨í…ìŠ¤íŠ¸
            start = max(0, pos - 50)
            end = min(len(text), pos + len(matched) + 50)
            context = text[start:end]
            
            # ì¸ë¼ì¸ ì°¸ì¡° íŒ¨í„´ ê°ì§€
            is_inline = False
            
            # íŒ¨í„´ 1: "ì œNì¡°ì œMí•­" (ì¡°ë¬¸ ì°¸ì¡°)
            if re.search(r'ì œ\d+ì¡°ì œ\d+[í•­í˜¸]', context):
                is_inline = True
            
            # íŒ¨í„´ 2: "ì œNì¡° ë° ì œMì¡°" (ë‚˜ì—´)
            if re.search(r'ì œ\d+ì¡°\s*[ë°ê³¼]\s*ì œ\d+ì¡°', context):
                is_inline = True
            
            # íŒ¨í„´ 3: ë¬¸ì¥ ì¤‘ê°„ (ì•ì— í•œê¸€ì´ ë°”ë¡œ ë¶™ìŒ)
            if pos > 0 and re.match(r'[ê°€-í£]', text[pos-1]):
                is_inline = True
            
            if not is_inline:
                filtered.append((pos, matched))
        
        return filtered
    
    def _extract_headers_for_qa(self, text: str) -> List[str]:
        """QAìš© ì¡°ë¬¸ í—¤ë” ì¶”ì¶œ (DualQAì™€ ë™ì¼)"""
        headers = set()
        
        # Strict íŒ¨í„´
        for m in self.ARTICLE_STRICT.finditer(text):
            headers.add(m.group(1).strip())
        
        # Loose íŒ¨í„´
        for m in self.ARTICLE_LOOSE.finditer(text):
            matched = m.group(1).strip()
            # ì¸ë¼ì¸ ì°¸ì¡° ì œì™¸
            pos = m.start()
            if pos == 0 or text[pos-1] in ['\n', ' ', 'âŸ¨', '<', '[']:
                headers.add(matched)
        
        return sorted(headers)
    
    def _post_merge_small_fragments(self, chunks: List[Dict], target_size: int = 512, min_len: int = 200) -> List[Dict]:
        """200ì ë¯¸ë§Œ íŒŒí¸ ë³‘í•©"""
        if not chunks:
            return chunks
        
        merged = []
        i = 0
        
        while i < len(chunks):
            current = chunks[i]
            
            # 200ì ì´ìƒì´ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
            if len(current['content']) >= min_len:
                merged.append(current)
                i += 1
                continue
            
            # 200ì ë¯¸ë§Œì´ë©´ ì• ì²­í¬ì— ë³‘í•©
            if merged:
                merged[-1]['content'] += '\n\n' + current['content']
                merged[-1]['metadata']['char_count'] = len(merged[-1]['content'])
                logger.info(f"   ğŸ§© íŒŒí¸ ë³‘í•©: {len(current['content'])}ì â†’ ì• ì²­í¬")
            else:
                # ì²« ì²­í¬ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
                merged.append(current)
            
            i += 1
        
        return merged
    
    def _fallback_chunk(self, text: str, target: int, min_len: int, max_len: int) -> List[Dict[str, Any]]:
        """ê¸¸ì´ ê¸°ë°˜ í˜ì¼ì„¸ì´í”„"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + target, len(text))
            
            if end < len(text):
                for sep in ['\n\n', '\n', '. ', 'ã€‚']:
                    last_sep = text.rfind(sep, start, end)
                    if last_sep > start:
                        end = last_sep + len(sep)
                        break
            
            content = text[start:end].strip()
            
            if len(content) >= min_len:
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'fallback',
                        'boundary': 'length-based',
                        'title': None,
                        'char_count': len(content),
                        'chunk_index': len(chunks) + 1
                    }
                })
            
            start = end
        
        logger.info(f"   âš ï¸ Fallback ì²­í‚¹: {len(chunks)}ê°œ")
        return chunks