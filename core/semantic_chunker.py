"""
core/semantic_chunker.py
PRISM Phase 0.4.0 P0-3a - QA í—¤ë” ì¶”ì¶œ ì •êµí™”

âœ… GPT í”¼ë“œë°± ë°˜ì˜:
1. ì¸ë¼ì¸ ì°¸ì¡° ë…¸ì´ì¦ˆ ì™„ì „ ì œê±°
2. ì²­í‚¹ ê²½ê³„ íŒ¨í„´ê³¼ QA í—¤ë” ì¶”ì¶œ í†µí•©
3. "ì§„ì§œ í—¤ë”"ë§Œ QA ëŒ€ìƒìœ¼ë¡œ

Author: ì •ìˆ˜ì•„ (QA Lead) + GPT ë³´ì •
Date: 2025-11-13
Version: Phase 0.4.0 P0-3a
"""

import re
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


class SemanticChunker:
    """Phase 0.4.0 P0-3a ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (QA ì •êµí™”)"""
    
    # ============================================
    # ì¡°ë¬¸ í—¤ë” íŒ¨í„´ (ì²­í‚¹ + QA ê³µí†µ ì‚¬ìš©)
    # ============================================
    NUM = r'\d+(?:ì˜\d+)?'
    AFTER_JO_NOT_NUM = r'(?!\s*ì œ?\s*\d)'
    
    # Strict: ì œNì¡°( í˜•ì‹ (ì‹¤ì œ í—¤ë”)
    ARTICLE_STRICT = re.compile(
        rf'^(ì œ\s*{NUM}\s*ì¡°){AFTER_JO_NOT_NUM}(?=\s*\()',
        re.MULTILINE
    )
    
    # Loose: ì œNì¡° ë‹¨ë… (ë°±ì—…)
    ARTICLE_LOOSE = re.compile(
        rf'^(ì œ\s*{NUM}\s*ì¡°){AFTER_JO_NOT_NUM}(?=\s|$)',
        re.MULTILINE
    )
    
    # ì¥ íŒ¨í„´
    CHAPTER = re.compile(r'^(ì œ\s*\d+\s*ì¥)', re.MULTILINE)
    
    # âœ… GPT í•µì‹¬: ê¸°ë³¸ì •ì‹  ìš°ì„  íƒì§€
    BASIC_SPIRIT = re.compile(r'(?:^|\n)(ê¸°ë³¸\s*ì •ì‹ )(?:\s|$)', re.MULTILINE)
    
    def __init__(self, target_size: int = 512, min_size: int = 100, max_size: int = 2048):
        self.target_size = target_size
        self.min_size = min_size
        self.max_size = max_size
        
        logger.info("âœ… SemanticChunker Phase 0.4.0 P0-3a ì´ˆê¸°í™” (QA ì •êµí™”)")
        logger.info("   ğŸ¯ ì¸ë¼ì¸ ì°¸ì¡° ë…¸ì´ì¦ˆ ì œê±° + í—¤ë” ì¶”ì¶œ ì •êµí™”")
    
    def chunk(self, text: str, target_size: int = None, min_size: int = None, max_size: int = None) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì‹¤í–‰"""
        if not text or not text.strip():
            return []
        
        target_size = target_size or self.target_size
        min_size = min_size or self.min_size
        max_size = max_size or self.max_size
        
        logger.info(f"âœ‚ï¸ ì²­í‚¹ ì‹œì‘: {len(text)}ì")
        
        # ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬
        text = self._preprocess_linebreaks(text)
        logger.info("   ğŸ”§ ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬ ì™„ë£Œ")
        
        # âœ… ê¸°ë³¸ì •ì‹  ìš°ì„  ê°ì§€
        has_basic_spirit = bool(self.BASIC_SPIRIT.search(text))
        if has_basic_spirit:
            logger.info(f"   ğŸ“– ê¸°ë³¸ì •ì‹  ê°ì§€: ê¸°ë³¸ì •ì‹ ")
        
        # ê²½ê³„ ì°¾ê¸°
        boundaries = self._find_boundaries(text)
        
        if not boundaries:
            logger.warning("   âš ï¸ ë¹ˆ ê²°ê³¼ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # ì²­í¬ ìƒì„±
        chunks = []
        for i, (pos, btype, matched, title) in enumerate(boundaries):
            if i == len(boundaries) - 1:
                content = text[pos:].strip()
            else:
                next_pos = boundaries[i + 1][0]
                content = text[pos:next_pos].strip()
            
            if not content:
                continue
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': btype,
                    'boundary': matched,
                    'title': title,
                    'char_count': len(content),
                    'chunk_index': len(chunks) + 1
                }
            })
        
        if not chunks:
            logger.warning("   âš ï¸ ë¹ˆ ê²°ê³¼ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # ì‘ì€ ì²­í¬ ë³‘í•©
        chunks = self._post_merge_small_fragments(chunks, target_size=target_size, min_len=150)
        
        logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # íƒ€ì… ë¶„í¬
        type_counts = {}
        for chunk in chunks:
            chunk_type = chunk['metadata']['type']
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        logger.info(f"   ğŸ“Š íƒ€ì… ë¶„í¬: {dict(type_counts)}")
        
        # ê¸°ë³¸ì •ì‹  ë³´ì¡´ í™•ì¸
        if has_basic_spirit:
            basic_count = type_counts.get('basic', 0)
            logger.info(f"   âœ… ê¸°ë³¸ì •ì‹  ì²­í¬ ë³´ì¡´: {basic_count}ê°œ")
        
        # article_loose ë¹„ìœ¨ ëª¨ë‹ˆí„°ë§
        loose_count = type_counts.get('article_loose', 0)
        loose_ratio = loose_count / len(chunks) if chunks else 0
        
        if loose_ratio > 0.3:
            logger.warning(f"   âš ï¸ article_loose ë¹„ìœ¨ ë†’ìŒ: {loose_ratio:.1%}")
        else:
            logger.info(f"   âœ… article_loose ë¹„ìœ¨ ì–‘í˜¸: {loose_ratio:.1%}")
        
        # âœ… GPT í•µì‹¬: QA ê²€ì¦ (ì •êµí™”ëœ í—¤ë” ì¶”ì¶œ)
        self._validate_chunks(text, chunks)
        
        return chunks
    
    def _preprocess_linebreaks(self, text: str) -> str:
        """ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬"""
        # ì¡°ë¬¸ í—¤ë” ì•ì— ì¤„ë°”ê¿ˆ í™•ë³´
        text = re.sub(
            r'([ã€‚\.])(\s*)(ì œ\s*\d+ì¡°)',
            r'\1\n\n\3',
            text
        )
        
        # ì¥ ì•ì— ì¤„ë°”ê¿ˆ
        text = re.sub(
            r'([ã€‚\.])(\s*)(ì œ\s*\d+\s*ì¥)',
            r'\1\n\n\3',
            text
        )
        
        return text
    
    def _find_boundaries(self, text: str) -> List[tuple]:
        """ì²­í‚¹ ê²½ê³„ ì°¾ê¸°"""
        boundaries = []
        
        # 1. ê¸°ë³¸ì •ì‹  (ìµœìš°ì„ )
        for m in self.BASIC_SPIRIT.finditer(text):
            boundaries.append((m.start(), 'basic', m.group(1), 'ê¸°ë³¸ì •ì‹ '))
        
        # 2. Strict ì¡°ë¬¸
        strict_articles = set()
        for m in self.ARTICLE_STRICT.finditer(text):
            pos = m.start()
            matched = m.group(1).strip()
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(rf'{re.escape(matched)}\s*\(([^)]+)\)', text[pos:pos+50])
            title = title_match.group(1) if title_match else None
            
            boundaries.append((pos, 'article', matched, title))
            strict_articles.add(matched)
        
        logger.info(f"   ğŸ” 1ë‹¨ê³„ (Strict): ì¡°ë¬¸ {len(strict_articles)}ê°œ")
        
        # 3. Loose ì¡°ë¬¸ ë³´ê°• (Strictì— ì—†ëŠ” ê²ƒë§Œ)
        if len(strict_articles) < 5:
            logger.info("   ğŸ” ì¡°ë¬¸ ë¶€ì¡± â†’ Loose íŒ¨í„´ ë³´ê°•")
            
            loose_candidates = []
            for m in self.ARTICLE_LOOSE.finditer(text):
                matched = m.group(1).strip()
                if matched not in strict_articles:
                    loose_candidates.append((m.start(), matched))
            
            # âœ… GPT í•µì‹¬: ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§
            loose_candidates = self._filter_inline_references(text, loose_candidates)
            
            for pos, matched in loose_candidates:
                boundaries.append((pos, 'article', matched, None))
            
            logger.info(f"   âœ… 2ë‹¨ê³„ (Loose): ì¡°ë¬¸ {len(strict_articles) + len(loose_candidates)}ê°œ")
            logger.info(f"   ğŸ—‘ï¸ ì¸ë¼ì¸ ì°¸ì¡° ì œê±°: {len(list(self.ARTICLE_LOOSE.finditer(text))) - len(loose_candidates)}ê°œ")
        
        # 4. ì¥
        for m in self.CHAPTER.finditer(text):
            boundaries.append((m.start(), 'chapter', m.group(1).strip(), None))
        
        # ì •ë ¬
        boundaries.sort(key=lambda x: x[0])
        
        # ìœ íš¨ì„± ê²€ì¦
        boundaries = [b for b in boundaries if b[0] < len(text)]
        
        logger.info(f"   ğŸ“‹ ìœ íš¨ ê²½ê³„: {len(boundaries)}ê°œ")
        
        # ê²½ê³„ ë¯¸ë¦¬ë³´ê¸°
        if boundaries:
            logger.info("   ğŸ” ê²½ê³„ ë¯¸ë¦¬ë³´ê¸°:")
            for i, (pos, btype, matched, title) in enumerate(boundaries[:10]):
                preview = text[max(0, pos-20):pos+80].replace('\n', 'â†µ')
                logger.info(f"   [{i+1:02d}:{btype:8s}] ...{preview}...")
        
        return boundaries
    
    def _filter_inline_references(self, text: str, candidates: List[tuple]) -> List[tuple]:
        """
        âœ… GPT í•µì‹¬: ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§
        
        ì œ28ì¡°, ì œ34ì¡° ê°™ì€ ë³¸ë¬¸ ë‚´ ì°¸ì¡°ë¥¼ ì œê±°
        ì§„ì§œ í—¤ë”ë§Œ ë‚¨ê¹€
        """
        filtered = []
        
        for pos, matched in candidates:
            # ì „í›„ 50ì ì»¨í…ìŠ¤íŠ¸
            start = max(0, pos - 50)
            end = min(len(text), pos + 100)
            context = text[start:end]
            
            # ì¸ë¼ì¸ ì°¸ì¡° íŒ¨í„´
            inline_patterns = [
                rf'{re.escape(matched)}\s*ì œ\s*\d+í•­',      # ì œ73ì¡°ì œ1í•­
                rf'{re.escape(matched)}\s*ì—\s*ë”°ë¥¸',       # ì œ34ì¡°ì— ë”°ë¥¸
                rf'{re.escape(matched)}\s*ë°',              # ì œ41ì¡° ë°
                rf'{re.escape(matched)}\s*ë˜ëŠ”',            # ì œ28ì¡° ë˜ëŠ”
                rf'{re.escape(matched)}\s*ì˜\s*ê·œì •',       # ì œ35ì¡°ì˜ ê·œì •
            ]
            
            is_inline = any(re.search(p, context) for p in inline_patterns)
            
            if not is_inline:
                filtered.append((pos, matched))
        
        return filtered
    
    def _post_merge_small_fragments(self, chunks: List[Dict], target_size: int = 512, min_len: int = 150) -> List[Dict]:
        """ì‘ì€ íŒŒí¸ ë³‘í•©"""
        if not chunks:
            return chunks
        
        merged = []
        i = 0
        merge_count = 0
        
        while i < len(chunks):
            current = chunks[i]
            current_len = len(current['content'])
            
            # min_len ì´ìƒì´ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
            if current_len >= min_len:
                merged.append(current)
                i += 1
                continue
            
            # ë§ˆì§€ë§‰ ì²­í¬ë©´ ì´ì „ê³¼ ë³‘í•©
            if i == len(chunks) - 1:
                if merged:
                    merged[-1]['content'] += '\n\n' + current['content']
                    merged[-1]['metadata']['char_count'] = len(merged[-1]['content'])
                    merge_count += 1
                else:
                    merged.append(current)
                i += 1
                continue
            
            # ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•©
            next_chunk = chunks[i + 1]
            if current_len + len(next_chunk['content']) <= target_size * 1.5:
                next_chunk['content'] = current['content'] + '\n\n' + next_chunk['content']
                next_chunk['metadata']['char_count'] = len(next_chunk['content'])
                merge_count += 1
                i += 1
            else:
                merged.append(current)
                i += 1
        
        if merge_count > 0:
            logger.info(f"   ğŸ§© íŒŒí¸ ë³‘í•©: {merge_count}ê°œ")
        
        return merged
    
    def _validate_chunks(self, markdown: str, chunks: List[Dict]) -> None:
        """
        âœ… GPT í•µì‹¬: QA ê²€ì¦ (ì •êµí™”ëœ í—¤ë” ì¶”ì¶œ)
        
        ì²­í‚¹ ê²½ê³„ íŒ¨í„´ê³¼ ë™ì¼í•œ ì •ê·œì‹ ì‚¬ìš©
        ì¸ë¼ì¸ ì°¸ì¡°ëŠ” QA ëŒ€ìƒì—ì„œ ì œì™¸
        """
        # âœ… ê°œì„ : Strict íŒ¨í„´ìœ¼ë¡œë§Œ í—¤ë” ì¶”ì¶œ (ì¸ë¼ì¸ ì°¸ì¡° ì œì™¸)
        md_headers = set()
        
        # Strict ì¡°ë¬¸ í—¤ë”ë§Œ ì¶”ì¶œ
        for m in self.ARTICLE_STRICT.finditer(markdown):
            header = m.group(1).strip()
            header = re.sub(r'\s+', '', header)  # ê³µë°± ì œê±°
            md_headers.add(header)
        
        # Loose ì¡°ë¬¸ë„ ì¶”ì¶œ (ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§)
        loose_matches = []
        for m in self.ARTICLE_LOOSE.finditer(markdown):
            pos = m.start()
            header = m.group(1).strip()
            header = re.sub(r'\s+', '', header)
            
            if header not in md_headers:
                loose_matches.append((pos, header))
        
        # ì¸ë¼ì¸ ì°¸ì¡° í•„í„°ë§
        loose_matches = self._filter_inline_references(markdown, loose_matches)
        for _, header in loose_matches:
            md_headers.add(header)
        
        # JSON ì²­í¬ í—¤ë” ì¶”ì¶œ
        json_headers = set()
        for chunk in chunks:
            if chunk['metadata']['type'] in ['article', 'article_loose']:
                header = chunk['metadata']['boundary']
                header = re.sub(r'\s+', '', header)
                json_headers.add(header)
        
        # ëˆ„ë½ ê²€ì¦
        missing = md_headers - json_headers
        
        if missing:
            logger.warning(f"   âš ï¸ ëˆ„ë½ëœ ì¡°ë¬¸ ê°ì§€: {sorted(missing)}")
            logger.warning(f"   â†’ JSON ì²­í¬ì—ì„œ ë‹¤ìŒ ì¡°ë¬¸ì´ ë¹ ì¡ŒìŠµë‹ˆë‹¤!")
        
        # í†µê³„
        logger.info(f"   ğŸ“Š QA ê²€ì¦:")
        logger.info(f"      MD í—¤ë”: {len(md_headers)}ê°œ")
        logger.info(f"      JSON í—¤ë”: {len(json_headers)}ê°œ")
        logger.info(f"      ëˆ„ë½: {len(missing)}ê°œ")
    
    def _fallback_chunk(self, text: str, target: int, min_len: int, max_len: int) -> List[Dict[str, Any]]:
        """ê¸¸ì´ ê¸°ë°˜ í˜ì¼ì„¸ì´í”„"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + target, len(text))
            
            if end < len(text):
                for sep in ['\n\n', '\n', '. ', 'ã€‚']:
                    last_sep = text.rfind(sep, start, end)
                    if last_sep > start:
                        end = last_sep + len(sep)
                        break
            
            content = text[start:end].strip()
            
            if len(content) >= min_len:
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'fallback',
                        'boundary': 'length-based',
                        'title': None,
                        'char_count': len(content),
                        'chunk_index': len(chunks) + 1
                    }
                })
            
            start = end
        
        logger.info(f"   âš ï¸ Fallback ì²­í‚¹: {len(chunks)}ê°œ")
        return chunks