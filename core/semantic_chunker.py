"""
core/semantic_chunker.py
PRISM Phase 0.3.4 P2.4 - ìµœì¢… ì™„ì„± (GPT ì œì•ˆ 100% ë°˜ì˜)

âœ… ë³€ê²½ì‚¬í•­:
1. ê²½ê³„ ì •ë°€ë„ ê°•í™” (ì¡°ë¬¸ ì°¸ì¡° ì˜¤íƒ ë°©ì§€)
2. íŒŒí¸ ë³‘í•© (200ì ë¯¸ë§Œ ì²­í¬ ìë™ ë³‘í•©)
3. ìˆ«ì ë³€ì¢… í™•ëŒ€ (ì œ28ì¡°ì œ2í•­ ì˜¤íƒ ë°©ì§€)
"""

import re
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


class SemanticChunker:
    """Phase 0.3.4 P2.4 ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (ìµœì¢… ì™„ì„±)"""
    
    # ìœ ì—°í•œ ìˆ«ì íŒ¨í„´
    NUM = r'(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[â… -â…«]+|[ï¼-ï¼™]+)'
    
    # GPT ì œì•ˆ: ì¡° ë’¤ì— ìˆ«ì ê¸ˆì§€ (ì œ28ì¡°ì œ2í•­ ì˜¤íƒ ë°©ì§€)
    AFTER_JO_NOT_NUM = r'(?!\s*ì œ?\s*[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åâ… -â…«ï¼-ï¼™])'
    
    # ê·œì • ì „ìš© íŒ¨í„´ (ì •ë°€)
    PATTERNS = {
        'basic': re.compile(r'ê¸°\s*ë³¸\s*ì •\s*ì‹ ', re.IGNORECASE),
        'chapter': re.compile(rf'ì œ\s*{NUM}\s*ì¥', re.IGNORECASE),
        'article': re.compile(rf'^(ì œ\s*{NUM}\s*ì¡°){AFTER_JO_NOT_NUM}(?=\s|\()', re.MULTILINE | re.IGNORECASE),
        'supplement': re.compile(r'ë¶€\s*ì¹™', re.IGNORECASE),
        'roman': re.compile(r'[â… -â…¨]+\.', re.IGNORECASE)
    }
    
    PRIORITY = {
        'basic': 1,
        'chapter': 2,
        'supplement': 3,
        'article': 4,
        'roman': 5
    }
    
    def __init__(self):
        logger.info("âœ… SemanticChunker Phase 0.3.4 P2.4 ì´ˆê¸°í™”")
        logger.info("   ğŸ¯ íŒ¨í„´: 5ê°œ (ìµœì¢… ì™„ì„±)")
        logger.info("   ğŸ”§ ê²½ê³„ ì •ë°€ë„ ê°•í™”")
        logger.info("   ğŸ§© íŒŒí¸ ìë™ ë³‘í•©")
    
    def _pre_normalize(self, text: str) -> str:
        """ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬"""
        # ì¥ ì•ì— ì¤„ë°”ê¿ˆ
        text = re.sub(rf'(ì œ\s*{self.NUM}\s*ì¥)', r'\n\1', text)
        
        # ì¡° ì•ì— ì¤„ë°”ê¿ˆ
        text = re.sub(rf'(ì œ\s*{self.NUM}\s*ì¡°)', r'\n\1', text)
        
        # ë¶€ì¹™ ì•ì— ì¤„ë°”ê¿ˆ
        text = re.sub(r'(ë¶€\s*ì¹™)', r'\n\1', text)
        
        # ë¶ˆê·œì¹™ ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        
        return text
    
    def _post_merge_small_fragments(self, chunks: List[Dict], min_len: int = 200) -> List[Dict]:
        """
        GPT ì œì•ˆ: íŒŒí¸ ë³‘í•©
        
        200ì ë¯¸ë§Œ ì²­í¬ë¥¼ ì• ì²­í¬ì— ë³‘í•©
        """
        if not chunks:
            return chunks
        
        merged = []
        fragment_count = 0
        
        for chunk in chunks:
            char_count = chunk['metadata']['char_count']
            
            # 200ì ë¯¸ë§Œ + article íƒ€ì… â†’ ë³‘í•© í›„ë³´
            if merged and char_count < min_len and chunk['metadata']['type'] == 'article':
                # ì• ì²­í¬ì— ë³‘í•©
                merged[-1]['content'] += '\n' + chunk['content']
                merged[-1]['metadata']['char_count'] += char_count
                fragment_count += 1
                logger.debug(f"   ğŸ§© íŒŒí¸ ë³‘í•©: {chunk['metadata']['boundary']} ({char_count}ì)")
            else:
                merged.append(chunk)
        
        if fragment_count > 0:
            logger.info(f"   ğŸ§© íŒŒí¸ ë³‘í•©: {fragment_count}ê°œ")
        
        # ì¸ë±ìŠ¤ ì¬ì •ë ¬
        for i, chunk in enumerate(merged, 1):
            chunk['metadata']['chunk_index'] = i
        
        return merged
    
    def chunk(self, text: str, target_size: int = 800, min_size: int = 50, max_size: int = 1500) -> List[Dict[str, Any]]:
        """ì²­í‚¹ ì‹¤í–‰"""
        if not text or len(text) < min_size:
            raise ValueError(f"ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(text)}ì)")
        
        logger.info(f"âœ‚ï¸ ì²­í‚¹ ì‹œì‘: {len(text)}ì")
        
        # ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬
        text = self._pre_normalize(text)
        logger.info("   ğŸ”§ ë¼ì¸ ë¸Œë ˆì´í¬ ì „ì²˜ë¦¬ ì™„ë£Œ")
        
        # 1. ëª¨ë“  ê²½ê³„ íƒì§€
        boundaries = []
        
        for pattern_name, pattern in self.PATTERNS.items():
            matches = list(pattern.finditer(text))
            if matches:
                logger.info(f"   ğŸ” {pattern_name}: {len(matches)}ê°œ ë§¤ì¹­")
            
            for match in matches:
                boundaries.append({
                    'type': pattern_name,
                    'priority': self.PRIORITY[pattern_name],
                    'start': match.start(),
                    'text': match.group(0).strip()
                })
        
        if not boundaries:
            logger.warning("   âš ï¸ íŒ¨í„´ ë¯¸ê²€ì¶œ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # 2. ìœ„ì¹˜ ê¸°ì¤€ ì •ë ¬
        boundaries.sort(key=lambda x: (x['start'], x['priority']))
        
        # 3. ì¤‘ë³µ ì œê±°
        unique_boundaries = []
        last_start = -1
        for b in boundaries:
            if b['start'] != last_start:
                unique_boundaries.append(b)
                last_start = b['start']
        
        logger.info(f"   ğŸ“‹ ìœ íš¨ ê²½ê³„: {len(unique_boundaries)}ê°œ")
        
        # 4. ì²­í¬ ìƒì„±
        chunks = []
        for i, boundary in enumerate(unique_boundaries):
            start = boundary['start']
            end = unique_boundaries[i + 1]['start'] if i + 1 < len(unique_boundaries) else len(text)
            
            content = text[start:end].strip()
            
            if len(content) < min_size:
                continue
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.match(rf'ì œ\s*{self.NUM}\s*ì¡°\s*\(([^)]+)\)', content)
            title = title_match.group(1) if title_match else None
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': boundary['type'],
                    'boundary': boundary['text'],
                    'title': title,
                    'char_count': len(content),
                    'chunk_index': len(chunks) + 1
                }
            })
        
        if not chunks:
            logger.warning("   âš ï¸ ë¹ˆ ê²°ê³¼ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # GPT ì œì•ˆ: íŒŒí¸ ë³‘í•©
        chunks = self._post_merge_small_fragments(chunks, min_len=200)
        
        logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        return chunks
    
    def _fallback_chunk(self, text: str, target: int, min_len: int, max_len: int) -> List[Dict[str, Any]]:
        """ê¸¸ì´ ê¸°ë°˜ í˜ì¼ì„¸ì´í”„ ì²­í‚¹"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + target, len(text))
            
            # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
            if end < len(text):
                for sep in ['\n\n', '\n', '. ', 'ã€‚']:
                    boundary = text.rfind(sep, start + min_len, end)
                    if boundary != -1:
                        end = boundary + len(sep)
                        break
            
            content = text[start:end].strip()
            
            if len(content) >= min_len:
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'fallback',
                        'char_count': len(content),
                        'chunk_index': len(chunks) + 1
                    }
                })
            
            start = end
        
        logger.info(f"   âœ… Fallback ì²­í‚¹: {len(chunks)}ê°œ")
        return chunks if chunks else [{'content': text, 'metadata': {'type': 'emergency', 'char_count': len(text), 'chunk_index': 1}}]