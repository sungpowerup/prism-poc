"""
core/semantic_chunker.py
PRISM Phase 0.3.2 - Semantic Chunker (ë¬¸ì¥ ê²½ê³„ ë³´ì¡´)

âœ… Phase 0.3.2 ê°œì„ :
1. ë¬¸ì¥ ê²½ê³„ ë³´ì¡´ ë¶„í•  ì¶”ê°€
2. ìµœì†Œ ì²­í¬ í¬ê¸° ê°€ë“œ (300ì)
3. í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ íŒ¨í„´

Author: ì´ì„œì˜ (Backend Lead) + GPT í”¼ë“œë°±
Date: 2025-11-07
Version: Phase 0.3.2
"""

import re
import logging
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)


class SemanticChunker:
    """
    Phase 0.3.2 ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì—”ì§„ (ë¬¸ì¥ ê²½ê³„ ë³´ì¡´)
    
    âœ… Phase 0.3.2 ê°œì„ :
    - ë¬¸ì¥ ê²½ê³„ ë³´ì¡´ ë¶„í• 
    - ìµœì†Œ ì²­í¬ í¬ê¸° ê°€ë“œ
    - í•œêµ­ì–´ ë¬¸ì¥ íŒ¨í„´
    """
    
    VERSION = "Phase 0.3.2"
    
    # ì²­í¬ í¬ê¸° ì„¤ì •
    MIN_SIZE = 300      # âœ… Phase 0.3.2: ìµœì†Œ í¬ê¸° ê°€ë“œ
    TARGET_SIZE = 900
    MAX_SIZE = 1200
    
    # ì¡°ë¬¸ íŒ¨í„´
    ARTICLE_PATTERN = re.compile(
        r'#{1,6}\s*ì œ\d+ì¡°(?:ì˜\d+)?(?:\([^)]+\))?',
        re.MULTILINE
    )
    
    # âœ… Phase 0.3.2: í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ íŒ¨í„´ (GPT ì œì•ˆ)
    SENTENCE_PATTERN = re.compile(
        r'(?<=[ë‹¤|ìš”|ì„|í•¨|ìŒ])\s+',  # í•œêµ­ì–´ ë¬¸ì¥ ë
        re.MULTILINE
    )
    
    # Fallback: ì˜ì–´ì‹ ë¬¸ì¥ ê²½ê³„
    SENTENCE_PATTERN_EN = re.compile(
        r'(?<=[.!?])\s+',
        re.MULTILINE
    )
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info(f"âœ… SemanticChunker {self.VERSION} ì´ˆê¸°í™”")
        logger.info(f"   ì²­í¬ í¬ê¸°: {self.MIN_SIZE}-{self.MAX_SIZE} (ëª©í‘œ: {self.TARGET_SIZE})")
        logger.info(f"   ìµœì†Œ ê°€ë“œ: {self.MIN_SIZE}ì")
        logger.info(f"   í•˜ë“œ ê°€ë“œ: {self.MAX_SIZE}ì ê°•ì œ flush")
        logger.info(f"   Fail-safe: ì¡°ë¬¸ < 2ê°œ ì‹œ ê¸¸ì´ ë¶„í• ")
        logger.info(f"   ì¡°ë¬¸ íŒ¨í„´: ### í—¤ë” ì§€ì›")
        logger.info(f"   âœ… ë¬¸ì¥ ê²½ê³„ ë³´ì¡´: í•œêµ­ì–´ íŒ¨í„´")
    
    def chunk(
        self,
        text: str,
        doc_type: str = 'statute',
        metadata: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            doc_type: ë¬¸ì„œ íƒ€ì…
            metadata: ë©”íƒ€ë°ì´í„°
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ”— SemanticChunking {self.VERSION} ì‹œì‘: {len(text)} ê¸€ì")
        
        # âœ… 1ë‹¨ê³„: ì¡°ë¬¸ ê²½ê³„ íƒì§€
        boundaries = self._find_article_boundaries(text)
        
        if len(boundaries) < 2:
            logger.warning(f"  âš ï¸ ì¡°ë¬¸ ê²½ê³„ ë¶€ì¡± ({len(boundaries)}ê°œ) - Fail-safe ê¸¸ì´ ë¶„í• ")
            return self._fallback_chunk(text, metadata)
        
        logger.info(f"   ì¡°ë¬¸ ê°ì§€: {len(boundaries)}ê°œ")
        
        # âœ… 2ë‹¨ê³„: ì¡°ë¬¸ ê¸°ë°˜ ì´ˆê¸° ë¶„í• 
        sections = self._split_by_articles(text, boundaries)
        
        # âœ… Phase 0.3.2: 3ë‹¨ê³„: ë¬¸ì¥ ê²½ê³„ ë³´ì¡´ ë¶„í• 
        adjusted_sections = []
        for section in sections:
            if len(section) > self.MAX_SIZE:
                # ë¬¸ì¥ ê²½ê³„ ê¸°ë°˜ ë¶„í• 
                split_sections = self._split_with_sentence_boundary(section, self.MAX_SIZE)
                adjusted_sections.extend(split_sections)
            else:
                adjusted_sections.append(section)
        
        logger.info(f"   ê¸¸ì´ ì¡°ì • í›„: {len(adjusted_sections)}ê°œ ì„¹ì…˜")
        
        # âœ… Phase 0.3.2: 4ë‹¨ê³„: ìµœì†Œ í¬ê¸° ê°€ë“œ
        adjusted_sections = self._merge_short_chunks(adjusted_sections, self.MIN_SIZE)
        
        # âœ… 5ë‹¨ê³„: ì²­í¬ ìƒì„±
        chunks = self._create_chunks(adjusted_sections, metadata)
        
        logger.info(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        return chunks
    
    def _find_article_boundaries(self, text: str) -> List[Tuple[int, str]]:
        """
        ì¡°ë¬¸ ê²½ê³„ íƒì§€
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            [(ìœ„ì¹˜, ì¡°ë¬¸ í—¤ë”), ...]
        """
        boundaries = []
        
        for match in self.ARTICLE_PATTERN.finditer(text):
            boundaries.append((match.start(), match.group()))
        
        return boundaries
    
    def _split_by_articles(
        self,
        text: str,
        boundaries: List[Tuple[int, str]]
    ) -> List[str]:
        """
        ì¡°ë¬¸ ê²½ê³„ ê¸°ë°˜ ë¶„í• 
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            boundaries: ì¡°ë¬¸ ê²½ê³„
        
        Returns:
            ë¶„í• ëœ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
        """
        sections = []
        
        for i in range(len(boundaries)):
            start = boundaries[i][0]
            end = boundaries[i + 1][0] if i + 1 < len(boundaries) else len(text)
            
            section = text[start:end].strip()
            if section:
                sections.append(section)
        
        return sections
    
    def _split_with_sentence_boundary(
        self,
        text: str,
        max_size: int
    ) -> List[str]:
        """
        âœ… Phase 0.3.2: ë¬¸ì¥ ê²½ê³„ ë³´ì¡´ ë¶„í•  (GPT ì œì•ˆ)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            max_size: ìµœëŒ€ í¬ê¸°
        
        Returns:
            ë¶„í• ëœ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
        """
        # í•œêµ­ì–´ ë¬¸ì¥ ë¶„í•  ì‹œë„
        sentences = self.SENTENCE_PATTERN.split(text)
        
        # í•œêµ­ì–´ íŒ¨í„´ ì‹¤íŒ¨ ì‹œ ì˜ì–´ íŒ¨í„´ ì‚¬ìš©
        if len(sentences) == 1:
            sentences = self.SENTENCE_PATTERN_EN.split(text)
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_len = len(sentence)
            
            # í•˜ë“œ ê°€ë“œ ë„ë‹¬ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            if current_size + sentence_len > max_size:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_size = sentence_len
            else:
                current_chunk.append(sentence)
                current_size += sentence_len
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def _merge_short_chunks(
        self,
        chunks: List[str],
        min_size: int
    ) -> List[str]:
        """
        âœ… Phase 0.3.2: ì§§ì€ ì²­í¬ ë³‘í•© (GPT ì œì•ˆ)
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            min_size: ìµœì†Œ í¬ê¸°
        
        Returns:
            ë³‘í•©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        merged = []
        i = 0
        
        while i < len(chunks):
            chunk = chunks[i]
            
            # ë§ˆì§€ë§‰ ì²­í¬ê±°ë‚˜ ì¶©ë¶„íˆ ê¸´ ê²½ìš°
            if i == len(chunks) - 1 or len(chunk) >= min_size:
                merged.append(chunk)
                i += 1
            # ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•©
            else:
                if i + 1 < len(chunks):
                    next_chunk = chunks[i + 1]
                    merged.append(chunk + '\n\n' + next_chunk)
                    i += 2
                else:
                    merged.append(chunk)
                    i += 1
        
        return merged
    
    def _create_chunks(
        self,
        sections: List[str],
        base_metadata: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """
        ì²­í¬ ê°ì²´ ìƒì„±
        
        Args:
            sections: ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
            base_metadata: ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        
        for i, section in enumerate(sections, 1):
            # ì¡°ë¬¸ ë²ˆí˜¸ ì¶”ì¶œ
            article_match = self.ARTICLE_PATTERN.search(section)
            article_no = article_match.group() if article_match else f"ì„¹ì…˜{i}"
            
            # ì¡°ë¬¸ ì œëª© ì¶”ì¶œ
            article_title = ""
            if article_match:
                title_match = re.search(r'\(([^)]+)\)', article_match.group())
                if title_match:
                    article_title = title_match.group(1)
            
            metadata = {
                'article_no': article_no.replace('#', '').strip(),
                'article_title': article_title,
                'char_count': len(section),
                'chunk_index': i
            }
            
            if base_metadata:
                metadata.update(base_metadata)
            
            chunks.append({
                'id': f'chunk_{i}',
                'content': section,
                'metadata': metadata
            })
        
        return chunks
    
    def _fallback_chunk(
        self,
        text: str,
        metadata: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """
        Fail-safe ê¸¸ì´ ê¸°ë°˜ ë¶„í• 
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„°
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        start = 0
        chunk_index = 1
        
        while start < len(text):
            end = min(start + self.TARGET_SIZE, len(text))
            
            # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
            if end < len(text):
                # í•œêµ­ì–´ ë¬¸ì¥ ë ì°¾ê¸°
                for pattern in [r'[ë‹¤|ìš”|ì„|í•¨|ìŒ]\s', r'[.!?]\s']:
                    match = re.search(pattern, text[end:end+100])
                    if match:
                        end += match.end()
                        break
            
            section = text[start:end].strip()
            
            if section:
                chunk = {
                    'id': f'chunk_{chunk_index}',
                    'content': section,
                    'metadata': {
                        'article_no': f'ì„¹ì…˜{chunk_index}',
                        'article_title': '',
                        'char_count': len(section),
                        'chunk_index': chunk_index
                    }
                }
                
                if metadata:
                    chunk['metadata'].update(metadata)
                
                chunks.append(chunk)
                chunk_index += 1
            
            start = end
        
        return chunks