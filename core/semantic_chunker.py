"""
semantic_chunker_v033.py
PRISM Phase 0.3.3 - Semantic Chunker with Boundary Validation

âœ… Phase 0.3.3 ê°œì„ :
1. í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ íŒ¨í„´ ê°•í™”
2. ì²­í¬ ìœ íš¨ì„± ìë™ ê²€ì‚¬
3. ë¶ˆì™„ì „ ì²­í¬ ìë™ ë³‘í•©
4. ì¡°ë¬¸ ìˆœì„œ ìë™ ì •ë ¬

ì„¤ì¹˜: ê¸°ì¡´ semantic_chunker.py ëŒ€ì²´

Author: ë§ˆì°½ìˆ˜ì‚° íŒ€
Date: 2025-11-07
Version: Phase 0.3.3
"""

import re
import logging
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)


class SemanticChunker:
    """
    Phase 0.3.3 ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì—”ì§„
    
    âœ… í•µì‹¬ ê°œì„ :
    - í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ ê°•í™”
    - ë¶ˆì™„ì „ ì²­í¬ ìë™ ë³‘í•©
    - ì¡°ë¬¸ ìˆœì„œ ìë™ ì •ë ¬
    """
    
    VERSION = "Phase 0.3.3"
    
    # âœ… í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
    KOREAN_SENTENCE_ENDINGS = [
        r'ë‹¤\.',  # ~í•˜ë‹¤.
        r'ë‹¤\)',  # ~í•œë‹¤)
        r'ë‹¤\s*$',  # ~í•œë‹¤
        r'í•¨\.',  # ~í•¨.
        r'ë¨\.',  # ~ë¨.
        r'ì„\.',  # ~ì„.
        r'<\d{4}\.\s*\d{1,2}\.\s*\d{1,2}>',  # <2024.1.1>
        r'\d+\.\)',  # 1.)
        r'í˜¸\s*$',  # ~í˜¸
        r'í•œë‹¤\.',  # ~í•œë‹¤.
    ]
    
    #âš ï¸ ë¶ˆì™„ì „ ì¢…ê²° íŒ¨í„´
    INCOMPLETE_ENDINGS = [
        r'ì˜\s*$', r'ë¥¼\s*$', r'ì„\s*$', r'ê°€\s*$',
        r'ì—\s*$', r'ì™€\s*$', r'ë¡œ\s*$', r'ì±„\s*$',
        r'í˜•\s+ë˜ëŠ”\s+ì¹˜ë£Œê°\s*$',  # ë¬¸ì¥ ì¤‘ê°„ ì ˆë‹¨
    ]
    
    def __init__(self, target_size: int = 800, min_size: int = 300):
        """ì´ˆê¸°í™”"""
        self.target_size = target_size
        self.min_size = min_size
        
        self.sentence_patterns = [re.compile(p) for p in self.KOREAN_SENTENCE_ENDINGS]
        self.incomplete_patterns = [re.compile(p) for p in self.INCOMPLETE_ENDINGS]
        
        logger.info(f"âœ… SemanticChunker {self.VERSION} ì´ˆê¸°í™”")
        logger.info(f"   ğŸ¯ ëª©í‘œ: {target_size}ì, ìµœì†Œ: {min_size}ì")
    
    def chunk(self, text: str, doc_type: str = 'statute') -> List[Dict[str, Any]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            doc_type: ë¬¸ì„œ íƒ€ì…
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"   âœ‚ï¸ SemanticChunker {self.VERSION} ì‹œì‘")
        
        # 1. ê¸°ë³¸ ì²­í‚¹
        chunks = self._basic_chunk(text, doc_type)
        logger.info(f"      ê¸°ë³¸ ì²­í‚¹: {len(chunks)}ê°œ")
        
        # 2. ê²½ê³„ ê²€ì¦ + ë³‘í•©
        validated = self._validate_boundaries(chunks)
        logger.info(f"      ê²½ê³„ ê²€ì¦: {len(validated)}ê°œ")
        
        # 3. ì¡°ë¬¸ ì •ë ¬
        sorted_chunks = self._sort_by_article(validated)
        logger.info(f"   âœ… ì²­í‚¹ ì™„ë£Œ: {len(sorted_chunks)}ê°œ")
        
        return sorted_chunks
    
    def _basic_chunk(self, text: str, doc_type: str) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ ì²­í‚¹"""
        chunks = []
        
        if doc_type == 'statute':
            # ì¡°ë¬¸ ê¸°ì¤€ ë¶„í• 
            article_pattern = re.compile(r'(#{1,4}\s*ì œ\d+ì¡°[^#]*?)(?=#{1,4}\s*ì œ\d+ì¡°|$)', re.DOTALL)
            matches = article_pattern.findall(text)
            
            for match in matches:
                article_match = re.search(r'ì œ(\d+)ì¡°(?:ì˜(\d+))?', match)
                article_no = article_match.group(0) if article_match else ''
                
                title_match = re.search(r'ì œ\d+ì¡°(?:ì˜\d+)?\s*\(([^)]+)\)', match)
                article_title = title_match.group(1) if title_match else ''
                
                chunks.append({
                    'content': match.strip(),
                    'metadata': {
                        'article_no': article_no,
                        'article_title': article_title,
                        'char_count': len(match.strip()),
                        'chunk_index': len(chunks) + 1
                    }
                })
        else:
            # ì¼ë°˜ ë¬¸ì„œ: ë¬¸ì¥ ë‹¨ìœ„
            sentences = re.split(r'(?<=[.!?])\s+', text)
            current_chunk = []
            current_size = 0
            
            for sentence in sentences:
                if current_size + len(sentence) > self.target_size and current_chunk:
                    chunk_text = ' '.join(current_chunk)
                    chunks.append({
                        'content': chunk_text,
                        'metadata': {
                            'article_no': '',
                            'article_title': '',
                            'char_count': len(chunk_text),
                            'chunk_index': len(chunks) + 1
                        }
                    })
                    current_chunk = []
                    current_size = 0
                
                current_chunk.append(sentence)
                current_size += len(sentence)
            
            if current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunks.append({
                    'content': chunk_text,
                    'metadata': {
                        'article_no': '',
                        'article_title': '',
                        'char_count': len(chunk_text),
                        'chunk_index': len(chunks) + 1
                    }
                })
        
        return chunks
    
    def _validate_boundaries(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê²½ê³„ ê²€ì¦ + ìë™ ë³‘í•©"""
        validated = []
        i = 0
        
        while i < len(chunks):
            chunk = chunks[i]
            content = chunk['content']
            
            is_complete = self._is_sentence_boundary(content)
            
            if not is_complete and i + 1 < len(chunks):
                # ë¶ˆì™„ì „ â†’ ë³‘í•©
                logger.warning(f"      âš ï¸ ë¶ˆì™„ì „ ì²­í¬: '{content[-50:]}'")
                next_chunk = chunks[i + 1]
                
                merged_content = content + '\n\n' + next_chunk['content']
                merged_chunk = {
                    'content': merged_content,
                    'metadata': {
                        'article_no': chunk['metadata']['article_no'] or next_chunk['metadata']['article_no'],
                        'article_title': chunk['metadata']['article_title'] or next_chunk['metadata']['article_title'],
                        'char_count': len(merged_content),
                        'chunk_index': len(validated) + 1,
                        'merged': True
                    }
                }
                
                validated.append(merged_chunk)
                logger.info(f"      âœ… ìë™ ë³‘í•©: Chunk {i+1} + {i+2}")
                i += 2
            else:
                validated.append(chunk)
                i += 1
        
        return validated
    
    def _is_sentence_boundary(self, text: str) -> bool:
        """ë¬¸ì¥ ê²½ê³„ ê²€ì‚¬"""
        text_end = text.strip()[-100:]
        
        # ì™„ì „í•œ ì¢…ê²° í™•ì¸
        for pattern in self.sentence_patterns:
            if pattern.search(text_end):
                return True
        
        # ë¶ˆì™„ì „ ì¢…ê²° í™•ì¸
        for pattern in self.incomplete_patterns:
            if pattern.search(text_end):
                return False
        
        return True
    
    def _sort_by_article(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¡°ë¬¸ ë²ˆí˜¸ ê¸°ì¤€ ì •ë ¬"""
        def extract_article_number(chunk: Dict[str, Any]) -> Tuple[int, int]:
            article_no = chunk['metadata'].get('article_no', '')
            
            if not article_no:
                return (999, 0)
            
            main_match = re.search(r'ì œ(\d+)ì¡°', article_no)
            sub_match = re.search(r'ì œ\d+ì¡°ì˜(\d+)', article_no)
            
            main_num = int(main_match.group(1)) if main_match else 999
            sub_num = int(sub_match.group(1)) if sub_match else 0
            
            return (main_num, sub_num)
        
        sorted_chunks = sorted(chunks, key=extract_article_number)
        
        # chunk_index ì¬ë¶€ì—¬
        for i, chunk in enumerate(sorted_chunks, 1):
            chunk['metadata']['chunk_index'] = i
        
        return sorted_chunks