"""
core/semantic_chunker.py
PRISM Phase 0.3.4 P2.1 - ê¸´ê¸‰ íŒ¨í„´ ìˆ˜ì •

âœ… ë³€ê²½ì‚¬í•­:
1. ì¤„ ì‹œì‘ íŒ¨í„´ ìˆ˜ì • (^ë¡œ ê°•ì œ)
2. ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
3. ìµœì†Œ í¬ê¸° ì™„í™” (10ì â†’ 50ì)
"""

import re
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


class SemanticChunker:
    """Phase 0.3.4 P2.1 ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (íŒ¨í„´ ê¸´ê¸‰ ìˆ˜ì •)"""
    
    # ê¸´ê¸‰ ìˆ˜ì •: ì¤„ ì‹œì‘ë§Œ ë§¤ì¹­, ê³µë°± í—ˆìš©
    PATTERNS = {
        'basic': re.compile(r'^ê¸°\s*ë³¸\s*ì •\s*ì‹ ', re.MULTILINE),
        'chapter': re.compile(r'^ì œ\s*\d+\s*ì¥', re.MULTILINE),
        'article': re.compile(r'^ì œ\s*\d+\s*ì¡°', re.MULTILINE),
        'supplement': re.compile(r'^ë¶€\s*ì¹™', re.MULTILINE)
    }
    
    PRIORITY = {
        'basic': 1,
        'chapter': 2,
        'article': 3,
        'supplement': 4
    }
    
    def __init__(self):
        logger.info("âœ… SemanticChunker Phase 0.3.4 P2.1 ì´ˆê¸°í™”")
        logger.info(f"   ğŸ¯ íŒ¨í„´: {len(self.PATTERNS)}ê°œ (ìˆœìˆ˜ ë³¸ë¬¸ í˜•íƒœ ì§€ì›)")
    
    def chunk(self, text: str, target_size: int = 800, min_size: int = 50, max_size: int = 1500) -> List[Dict[str, Any]]:
        """ì²­í‚¹ ì‹¤í–‰"""
        if not text or len(text) < min_size:
            raise ValueError(f"ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(text)}ì)")
        
        logger.info(f"âœ‚ï¸ ì²­í‚¹ ì‹œì‘: {len(text)}ì")
        
        # ë””ë²„ê·¸: ìƒ˜í”Œ ì¶œë ¥
        sample = text[:200].replace('\n', '\\n')
        logger.debug(f"   ğŸ“ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {sample}")
        
        # 1. ëª¨ë“  ê²½ê³„ íƒì§€
        boundaries = []
        
        for pattern_name, pattern in self.PATTERNS.items():
            matches = list(pattern.finditer(text))
            logger.debug(f"   ğŸ” {pattern_name}: {len(matches)}ê°œ ë§¤ì¹­")
            
            for match in matches:
                boundaries.append({
                    'type': pattern_name,
                    'priority': self.PRIORITY[pattern_name],
                    'start': match.start(),
                    'text': match.group(0).strip()
                })
                logger.debug(f"      â†’ {match.group(0).strip()[:30]} @ {match.start()}")
        
        if not boundaries:
            logger.warning("   âš ï¸ íŒ¨í„´ ë¯¸ê²€ì¶œ â†’ Fallback")
            logger.warning(f"   ğŸ“ ì²« 100ì: {text[:100]}")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        # 2. ìœ„ì¹˜ ê¸°ì¤€ ì •ë ¬
        boundaries.sort(key=lambda x: (x['start'], x['priority']))
        
        # 3. ì¤‘ë³µ ì œê±°
        unique_boundaries = []
        last_start = -1
        for b in boundaries:
            if b['start'] != last_start:
                unique_boundaries.append(b)
                last_start = b['start']
        
        logger.info(f"   ğŸ“‹ ê²½ê³„ {len(unique_boundaries)}ê°œ ê°ì§€ (ì¤‘ë³µ ì œê±° í›„)")
        
        # 4. ì²­í¬ ìƒì„±
        chunks = []
        for i, boundary in enumerate(unique_boundaries):
            start = boundary['start']
            end = unique_boundaries[i + 1]['start'] if i + 1 < len(unique_boundaries) else len(text)
            
            content = text[start:end].strip()
            
            # ìµœì†Œ í¬ê¸° ì™„í™” (10 â†’ 50)
            if len(content) < min_size:
                logger.debug(f"   ê±´ë„ˆëœ€: {boundary['text'][:20]} ({len(content)}ì < {min_size}ì)")
                continue
            
            if len(content) > max_size:
                logger.warning(f"   í¬ê¸° ì´ˆê³¼: {boundary['text'][:20]} ({len(content)}ì > {max_size}ì)")
            
            # ì œëª© ì¶”ì¶œ (ì œNì¡°(ì œëª©))
            title_match = re.match(r'^ì œ\s*\d+\s*ì¡°\s*\(([^)]+)\)', content)
            title = title_match.group(1) if title_match else None
            
            chunks.append({
                'content': content,
                'metadata': {
                    'type': boundary['type'],
                    'boundary': boundary['text'],
                    'title': title,
                    'char_count': len(content),
                    'chunk_index': len(chunks) + 1
                }
            })
        
        logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        
        if not chunks:
            logger.warning("   âš ï¸ ë¹ˆ ê²°ê³¼ â†’ Fallback")
            return self._fallback_chunk(text, target_size, min_size, max_size)
        
        return chunks
    
    def _fallback_chunk(self, text: str, target: int, min_len: int, max_len: int) -> List[Dict[str, Any]]:
        """ê¸¸ì´ ê¸°ë°˜ í˜ì¼ì„¸ì´í”„ ì²­í‚¹"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + target, len(text))
            
            # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
            if end < len(text):
                for sep in ['\n\n', '\n', '. ', 'ã€‚']:
                    boundary = text.rfind(sep, start + min_len, end)
                    if boundary != -1:
                        end = boundary + len(sep)
                        break
            
            content = text[start:end].strip()
            
            if len(content) >= min_len:
                chunks.append({
                    'content': content,
                    'metadata': {
                        'type': 'fallback',
                        'char_count': len(content),
                        'chunk_index': len(chunks) + 1
                    }
                })
            
            start = end
        
        logger.info(f"   âœ… Fallback ì²­í‚¹: {len(chunks)}ê°œ")
        return chunks if chunks else [{'content': text, 'metadata': {'type': 'emergency', 'char_count': len(text), 'chunk_index': 1}}]