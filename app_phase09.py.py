"""
app.py - PRISM Phase 0.9 "LLM Rewriting View"
3ë‹¨ ê³„ì¸µ UI + GPT ê¶Œì¥ ì•ˆì „ì¥ì¹˜ ì™„ë¹„

âœ… Phase 0.9 ì‹ ê·œ ê¸°ëŠ¥:
1. 3ë‹¨ ê³„ì¸µ ë³´ê¸° ëª¨ë“œ (ì›ë³¸/ì—”ì§„/AI ê°€ë…ì„±)
2. ì¡°ë¬¸ ë‹¨ìœ„ On-Demand ë¦¬ë¼ì´íŒ…
3. Sanity Check ìë™ ê²€ì¦
4. ë²•ì  íš¨ë ¥ ëª…ì‹œ í‘œì‹œ

âœ… GPT ì•ˆì „ì¥ì¹˜:
1. ì—”ì§„ JSON ì ˆëŒ€ ë¶ˆë³€
2. ë¦¬ë¼ì´íŒ… ê²°ê³¼ëŠ” ë·° ì „ìš©
3. ì›ë³¸ í•­ìƒ ë…¸ì¶œ
4. ìºì‹œ êµ¬ì¡° (ì†ë„/ë¹„ìš© ì ˆê°)

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ (ìµœë™í˜„ Frontend Lead + GPT í”¼ë“œë°±)
Date: 2025-11-14
Version: Phase 0.9
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import json
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ëª¨ë“ˆ Import
try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.hybrid_extractor import HybridExtractor
    from core.semantic_chunker import SemanticChunker
    from core.dual_qa_gate import DualQAGate, extract_pdf_text_layer
    from core.utils_fs import safe_temp_path, safe_remove
    
    logger.info("âœ… ëª¨ë“ˆ import ì„±ê³µ (Phase 0.9)")
    
except Exception as e:
    logger.error(f"âŒ Import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()

# LawParser Import
try:
    from core.law_parser import LawParser
    LAW_MODE_AVAILABLE = True
    logger.info("âœ… LawParser ë¡œë“œ ì„±ê³µ")
except ImportError:
    LAW_MODE_AVAILABLE = False
    logger.warning("âš ï¸ LawParser ë¯¸ì„¤ì¹˜")

# DocumentProfile Import
try:
    from core.document_profile import auto_detect_profile
    PROFILE_AVAILABLE = True
    logger.info("âœ… DocumentProfile ë¡œë“œ ì„±ê³µ")
except ImportError:
    PROFILE_AVAILABLE = False
    logger.warning("âš ï¸ DocumentProfile ë¯¸ì„¤ì¹˜")

# âœ… Phase 0.9: LLM Rewriter Import
try:
    sys.path.insert(0, str(Path(__file__).parent / 'tests'))
    from llm_rewriter import LLMRewriter
    LLM_REWRITER_AVAILABLE = True
    logger.info("âœ… LLMRewriter ë¡œë“œ ì„±ê³µ (Phase 0.9)")
except ImportError as e:
    LLM_REWRITER_AVAILABLE = False
    logger.warning(f"âš ï¸ LLMRewriter ë¯¸ì„¤ì¹˜: {e}")


# ============================================
# âœ… Phase 0.9: ë¦¬ë·°ìš© Markdown ìƒì„±
# ============================================

def to_review_md(chunks: list) -> str:
    """
    ë¦¬ë·°ìš© Markdown ìƒì„± (Phase 0.7 ë°©ì‹ ìœ ì§€)
    
    íƒ€ì…ë³„ë¡œ ì •í™•íˆ ë Œë”ë§
    """
    import re
    
    lines = []
    chunks_sorted = sorted(chunks, key=lambda c: c['metadata'].get('section_order', 999))
    
    for chunk in chunks_sorted:
        meta = chunk["metadata"]
        text = chunk["content"]
        chunk_type = meta["type"]
        
        if chunk_type == "title":
            title = meta.get('title', text)
            lines.append(f"# {title}\n")
        
        elif chunk_type == "amendment_history":
            lines.append("## ê°œì • ì´ë ¥\n")
            items = re.split(r'(?=ì œ\d+ì°¨)', text)
            for item in items:
                item = item.strip()
                if item:
                    lines.append(f"- {item}")
            lines.append("")
        
        elif chunk_type == "basic":
            lines.append("## ê¸°ë³¸ì •ì‹ \n")
            lines.append(text)
            lines.append("")
        
        elif chunk_type == "chapter":
            ch_num = meta["chapter_number"]
            ch_title = meta["chapter_title"]
            lines.append(f"## {ch_num} {ch_title}\n")
        
        elif chunk_type == "article":
            art_num = meta["article_number"]
            art_title = meta["article_title"]
            lines.append(f"### {art_num}({art_title})\n")
            
            body = text
            header = f"{art_num}({art_title})"
            if header in body:
                body = body.replace(header, '', 1).strip()
            
            lines.append(body)
            lines.append("")
    
    return "\n".join(lines)


# ============================================
# âœ… Phase 0.9: LLM ë¦¬ë¼ì´íŒ… (ì¡°ë¬¸ ë‹¨ìœ„)
# ============================================

def rewrite_articles_with_llm(
    chunks: list,
    rewriter: LLMRewriter,
    document_id: str = "default"
) -> dict:
    """
    ì¡°ë¬¸ ë‹¨ìœ„ LLM ë¦¬ë¼ì´íŒ…
    
    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        rewriter: LLMRewriter ì¸ìŠ¤í„´ìŠ¤
        document_id: ë¬¸ì„œ ID
    
    Returns:
        {
            'rewritten_chunks': [...],
            'validation_summary': {...}
        }
    """
    
    rewritten_chunks = []
    validation_results = []
    
    chunks_sorted = sorted(chunks, key=lambda c: c['metadata'].get('section_order', 999))
    
    total_articles = sum(1 for c in chunks_sorted if c['metadata']['type'] == 'article')
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    article_count = 0
    
    for chunk in chunks_sorted:
        meta = chunk["metadata"]
        chunk_type = meta["type"]
        
        # ì¡°ë¬¸ë§Œ ë¦¬ë¼ì´íŒ…
        if chunk_type == "article":
            article_count += 1
            article_number = meta["article_number"]
            article_title = meta["article_title"]
            article_body = chunk["content"]
            
            # í—¤ë” ì œê±°
            header = f"{article_number}({article_title})"
            if header in article_body:
                article_body = article_body.replace(header, '', 1).strip()
            
            status_text.text(f"âœ¨ ë¦¬ë¼ì´íŒ… ì¤‘... {article_number} ({article_count}/{total_articles})")
            
            # LLM ë¦¬ë¼ì´íŒ…
            rewritten_text, validation = rewriter.rewrite_article(
                article_number=article_number,
                article_title=article_title,
                article_body=article_body,
                document_id=document_id,
                parser_version="0.9.0"
            )
            
            validation_results.append({
                'article': article_number,
                'is_valid': validation.is_valid,
                'warnings': validation.warnings
            })
            
            # ì²­í¬ ì—…ë°ì´íŠ¸ (contentë§Œ)
            rewritten_chunk = chunk.copy()
            rewritten_chunk['content'] = rewritten_text
            rewritten_chunks.append(rewritten_chunk)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.progress(article_count / total_articles)
        
        else:
            # ì¡°ë¬¸ ì™¸ì—ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            rewritten_chunks.append(chunk)
    
    progress_bar.empty()
    status_text.empty()
    
    # ê²€ì¦ ìš”ì•½
    total_validations = len(validation_results)
    passed = sum(1 for v in validation_results if v['is_valid'])
    failed = total_validations - passed
    
    validation_summary = {
        'total': total_validations,
        'passed': passed,
        'failed': failed,
        'pass_rate': passed / total_validations if total_validations > 0 else 0.0,
        'details': validation_results
    }
    
    return {
        'rewritten_chunks': rewritten_chunks,
        'validation_summary': validation_summary
    }


# ============================================
# VLM/LawMode ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
# ============================================

def process_document_vlm_mode(pdf_path: str, pdf_text: str, max_pages: int = 20):
    """VLM íŒŒì´í”„ë¼ì¸"""
    
    st.info("ğŸ”¬ VLM ëª¨ë“œ: Azure OpenAI GPT-4 Vision ì²˜ë¦¬ ì¤‘...")
    progress_bar = st.progress(0)
    
    try:
        pdf_processor = PDFProcessor()
        pages = pdf_processor.process_pdf(pdf_path)
        progress_bar.progress(25)
        
        if len(pages) > max_pages:
            st.warning(f"âš ï¸ ìµœëŒ€ {max_pages}í˜ì´ì§€ê¹Œì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            pages = pages[:max_pages]
        
        vlm_service = VLMServiceV50(provider='azure_openai')
        extractor = HybridExtractor(vlm_service)
        markdown_text = extractor.extract(pages)
        progress_bar.progress(50)
        
        st.info("ğŸ§© ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì¤‘...")
        chunker = SemanticChunker()
        chunks = chunker.chunk(markdown_text)
        st.success(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        st.info("ğŸ”¬ DualQA ê²€ì¦ ì¤‘...")
        qa_gate = DualQAGate()
        qa_result = qa_gate.validate(
            pdf_text=pdf_text,
            processed_text=markdown_text,
            source="vlm"
        )
        
        progress_bar.progress(100)
        
        return {
            'markdown': markdown_text,
            'chunks': chunks,
            'qa_result': qa_result,
            'is_qa_pass': qa_result.get('is_pass', False),
            'mode': 'VLM'
        }
    
    except Exception as e:
        logger.error(f"âŒ VLM ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


def process_document_law_mode(pdf_path: str, pdf_text: str, document_title: str):
    """LawMode íŒŒì´í”„ë¼ì¸"""
    
    st.info("ğŸ“œ LawMode: ê·œì •/ë²•ë ¹ íŒŒì‹± ì¤‘...")
    progress_bar = st.progress(0)
    
    if PROFILE_AVAILABLE:
        profile = auto_detect_profile(pdf_text, document_title)
        st.info(f"ğŸ“ ë¬¸ì„œ í”„ë¡œíŒŒì¼: {profile.name}")
    
    parser = LawParser()
    parsed_result = parser.parse(
        pdf_text=pdf_text,
        document_title=document_title,
        clean_artifacts=True,
        normalize_linebreaks=True
    )
    progress_bar.progress(50)
    
    chunks = parser.to_chunks(parsed_result)
    progress_bar.progress(75)
    
    markdown_text = parser.to_markdown(parsed_result)
    
    st.info("ğŸ”¬ DualQA ê²€ì¦ ì¤‘...")
    qa_gate = DualQAGate()
    qa_result = qa_gate.validate(
        pdf_text=pdf_text,
        processed_text=markdown_text,
        source="lawmode"
    )
    
    progress_bar.progress(100)
    
    return {
        'markdown': markdown_text,
        'chunks': chunks,
        'qa_result': qa_result,
        'is_qa_pass': qa_result.get('is_pass', False),
        'mode': 'LawMode',
        'parsed_result': parsed_result
    }


# ============================================
# Streamlit UI (Phase 0.9)
# ============================================

def main():
    st.set_page_config(
        page_title="PRISM Phase 0.9",
        page_icon="ğŸ”·",
        layout="wide"
    )
    
    st.title("ğŸ”· PRISM Phase 0.9 \"LLM Rewriting View\"")
    st.caption("3ë‹¨ ê³„ì¸µ ë³´ê¸° + AI ê°€ë…ì„± ê°•í™” (GPT ì•ˆì „ì¥ì¹˜ ì™„ë¹„)")
    
    # ì‚¬ì´ë“œë°”: ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # LawMode í† ê¸€
        use_law_mode = st.checkbox(
            "ğŸ“œ LawMode ì‚¬ìš© (ê·œì •/ë²•ë ¹ ì „ìš©)",
            value=LAW_MODE_AVAILABLE,
            disabled=not LAW_MODE_AVAILABLE,
            help="PDF í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™•í•œ ì¡°ë¬¸ ì¶”ì¶œ"
        )
        
        if not LAW_MODE_AVAILABLE:
            st.warning("âš ï¸ LawParser ë¯¸ì„¤ì¹˜")
        
        st.divider()
        
        # âœ… Phase 0.9: LLM ë¦¬ë¼ì´íŒ… ì„¤ì •
        st.subheader("âœ¨ AI ê°€ë…ì„± ê°•í™” (Phase 0.9)")
        
        enable_llm_rewrite = st.checkbox(
            "AI ë¦¬ë¼ì´íŒ… ì‚¬ìš©",
            value=LLM_REWRITER_AVAILABLE,
            disabled=not LLM_REWRITER_AVAILABLE,
            help="ì¡°ë¬¸ ë‹¨ìœ„ LLM ë¦¬ë¼ì´íŒ… (ìºì‹œ í™œìš©)"
        )
        
        if not LLM_REWRITER_AVAILABLE:
            st.warning("âš ï¸ LLMRewriter ë¯¸ì„¤ì¹˜")
        
        if enable_llm_rewrite:
            llm_provider = st.selectbox(
                "LLM Provider",
                ["azure_openai", "anthropic"],
                help="ë¦¬ë¼ì´íŒ…ì— ì‚¬ìš©í•  LLM"
            )
            
            st.info("ğŸ’¡ API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”")
            st.code("""
# Azure OpenAI
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_ENDPOINT=...
AZURE_OPENAI_DEPLOYMENT=gpt-4

# Anthropic
ANTHROPIC_API_KEY=...
            """, language="bash")
        
        st.divider()
        
        # Phase 0.9 ë³€ê²½ì‚¬í•­
        with st.expander("âœ¨ Phase 0.9 ì‹ ê·œ ê¸°ëŠ¥"):
            st.markdown("""
            **3ë‹¨ ê³„ì¸µ ë³´ê¸° ëª¨ë“œ:**
            
            1. **ì›ë³¸ PDF í…ìŠ¤íŠ¸**
               - PDFì—ì„œ ì¶”ì¶œí•œ ì›ë³¸
            
            2. **ì—”ì§„ ì²˜ë¦¬ í…ìŠ¤íŠ¸**
               - LawParser íŒŒì‹± ê²°ê³¼
               - RAG/ê²€ìƒ‰ ê¸°ì¤€
            
            3. **AI ê°€ë…ì„± ê°•í™”** âœ¨ NEW
               - LLM ë¦¬ë¼ì´íŒ… ê²°ê³¼
               - ë„ì–´ì“°ê¸° ìì—°ìŠ¤ëŸ½ê²Œ ê°œì„ 
               - **ë²•ì  íš¨ë ¥ì€ ì›ë³¸ ê¸°ì¤€**
            
            ---
            
            **GPT ê¶Œì¥ ì•ˆì „ì¥ì¹˜:**
            
            âœ… ì—”ì§„ JSON ì ˆëŒ€ ë¶ˆë³€
            âœ… Sanity Check ìë™ ê²€ì¦
            âœ… ì›ë³¸ í•­ìƒ ë…¸ì¶œ
            âœ… ì¡°ë¬¸ ë‹¨ìœ„ + ìºì‹œ (ì†ë„/ë¹„ìš© ì ˆê°)
            
            ---
            
            **Sanity Check (4ì¢…):**
            
            1. ì¡°ë¬¸ í—¤ë” ë³´ì¡´ í™•ì¸
            2. ìˆ«ì/ë‚ ì§œ ë³€ê²½ ê°ì§€
            3. ë²•ë¥  ìš©ì–´ ëˆ„ë½ ê°ì§€
            4. ì¡°ë¬¸ êµ¬ì¡° ë³´ì¡´ í™•ì¸
            """)
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ",
        type=['pdf'],
        help="ê·œì •/ë²•ë ¹ ë¬¸ì„œ ê¶Œì¥ (LawMode)"
    )
    
    if not uploaded_file:
        st.info("ğŸ‘† PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”")
        
        # ìƒ˜í”Œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ì˜µì…˜)
        with st.expander("ğŸ“– Phase 0.9 ìƒ˜í”Œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"):
            st.markdown("""
            ### Before (Phase 0.7 - ë£° ê¸°ë°˜)
            ```
            ì´ê·œì •ì€í•œêµ­ë†ì–´ì´Œê³µì‚¬ì§ì›ì—ê²Œì„ìš©ìŠ¹ì§„ë³´ìˆ˜ë³µë¬´...
            ```
            
            ### After (Phase 0.9 - AI ë¦¬ë¼ì´íŒ…)
            ```
            ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬ ì§ì›ì—ê²Œ ì„ìš©, ìŠ¹ì§„, ë³´ìˆ˜, ë³µë¬´ ë“±
            ì¸ì‚¬ ì „ë°˜ì˜ ê¸°ì¤€ì„ ì •í•˜ì—¬ í•©ë¦¬ì ì´ê³  ì¼ê´€ëœ ì¸ì‚¬ ìš´ì˜ì„ ëª©í‘œë¡œ í•œë‹¤.
            ```
            
            **ê°œì„  ì‚¬í•­:**
            - âœ… ìì—°ìŠ¤ëŸ¬ìš´ ë„ì–´ì“°ê¸°
            - âœ… ì½ê¸° í¸í•œ ë¬¸ì¥ íë¦„
            - âœ… ë²•ë¥  ìš©ì–´ 100% ë³´ì¡´
            - âœ… ì˜ë¯¸ ë³€ê²½ ì—†ìŒ
            """)
        
        return
    
    # ë¬¸ì„œ ì²˜ë¦¬
    try:
        pdf_path = safe_temp_path('.pdf')
        with open(pdf_path, 'wb') as f:
            f.write(uploaded_file.read())
        
        pdf_text = extract_pdf_text_layer(pdf_path)
        
        if not pdf_text:
            st.error("âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            return
        
        # ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ
        if use_law_mode:
            result = process_document_law_mode(
                pdf_path=pdf_path,
                pdf_text=pdf_text,
                document_title=uploaded_file.name
            )
        else:
            result = process_document_vlm_mode(
                pdf_path=pdf_path,
                pdf_text=pdf_text
            )
        
        # ê²°ê³¼ í‘œì‹œ
        st.success(f"âœ… {result['mode']} ì²˜ë¦¬ ì™„ë£Œ!")
        
        # QA ê²°ê³¼
        match_rate = result['qa_result']['match_rate']
        is_qa_pass = result['is_qa_pass']
        
        if is_qa_pass:
            st.success(f"ğŸ¯ DualQA í†µê³¼: {match_rate:.1%} ë§¤ì¹­")
        else:
            st.warning(f"âš ï¸ DualQA ê²€í†  í•„ìš”: {match_rate:.1%} ë§¤ì¹­")
        
        # âœ… Phase 0.9: LLM ë¦¬ë¼ì´íŒ… (ì˜µì…˜)
        rewritten_result = None
        if enable_llm_rewrite and LLM_REWRITER_AVAILABLE:
            with st.spinner("âœ¨ AI ë¦¬ë¼ì´íŒ… ì¤‘... (ì¡°ë¬¸ ë‹¨ìœ„ ì²˜ë¦¬)"):
                try:
                    rewriter = LLMRewriter(
                        provider=llm_provider,
                        cache_enabled=True,
                        sanity_check_enabled=True
                    )
                    
                    rewritten_result = rewrite_articles_with_llm(
                        chunks=result['chunks'],
                        rewriter=rewriter,
                        document_id=uploaded_file.name
                    )
                    
                    # ê²€ì¦ ê²°ê³¼ í‘œì‹œ
                    val_summary = rewritten_result['validation_summary']
                    
                    if val_summary['pass_rate'] >= 0.95:
                        st.success(f"âœ… Sanity Check í†µê³¼: {val_summary['pass_rate']:.1%} ({val_summary['passed']}/{val_summary['total']})")
                    else:
                        st.warning(f"âš ï¸ Sanity Check ê²½ê³ : {val_summary['pass_rate']:.1%} ({val_summary['passed']}/{val_summary['total']})")
                        
                        # ì‹¤íŒ¨í•œ ì¡°ë¬¸ í‘œì‹œ
                        failed_articles = [
                            v['article'] for v in val_summary['details'] 
                            if not v['is_valid']
                        ]
                        if failed_articles:
                            st.write(f"**ê²€ì¦ ì‹¤íŒ¨ ì¡°ë¬¸**: {', '.join(failed_articles)}")
                
                except Exception as e:
                    logger.error(f"âŒ ë¦¬ë¼ì´íŒ… ì‹¤íŒ¨: {e}")
                    st.error(f"âŒ AI ë¦¬ë¼ì´íŒ… ì‹¤íŒ¨: {e}")
                    st.info("ğŸ’¡ ì›ë³¸/ì—”ì§„ í…ìŠ¤íŠ¸ëŠ” ì •ìƒ í‘œì‹œë©ë‹ˆë‹¤")
        
        # âœ… Phase 0.9: 3ë‹¨ ê³„ì¸µ íƒ­
        if rewritten_result:
            tab_names = ["ğŸ“Š ìš”ì•½", "ğŸ“ ì›ë³¸", "âš™ï¸ ì—”ì§„ í…ìŠ¤íŠ¸", "âœ¨ AI ê°€ë…ì„±", "ğŸ“¦ JSON ì²­í¬"]
        else:
            tab_names = ["ğŸ“Š ìš”ì•½", "ğŸ“ ì›ë³¸", "âš™ï¸ ì—”ì§„ í…ìŠ¤íŠ¸", "ğŸ“¦ JSON ì²­í¬"]
        
        tabs = st.tabs(tab_names)
        
        # Tab 1: ìš”ì•½
        with tabs[0]:
            st.subheader("ğŸ“Š ì²˜ë¦¬ ìš”ì•½")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ì²˜ë¦¬ ëª¨ë“œ", result['mode'])
                st.metric("DualQA ë§¤ì¹­ë¥ ", f"{match_rate:.1%}")
            
            with col2:
                st.metric("QA í†µê³¼", "âœ…" if is_qa_pass else "âš ï¸")
                st.metric("ì´ ì²­í¬ ìˆ˜", len(result['chunks']))
            
            with col3:
                if result['mode'] == 'LawMode' and 'parsed_result' in result:
                    parsed = result['parsed_result']
                    st.metric("ì¥ ìˆ˜", parsed['total_chapters'])
                    st.metric("ì¡°ë¬¸ ìˆ˜", parsed['total_articles'])
                
                if rewritten_result:
                    val_summary = rewritten_result['validation_summary']
                    st.metric("AI ë¦¬ë¼ì´íŒ…", f"{val_summary['pass_rate']:.0%}")
        
        # Tab 2: ì›ë³¸ PDF í…ìŠ¤íŠ¸
        with tabs[1]:
            st.subheader("ğŸ“ ì›ë³¸ PDF í…ìŠ¤íŠ¸")
            st.info("âš ï¸ ì´ê²ƒì´ ë²•ì  íš¨ë ¥ì„ ê°€ì§€ëŠ” ê¸°ì¤€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤")
            
            st.text_area(
                "PDF ì¶”ì¶œ ì›ë³¸",
                value=pdf_text[:3000] + "..." if len(pdf_text) > 3000 else pdf_text,
                height=400
            )
            
            st.download_button(
                "ğŸ’¾ ì›ë³¸ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                data=pdf_text,
                file_name=f"{uploaded_file.name}_original.txt",
                mime="text/plain"
            )
        
        # Tab 3: ì—”ì§„ ì²˜ë¦¬ í…ìŠ¤íŠ¸
        with tabs[2]:
            st.subheader("âš™ï¸ ì—”ì§„ ì²˜ë¦¬ í…ìŠ¤íŠ¸")
            st.info("ğŸ” RAG/ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ê¸°ì¤€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤")
            
            review_md = to_review_md(result['chunks'])
            
            st.markdown(review_md)
            
            st.download_button(
                "ğŸ’¾ ì—”ì§„ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                data=review_md,
                file_name=f"{uploaded_file.name}_engine_phase09.md",
                mime="text/markdown"
            )
        
        # Tab 4: AI ê°€ë…ì„± ê°•í™” (Phase 0.9)
        if rewritten_result:
            with tabs[3]:
                st.subheader("âœ¨ AI ê°€ë…ì„± ê°•í™”")
                
                # âœ… GPT í•„ìˆ˜: ë²•ì  íš¨ë ¥ í‘œì‹œ
                st.warning("âš ï¸ **ë²•ì  íš¨ë ¥ì€ ì›ë³¸ ê¸°ì¤€ì…ë‹ˆë‹¤.** ì´ í…ìŠ¤íŠ¸ëŠ” ê°€ë…ì„± ê°œì„  ëª©ì ì…ë‹ˆë‹¤.")
                
                # Before/After ë¹„êµ (ìƒ˜í”Œ)
                st.markdown("### ğŸ“Š ê°œì„  ë¹„êµ (ìƒ˜í”Œ)")
                
                col1, col2 = st.columns(2)
                
                # ì²« ì¡°ë¬¸ ì°¾ê¸°
                first_article_idx = next(
                    (i for i, c in enumerate(result['chunks']) 
                     if c['metadata']['type'] == 'article'),
                    None
                )
                
                if first_article_idx is not None:
                    original_chunk = result['chunks'][first_article_idx]
                    rewritten_chunk = rewritten_result['rewritten_chunks'][first_article_idx]
                    
                    with col1:
                        st.markdown("**Before (ì—”ì§„ í…ìŠ¤íŠ¸):**")
                        st.text_area(
                            "ì›ë³¸",
                            value=original_chunk['content'][:300] + "...",
                            height=200,
                            key="before_sample"
                        )
                    
                    with col2:
                        st.markdown("**After (AI ë¦¬ë¼ì´íŒ…):**")
                        st.text_area(
                            "ê°œì„ ",
                            value=rewritten_chunk['content'][:300] + "...",
                            height=200,
                            key="after_sample"
                        )
                
                st.markdown("---")
                st.markdown("### ğŸ“– ì „ì²´ AI ë¦¬ë¼ì´íŒ… ê²°ê³¼")
                
                # ì „ì²´ ë¦¬ë¼ì´íŒ… ê²°ê³¼
                rewritten_md = to_review_md(rewritten_result['rewritten_chunks'])
                st.markdown(rewritten_md)
                
                st.download_button(
                    "ğŸ’¾ AI ë¦¬ë¼ì´íŒ… ë‹¤ìš´ë¡œë“œ",
                    data=rewritten_md,
                    file_name=f"{uploaded_file.name}_ai_rewritten_phase09.md",
                    mime="text/markdown"
                )
                
                # Sanity Check ìƒì„¸ ê²°ê³¼
                with st.expander("ğŸ”¬ Sanity Check ìƒì„¸ ê²°ê³¼"):
                    val_summary = rewritten_result['validation_summary']
                    
                    st.write(f"**í†µê³¼ìœ¨**: {val_summary['pass_rate']:.1%}")
                    st.write(f"**í†µê³¼**: {val_summary['passed']}ê°œ")
                    st.write(f"**ì‹¤íŒ¨**: {val_summary['failed']}ê°œ")
                    
                    if val_summary['failed'] > 0:
                        st.write("**ì‹¤íŒ¨ ìƒì„¸:**")
                        for detail in val_summary['details']:
                            if not detail['is_valid']:
                                st.write(f"- {detail['article']}: {', '.join(detail['warnings'])}")
        
        # Tab 5: JSON ì²­í¬
        with tabs[-1]:
            st.subheader("ğŸ“¦ JSON ì²­í¬")
            
            chunk_types = {}
            for chunk in result['chunks']:
                chunk_type = chunk['metadata']['type']
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
            
            st.write(f"**ì´ ì²­í¬ ìˆ˜**: {len(result['chunks'])}ê°œ")
            st.write(f"**ì²­í¬ íƒ€ì… ë¶„í¬**: {chunk_types}")
            
            st.json(result['chunks'], expanded=False)
            
            st.download_button(
                "ğŸ’¾ JSON ë‹¤ìš´ë¡œë“œ",
                data=json.dumps(result['chunks'], ensure_ascii=False, indent=2),
                file_name=f"{uploaded_file.name}_chunks_phase09.json",
                mime="application/json"
            )
    
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        if 'pdf_path' in locals():
            safe_remove(pdf_path)


if __name__ == '__main__':
    main()
