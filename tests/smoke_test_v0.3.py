"""
tests/smoke_test_v0.3.py
PRISM Phase 0.3 - Golden File Regression Test

âœ… Phase 0.3 í…ŒìŠ¤íŠ¸ ìë™í™”:
1. ê³¨ë“  íŒŒì¼ ê¸°ë°˜ íšŒê·€ ê²€ì¦
2. Diff ê¸°ë°˜ ë³€ê²½ ê°ì§€
3. ìë™ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦

ëª©í‘œ:
- ì²­í‚¹ ìˆ˜Â·DedupÂ·ë§ˆì»¤ ì œê±°Â·í—¤ë” ì¹´ìš´íŠ¸ ìë™ ì²´í¬
- ê¸´ ë¬¸ì„œ(10~30p) 3ì¢… íšŒê·€ ì„¸íŠ¸

Author: ì •ìˆ˜ì•„ (QA Lead) + GPT í”¼ë“œë°± ë°˜ì˜
Date: 2025-11-06
Version: Phase 0.3
"""

import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import difflib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class TestDocument:
    """í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì •ì˜"""
    id: str
    path: str
    description: str
    expected: Dict[str, Any]


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    doc_id: str
    passed: bool
    score: int
    details: Dict[str, Any]
    diffs: List[str]


class GoldenFileRegressionTest:
    """
    Phase 0.3 ê³¨ë“  íŒŒì¼ ê¸°ë°˜ íšŒê·€ í…ŒìŠ¤íŠ¸
    
    âœ… Phase 0.3 íŠ¹ì§•:
    - Markdown/JSON ìŠ¤ëƒ…ìƒ· ë¹„êµ
    - Diff ê¸°ë°˜ ë³€ê²½ ê°ì§€
    - ìë™ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦
    """
    
    # âœ… Phase 0.3: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„¸íŠ¸
    TEST_DOCUMENTS = [
        # ì§§ì€ ë¬¸ì„œ (ê¸°ì¤€)
        TestDocument(
            id='short_statute',
            path='tests/data/ì¸ì‚¬ê·œì •_ì¼ë¶€ê°œì •ì „ë¬¸-1-3_ì›ë³¸.pdf',
            description='ì§§ì€ ê·œì • (3í˜ì´ì§€, ê¸°ì¤€)',
            expected={
                'pages': 3,
                'revisions': 17,
                'has_preamble': True,
                'chunks': (6, 8),  # ìµœì†Œ-ìµœëŒ€
                'page_markers': 0,
                'vlm_success_rate': 0.9,
                'total_score': 95
            }
        ),
        
        # âœ… GPT ì œì•ˆ: ê¸´ ë¬¸ì„œ 3ì¢…
        TestDocument(
            id='long_with_tables',
            path='tests/data/statute_with_many_tables.pdf',
            description='ê¸´ ë¬¸ì„œ - í‘œ ë‹¤ìˆ˜ + ì¡°ë¬¸ ì ìŒ (10~15í˜ì´ì§€)',
            expected={
                'pages': (10, 15),
                'has_tables': True,
                'table_count': (5, 20),
                'chunks': (15, 25),
                'page_markers': 0,
                'vlm_success_rate': 0.85,
                'total_score': 90
            }
        ),
        
        TestDocument(
            id='long_complex_hierarchy',
            path='tests/data/statute_complex_hierarchy.pdf',
            description='ê¸´ ë¬¸ì„œ - ì¡°ë¬¸ ë§ìŒ + í•­/í˜¸ ë³µì¡ (20~25í˜ì´ì§€)',
            expected={
                'pages': (20, 25),
                'articles': (50, 100),
                'has_hierarchy': True,
                'chunks': (30, 50),
                'page_markers': 0,
                'vlm_success_rate': 0.85,
                'total_score': 90
            }
        ),
        
        TestDocument(
            id='long_mixed_content',
            path='tests/data/statute_with_appendix.pdf',
            description='ê¸´ ë¬¸ì„œ - ë³¸ë¬¸+ë¶€ë¡ í˜¼í•© (25~30í˜ì´ì§€)',
            expected={
                'pages': (25, 30),
                'has_appendix': True,
                'chunks': (40, 60),
                'page_markers': 0,
                'vlm_success_rate': 0.80,
                'total_score': 88
            }
        ),
    ]
    
    def __init__(self, golden_dir: str = 'tests/golden'):
        """
        ì´ˆê¸°í™”
        
        Args:
            golden_dir: ê³¨ë“  íŒŒì¼ ë””ë ‰í† ë¦¬
        """
        self.golden_dir = Path(golden_dir)
        self.golden_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("âœ… GoldenFileRegressionTest Phase 0.3 ì´ˆê¸°í™”")
        logger.info(f"   ğŸ“ ê³¨ë“  íŒŒì¼ ë””ë ‰í† ë¦¬: {self.golden_dir}")
        logger.info(f"   ğŸ“‹ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {len(self.TEST_DOCUMENTS)}ê°œ")
    
    def create_golden_files(self, doc: TestDocument, result: Dict[str, Any]) -> None:
        """
        ê³¨ë“  íŒŒì¼ ìƒì„±
        
        Args:
            doc: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
            result: ì²˜ë¦¬ ê²°ê³¼
        """
        golden_base = self.golden_dir / doc.id
        golden_base.mkdir(exist_ok=True)
        
        # Markdown ì €ì¥
        md_path = golden_base / 'output.md'
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(result.get('markdown', ''))
        
        # JSON ì €ì¥
        json_path = golden_base / 'output.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({
                'chunks': result.get('chunks', []),
                'metadata': result.get('metadata', {})
            }, f, ensure_ascii=False, indent=2)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        meta_path = golden_base / 'metadata.json'
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump({
                'doc_id': doc.id,
                'description': doc.description,
                'expected': doc.expected,
                'created_at': result.get('timestamp', '')
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   âœ… ê³¨ë“  íŒŒì¼ ìƒì„±: {doc.id}")
    
    def compare_with_golden(
        self,
        doc: TestDocument,
        result: Dict[str, Any]
    ) -> TestResult:
        """
        ê³¨ë“  íŒŒì¼ê³¼ ë¹„êµ
        
        Args:
            doc: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
            result: ì²˜ë¦¬ ê²°ê³¼
        
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        golden_base = self.golden_dir / doc.id
        
        if not golden_base.exists():
            logger.warning(f"   âš ï¸ ê³¨ë“  íŒŒì¼ ì—†ìŒ: {doc.id} (ìƒˆë¡œ ìƒì„± í•„ìš”)")
            return TestResult(
                doc_id=doc.id,
                passed=False,
                score=0,
                details={'error': 'no_golden_file'},
                diffs=[]
            )
        
        diffs = []
        checks = {}
        
        # Markdown ë¹„êµ
        md_path = golden_base / 'output.md'
        if md_path.exists():
            with open(md_path, 'r', encoding='utf-8') as f:
                golden_md = f.read()
            
            current_md = result.get('markdown', '')
            md_diff = self._compute_diff(golden_md, current_md)
            
            if md_diff:
                diffs.append(f"Markdown ì°¨ì´: {len(md_diff)}ì¤„")
                checks['markdown_match'] = False
            else:
                checks['markdown_match'] = True
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦
        checks.update(self._validate_checklist(doc, result))
        
        # ì ìˆ˜ ê³„ì‚°
        passed_count = sum(1 for v in checks.values() if v)
        total_count = len(checks)
        score = int((passed_count / total_count) * 100) if total_count > 0 else 0
        
        passed = score >= 95
        
        return TestResult(
            doc_id=doc.id,
            passed=passed,
            score=score,
            details=checks,
            diffs=diffs
        )
    
    def _compute_diff(self, golden: str, current: str) -> List[str]:
        """
        Diff ê³„ì‚°
        
        Args:
            golden: ê³¨ë“  í…ìŠ¤íŠ¸
            current: í˜„ì¬ í…ìŠ¤íŠ¸
        
        Returns:
            ì°¨ì´ì  ëª©ë¡
        """
        golden_lines = golden.splitlines()
        current_lines = current.splitlines()
        
        diff = list(difflib.unified_diff(
            golden_lines,
            current_lines,
            lineterm='',
            n=0
        ))
        
        # ë³€ê²½ëœ ì¤„ë§Œ ì¶”ì¶œ
        changes = [line for line in diff if line.startswith(('+', '-')) and not line.startswith(('+++', '---'))]
        
        return changes[:20]  # ìµœëŒ€ 20ì¤„
    
    def _validate_checklist(
        self,
        doc: TestDocument,
        result: Dict[str, Any]
    ) -> Dict[str, bool]:
        """
        âœ… Phase 0.3: ìë™ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦
        
        Args:
            doc: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
            result: ì²˜ë¦¬ ê²°ê³¼
        
        Returns:
            ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²°ê³¼
        """
        checks = {}
        expected = doc.expected
        metadata = result.get('metadata', {})
        
        # 1. í˜ì´ì§€ ìˆ˜
        if 'pages' in expected:
            if isinstance(expected['pages'], tuple):
                min_p, max_p = expected['pages']
                checks['page_count'] = min_p <= metadata.get('pages', 0) <= max_p
            else:
                checks['page_count'] = metadata.get('pages', 0) == expected['pages']
        
        # 2. ê°œì •ì´ë ¥
        if 'revisions' in expected:
            checks['revisions'] = metadata.get('revisions', 0) == expected['revisions']
        
        # 3. "ê¸°ë³¸ ì •ì‹ "
        if expected.get('has_preamble'):
            markdown = result.get('markdown', '')
            checks['has_preamble'] = 'ê¸°ë³¸ì •ì‹ ' in markdown or 'ê¸°ë³¸ ì •ì‹ ' in markdown
        
        # 4. ì²­í‚¹
        if 'chunks' in expected:
            min_c, max_c = expected['chunks']
            chunk_count = len(result.get('chunks', []))
            checks['chunk_count'] = min_c <= chunk_count <= max_c
        
        # 5. í˜ì´ì§€ ë§ˆì»¤
        if 'page_markers' in expected:
            markdown = result.get('markdown', '')
            # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì²´í¬
            import re
            markers = re.findall(r'\d{3,4}-\d{1,2}', markdown)
            checks['no_page_markers'] = len(markers) == expected['page_markers']
        
        # 6. VLM ì„±ê³µë¥ 
        if 'vlm_success_rate' in expected:
            vlm_rate = metadata.get('vlm_success_rate', 0)
            checks['vlm_success_rate'] = vlm_rate >= expected['vlm_success_rate']
        
        # 7. ì¢…í•© ì ìˆ˜
        if 'total_score' in expected:
            score = metadata.get('total_score', 0)
            checks['total_score'] = score >= expected['total_score']
        
        return checks
    
    def run_regression_test(self) -> Dict[str, Any]:
        """
        íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        """
        logger.info("ğŸ§ª Phase 0.3 íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info(f"   ğŸ“‹ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {len(self.TEST_DOCUMENTS)}ê°œ")
        
        results = []
        
        for doc in self.TEST_DOCUMENTS:
            logger.info(f"\n   ğŸ” í…ŒìŠ¤íŠ¸: {doc.id} - {doc.description}")
            
            # íŒŒì¼ ì¡´ì¬ ì²´í¬
            if not Path(doc.path).exists():
                logger.warning(f"      âš ï¸ íŒŒì¼ ì—†ìŒ: {doc.path} (ìŠ¤í‚µ)")
                continue
            
            # ì²˜ë¦¬ ì‹¤í–‰ (ì‹¤ì œ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ)
            try:
                result = self._process_document(doc)
                test_result = self.compare_with_golden(doc, result)
                results.append(test_result)
                
                # ê²°ê³¼ ì¶œë ¥
                status = "âœ… PASS" if test_result.passed else "âŒ FAIL"
                logger.info(f"      {status} - ì ìˆ˜: {test_result.score}/100")
                
                if test_result.diffs:
                    logger.info(f"      ğŸ“ ì°¨ì´ì : {len(test_result.diffs)}ê±´")
            
            except Exception as e:
                logger.error(f"      âŒ ì˜¤ë¥˜: {e}")
        
        # ìš”ì•½
        passed = sum(1 for r in results if r.passed)
        total = len(results)
        
        logger.info(f"\nğŸ¯ íšŒê·€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
        logger.info(f"   âœ… í†µê³¼: {passed}/{total}")
        logger.info(f"   í‰ê·  ì ìˆ˜: {sum(r.score for r in results) / total:.1f}/100" if total > 0 else "   N/A")
        
        return {
            'passed': passed,
            'total': total,
            'results': results
        }
    
    def _process_document(self, doc: TestDocument) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ì²˜ë¦¬ (ì‹¤ì œ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ)
        
        Args:
            doc: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        # TODO: ì‹¤ì œ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ
        # from core.pipeline import Phase53Pipeline
        # pipeline = Phase53Pipeline(...)
        # result = pipeline.process(doc.path)
        
        # í˜„ì¬ëŠ” ë”ë¯¸ ê²°ê³¼ ë°˜í™˜
        return {
            'markdown': '',
            'chunks': [],
            'metadata': {
                'pages': 3,
                'revisions': 17,
                'vlm_success_rate': 1.0,
                'total_score': 98
            },
            'timestamp': '2025-11-06T18:41:36'
        }


if __name__ == '__main__':
    # íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = GoldenFileRegressionTest()
    summary = tester.run_regression_test()
    
    # ê²°ê³¼ ì¶œë ¥
    if summary['passed'] == summary['total']:
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        sys.exit(0)
    else:
        print(f"\nâŒ {summary['total'] - summary['passed']}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1)
