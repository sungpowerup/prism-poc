"""
benchmark_golden.py - PRISM Phase 0.8 Performance Benchmark
Golden File ìƒì„±/ë¹„êµ ì„±ëŠ¥ ì¸¡ì •

Usage:
    python tests/benchmark_golden.py

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ (í™©íƒœë¯¼ DevOps Lead)
Date: 2025-11-14
Version: Phase 0.8
"""

import sys
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# PRISM ëª¨ë“ˆ import
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.law_parser import LawParser
from core.dual_qa_gate import extract_pdf_text_layer
from tests.golden_schema import create_golden_from_parsed_result, GoldenMetadata, GoldenFile
from tests.golden_diff_engine import GoldenDiffEngine

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    
    def __init__(self, name: str):
        self.name = name
        self.parse_time = 0.0
        self.golden_create_time = 0.0
        self.golden_compare_time = 0.0
        self.total_time = 0.0
        self.memory_mb = 0.0
        self.document_stats = {}
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'name': self.name,
            'parse_time_sec': round(self.parse_time, 3),
            'golden_create_time_sec': round(self.golden_create_time, 3),
            'golden_compare_time_sec': round(self.golden_compare_time, 3),
            'total_time_sec': round(self.total_time, 3),
            'memory_mb': round(self.memory_mb, 2),
            'document_stats': self.document_stats
        }


def benchmark_single_document(pdf_path: str, doc_name: str) -> BenchmarkResult:
    """ë‹¨ì¼ ë¬¸ì„œ ë²¤ì¹˜ë§ˆí¬"""
    
    logger.info(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {doc_name}")
    
    result = BenchmarkResult(doc_name)
    start_total = time.time()
    
    # ë©”ëª¨ë¦¬ ì¸¡ì • ì‹œì‘
    import psutil
    import os
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 / 1024  # MB
    
    try:
        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ + íŒŒì‹±
        start = time.time()
        pdf_text = extract_pdf_text_layer(pdf_path)
        parser = LawParser()
        parsed_result = parser.parse(
            pdf_text=pdf_text,
            document_title=doc_name,
            clean_artifacts=True,
            normalize_linebreaks=True
        )
        result.parse_time = time.time() - start
        
        result.document_stats = {
            'total_chapters': parsed_result['total_chapters'],
            'total_articles': parsed_result['total_articles'],
            'text_length': len(pdf_text)
        }
        
        # 2. Golden File ìƒì„±
        start = time.time()
        metadata = GoldenMetadata(
            parser_version="0.8.0",
            schema_version="1.0",
            document_title=doc_name,
            document_type="benchmark",
            golden_verified_date=datetime.now().strftime("%Y-%m-%d"),
            golden_verified_by="ìë™ ë²¤ì¹˜ë§ˆí¬",
            source_file=Path(pdf_path).name,
            page_count=3
        )
        golden = create_golden_from_parsed_result(parsed_result, metadata)
        result.golden_create_time = time.time() - start
        
        # 3. Golden File ë¹„êµ
        start = time.time()
        diff_engine = GoldenDiffEngine()
        report = diff_engine.compare(
            golden=golden.to_dict(),
            result=parsed_result
        )
        result.golden_compare_time = time.time() - start
        
        # ë©”ëª¨ë¦¬ ì¸¡ì • ì¢…ë£Œ
        mem_after = process.memory_info().rss / 1024 / 1024
        result.memory_mb = mem_after - mem_before
        
        result.total_time = time.time() - start_total
        
        logger.info(f"   âœ… ì™„ë£Œ: {result.total_time:.2f}ì´ˆ")
        logger.info(f"      - íŒŒì‹±: {result.parse_time:.2f}ì´ˆ")
        logger.info(f"      - Golden ìƒì„±: {result.golden_create_time:.2f}ì´ˆ")
        logger.info(f"      - ë¹„êµ: {result.golden_compare_time:.2f}ì´ˆ")
        logger.info(f"      - ë©”ëª¨ë¦¬: {result.memory_mb:.2f}MB")
        
        return result
    
    except Exception as e:
        logger.error(f"   âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        raise


def run_benchmark_suite() -> Dict[str, Any]:
    """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
    
    print("\n" + "="*60)
    print("ğŸƒ PRISM Phase 0.8 Performance Benchmark")
    print("="*60)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡
    test_documents = [
        {
            'path': 'ì¸ì‚¬ê·œì •_ì¼ë¶€ê°œì •ì „ë¬¸-1-3_ì›ë³¸.pdf',
            'name': 'ì¸ì‚¬ê·œì • (Standard)'
        },
        # ì¶”ê°€ ë¬¸ì„œëŠ” ì—¬ê¸° ì¶”ê°€
    ]
    
    results = []
    
    for doc in test_documents:
        try:
            if Path(doc['path']).exists():
                result = benchmark_single_document(doc['path'], doc['name'])
                results.append(result)
            else:
                logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {doc['path']}")
        except Exception as e:
            logger.error(f"âŒ {doc['name']} ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
    
    # í†µê³„ ê³„ì‚°
    if results:
        avg_parse_time = sum(r.parse_time for r in results) / len(results)
        avg_golden_time = sum(r.golden_create_time for r in results) / len(results)
        avg_compare_time = sum(r.golden_compare_time for r in results) / len(results)
        avg_total_time = sum(r.total_time for r in results) / len(results)
        avg_memory = sum(r.memory_mb for r in results) / len(results)
    else:
        avg_parse_time = avg_golden_time = avg_compare_time = avg_total_time = avg_memory = 0.0
    
    # ê²°ê³¼ ìš”ì•½
    summary = {
        'timestamp': datetime.now().isoformat(),
        'total_documents': len(results),
        'average_times': {
            'parse_sec': round(avg_parse_time, 3),
            'golden_create_sec': round(avg_golden_time, 3),
            'golden_compare_sec': round(avg_compare_time, 3),
            'total_sec': round(avg_total_time, 3)
        },
        'average_memory_mb': round(avg_memory, 2),
        'results': [r.to_dict() for r in results]
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    print(f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {len(results)}ê°œ")
    print(f"í‰ê·  íŒŒì‹± ì‹œê°„: {avg_parse_time:.2f}ì´ˆ")
    print(f"í‰ê·  Golden ìƒì„±: {avg_golden_time:.2f}ì´ˆ")
    print(f"í‰ê·  ë¹„êµ ì‹œê°„: {avg_compare_time:.2f}ì´ˆ")
    print(f"í‰ê·  ì „ì²´ ì‹œê°„: {avg_total_time:.2f}ì´ˆ")
    print(f"í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©: {avg_memory:.2f}MB")
    print("="*60)
    
    # ì„±ëŠ¥ ê¸°ì¤€ ì²´í¬
    print("\nğŸ¯ ì„±ëŠ¥ ê¸°ì¤€ ì²´í¬:")
    
    checks = {
        'íŒŒì‹± ì‹œê°„ < 5ì´ˆ': avg_parse_time < 5.0,
        'Golden ìƒì„± < 2ì´ˆ': avg_golden_time < 2.0,
        'ë¹„êµ ì‹œê°„ < 1ì´ˆ': avg_compare_time < 1.0,
        'ì „ì²´ ì‹œê°„ < 10ì´ˆ': avg_total_time < 10.0,
        'ë©”ëª¨ë¦¬ < 500MB': avg_memory < 500.0
    }
    
    all_pass = True
    for check, passed in checks.items():
        status = "âœ…" if passed else "âŒ"
        print(f"   {status} {check}")
        if not passed:
            all_pass = False
    
    if all_pass:
        print("\nâœ… ëª¨ë“  ì„±ëŠ¥ ê¸°ì¤€ í†µê³¼!")
    else:
        print("\nâš ï¸ ì¼ë¶€ ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬")
    
    return summary


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    try:
        summary = run_benchmark_suite()
        
        # ê²°ê³¼ ì €ì¥
        output_path = Path('tests/benchmark_results.json')
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # CI í™˜ê²½ì—ì„œ ì‹¤íŒ¨ ì‹œ exit code 1
        if not all([
            summary['average_times']['parse_sec'] < 5.0,
            summary['average_times']['golden_create_sec'] < 2.0,
            summary['average_times']['golden_compare_sec'] < 1.0,
            summary['average_times']['total_sec'] < 10.0,
            summary['average_memory_mb'] < 500.0
        ]):
            print("\nâŒ ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬ë¡œ ì‹¤íŒ¨")
            sys.exit(1)
        
        print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        sys.exit(0)
    
    except Exception as e:
        logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()
