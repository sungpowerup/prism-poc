# research/table_parser.py
"""
research/table_parser.py - PRISM Phase 0.9 ì—°êµ¬ìš©
Annex í‘œ êµ¬ì¡°í™” íŒŒì„œ (ì‹¤í—˜ ì½”ë“œ)

âš ï¸ ì´ íŒŒì¼ì€ ì—°êµ¬/ì‹¤í—˜ìš©ì…ë‹ˆë‹¤.
âš ï¸ core/ ì œí’ˆ ë¼ì¸ì— ì§ì ‘ ì—°ê²°í•˜ì§€ ë§ˆì„¸ìš”.
âš ï¸ Golden Set 100% ì •í™•ë„ ë‹¬ì„± ì „ê¹Œì§€ ì œí’ˆì— í¸ì… ê¸ˆì§€.

ì—­í• :
- annex_table_rows í…ìŠ¤íŠ¸ ì²­í¬ â†’ í–‰ ë‹¨ìœ„ êµ¬ì¡°í™” ì‹¤í—˜
- Golden Set ê¸°ë°˜ ì •í™•ë„ ê²€ì¦

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ (ë°•ì¤€í˜¸ AI/ML Lead)
Date: 2025-11-18
Version: Phase 0.9.0 (ì—°êµ¬ìš© ìŠ¤ì¼ˆë ˆí†¤)
"""

import re
import logging
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Tuple

logger = logging.getLogger(__name__)


# ==============================
# Dataclasses (CHUNK_SCHEMA ë°˜ì˜)
# ==============================

@dataclass
class TableOverviewChunk:
    """í‘œ ì „ì²´ ì„¤ëª… ì²­í¬"""
    chunk_type: str
    table_id: str
    title: str
    description: str
    column_names: List[str]
    row_count: int
    metadata: Dict[str, Any]


@dataclass
class TableRowChunk:
    """í–‰ ë‹¨ìœ„ ì²­í¬ - LLMì´ ë‹µì„ ì°¾ëŠ” ìµœì†Œ ë‹¨ìœ„"""
    chunk_type: str
    table_id: str
    row_index: int
    columns: Dict[str, Any]
    content: str
    metadata: Dict[str, Any]


@dataclass
class TableNoteChunk:
    """í‘œ ì£¼ì„ ì²­í¬"""
    chunk_type: str
    table_id: str
    note_index: int
    content: str
    note_type: str
    metadata: Dict[str, Any]


class TableParser:
    """
    Phase 0.9 - Annex í‘œ êµ¬ì¡°í™” íŒŒì„œ
    
    ì±…ì„:
    - annex_table_rows í…ìŠ¤íŠ¸ ì²­í¬ -> í–‰ ë‹¨ìœ„ êµ¬ì¡°í™”
    - chunks.json / review.md / engine.md ìƒì„±ìš© ë°ì´í„° ë°˜í™˜
    
    âœ… GPT í”¼ë“œë°± ë°˜ì˜:
    - ìŠ¤í™/ìŠ¤í‚¤ë§ˆ/ì—­í•  ë²”ìœ„ í™•ì •
    - ì•Œê³ ë¦¬ì¦˜ì€ Week 1~2ì—ì„œ êµ¬í˜„
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("âœ… TableParser v0.9.0 ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ---------- public API ----------
    
    def parse_annex_table(self, annex_chunk: Dict[str, Any]) -> Dict[str, Any]:
        """
        AnnexSubChunkerê°€ ë§Œë“¤ì–´ë‚¸ annex_table_rows ì²­í¬ë¥¼ êµ¬ì¡°í™”í•œë‹¤.
        
        Args:
            annex_chunk: {
                "content": "...",
                "metadata": {
                    "type": "annex_table_rows",
                    "table_title": "3ê¸‰ìŠ¹ì§„ì œì™¸",
                    "section_id": "annex_1_table_1",
                    ...
                }
            }
        
        Returns:
            {
                "table_overview": TableOverviewChunk,
                "table_rows": List[TableRowChunk],
                "table_notes": List[TableNoteChunk]
            }
        """
        raw_text = annex_chunk.get("content", "")
        meta = annex_chunk.get("metadata", {})
        
        logger.info("ğŸ“Š TableParser.parse_annex_table ì‹œì‘")
        logger.info(f"   - section_id: {meta.get('section_id')}")
        logger.info(f"   - table_title: {meta.get('table_title')}")
        logger.info(f"   - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_text)}ì")
        
        # ì „ì²˜ë¦¬
        cleaned = self._preprocess_text(raw_text)
        
        # 1) ìˆ«ì/ë²”ìœ„ í† í° ì¶”ì¶œ
        people_list, rank_list = self._extract_columns(cleaned)
        
        logger.info(f"   - ì¶”ì¶œëœ people: {len(people_list)}ê°œ")
        logger.info(f"   - ì¶”ì¶œëœ rank: {len(rank_list)}ê°œ")
        
        # 2) í˜ì–´ë§ (people â†” rank_max)
        rows = self._pair_rows(people_list, rank_list)
        
        # 3) overview / row chunks / note chunks ìƒì„±
        table_id = f"annex1_promotion_range_{meta.get('table_title', 'unknown')}"
        overview = self._build_overview_chunk(table_id, meta, rows)
        row_chunks = self._build_row_chunks(table_id, meta, rows)
        note_chunks = self._build_note_chunks(table_id, cleaned, meta)
        
        logger.info(f"âœ… TableParser.parse_annex_table ì™„ë£Œ: rows={len(row_chunks)}, notes={len(note_chunks)}")
        
        return {
            "table_overview": overview,
            "table_rows": row_chunks,
            "table_notes": note_chunks,
        }
    
    def to_markdown_table(self, parsed: Dict[str, Any]) -> str:
        """
        íŒŒì‹± ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³€í™˜ (review.mdìš©)
        
        Args:
            parsed: parse_annex_table() ë°˜í™˜ê°’
        
        Returns:
            ë§ˆí¬ë‹¤ìš´ í‘œ ë¬¸ìì—´
        """
        overview = parsed["table_overview"]
        rows = parsed["table_rows"]
        notes = parsed["table_notes"]
        
        lines = []
        
        # ì œëª©
        lines.append(f"## {overview.title}")
        lines.append("")
        
        # ì„¤ëª…
        lines.append(f"> {overview.description}")
        lines.append("")
        
        # í‘œ í—¤ë”
        col_names = overview.column_names
        lines.append(f"| {col_names[0]} | {col_names[1]} |")
        lines.append("|------------|----------------|")
        
        # í‘œ ë³¸ë¬¸
        for row in rows:
            people = row.columns["people"]
            rank_max = row.columns["rank_max"]
            lines.append(f"| {people} | {rank_max}ë²ˆê¹Œì§€ |")
        
        lines.append("")
        
        # ì£¼ì„
        for note in notes:
            lines.append(f"â€» {note.content}")
        
        return "\n".join(lines)
    
    # ---------- ë‚´ë¶€ ë‹¨ê³„: ì „ì²˜ë¦¬ ----------
    
    def _preprocess_text(self, text: str) -> str:
        """
        1) í˜ì´ì§€ decorative ë¬¸ì ì œê±°
        2) ë¶ˆí•„ìš”í•œ ê³µë°±/ì¤‘ë³µ ê°œí–‰ ì •ë¦¬
        """
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        text = text.replace("", "")
        text = text.replace("\uf0d8", "")
        
        # ì¤‘ë³µ ê°œí–‰ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì œê±° (ì˜ˆ: 402-26)
        text = re.sub(r'\d{3}-\d{1,2}', '', text)
        
        return text
    
    # ---------- ë‚´ë¶€ ë‹¨ê³„: ì—´ ì¶”ì¶œ ----------
    
    def _extract_columns(self, text: str) -> Tuple[List[int], List[int]]:
        """
        people(ì„ìš© ì¸ì›ìˆ˜) / rank_max("në²ˆê¹Œì§€") ë‘ ì—´ì„ ë¶„ë¦¬í•´ì„œ ì¶”ì¶œí•œë‹¤.
        
        Phase 0.9.1: 2ì—´ ìˆ«ì+ë²”ìœ„ íŒ¨í„´ì—ë§Œ ì§‘ì¤‘
        
        Returns:
            people_list: [1,2,3,...,75]
            rank_list: [5,10,15,...,235]
        """
        lines = [l.strip() for l in text.splitlines() if l.strip()]
        
        # ìˆ«ìë§Œ ìˆëŠ” ë¼ì¸ (ì„ìš© ì¸ì›ìˆ˜)
        number_lines = [l for l in lines if re.fullmatch(r"\d+", l)]
        
        # "në²ˆê¹Œì§€" íŒ¨í„´ ë¼ì¸ (ì„œì—´ ìƒí•œ)
        range_lines = [l for l in lines if re.fullmatch(r"\d+ë²ˆê¹Œì§€", l)]
        
        # ìˆ«ì ì¶”ì¶œ
        people = []
        for x in number_lines:
            try:
                num = int(x)
                # 1~100 ë²”ìœ„ì˜ ìˆ«ìë§Œ peopleë¡œ ê°„ì£¼ (ì„œì—´ ìˆ«ìì™€ êµ¬ë¶„)
                if 1 <= num <= 100:
                    people.append(num)
            except ValueError:
                continue
        
        # ë²”ìœ„ ìˆ«ì ì¶”ì¶œ
        ranks = []
        for x in range_lines:
            match = re.match(r"(\d+)", x)
            if match:
                ranks.append(int(match.group(1)))
        
        # TODO Phase 0.9 Week 2: ê¹¨ì§„ í† í° ë³´ì • ë¡œì§
        # ex) "385ë²ˆê¹Œì§€" â†’ "38" + "5ë²ˆê¹Œì§€" ë¶„ë¦¬
        # ex) Golden Setê³¼ ë¹„êµí•˜ì—¬ ìë™ ë³´ì •
        
        return people, ranks
    
    # ---------- ë‚´ë¶€ ë‹¨ê³„: í–‰ í˜ì–´ë§ ----------
    
    def _pair_rows(self, people_list: List[int], rank_list: List[int]) -> List[Dict[str, int]]:
        """
        people_list[i] <-> rank_list[i] ë¡œ ë§¤í•‘
        
        ê¸¸ì´ ë¶ˆì¼ì¹˜ ì‹œ ë¡œê¹… + ìµœì†Œ ê¸¸ì´ë¡œ ì²˜ë¦¬
        """
        if len(people_list) != len(rank_list):
            logger.warning(f"âš ï¸ ì—´ ê¸¸ì´ ë¶ˆì¼ì¹˜: people={len(people_list)}, rank={len(rank_list)}")
            # TODO Phase 0.9 Week 2: ë³´ì • ë¡œì§ êµ¬í˜„
            min_len = min(len(people_list), len(rank_list))
        else:
            min_len = len(people_list)
        
        rows = []
        for i in range(min_len):
            rows.append({
                "row_index": i + 1,
                "people": people_list[i],
                "rank_max": rank_list[i],
            })
        
        return rows
    
    # ---------- ë‚´ë¶€ ë‹¨ê³„: ì²­í¬ ìƒì„± ----------
    
    def _build_overview_chunk(
        self, 
        table_id: str, 
        meta: Dict[str, Any], 
        rows: List[Dict[str, Any]]
    ) -> TableOverviewChunk:
        """í‘œ ì „ì²´ ì„¤ëª… ì²­í¬ ìƒì„±"""
        
        table_title = meta.get('table_title', '')
        title = f"[ë³„í‘œ1] ì„ìš© ì¸ì›ìˆ˜ì— ëŒ€í•œ ìŠ¹ì§„í›„ë³´ì ë²”ìœ„ ({table_title})"
        description = "ì´ í‘œëŠ” ì„ìš© ì¸ì›ìˆ˜ì— ë”°ë¼ ìŠ¹ì§„í›„ë³´ìì— í¬í•¨ë˜ëŠ” ì„œì—´ëª…ë¶€ ìˆœìœ„ì˜ ìƒí•œì„ ì •ì˜í•œ ê²ƒì´ë‹¤."
        
        return TableOverviewChunk(
            chunk_type="table_overview",
            table_id=table_id,
            title=title,
            description=description,
            column_names=["ì„ìš© ì¸ì›ìˆ˜", "ì„œì—´ëª…ë¶€ ìˆœìœ„ ìƒí•œ"],
            row_count=len(rows),
            metadata={
                "annex_no": 1,
                "section_type": meta.get("boundary", "annex"),
                "section_id": meta.get("section_id"),
                "table_title": table_title,
                "related_article": "ì œ20ì¡°ì œ2í•­",
            },
        )
    
    def _build_row_chunks(
        self, 
        table_id: str, 
        meta: Dict[str, Any], 
        rows: List[Dict[str, Any]]
    ) -> List[TableRowChunk]:
        """í–‰ ë‹¨ìœ„ ì²­í¬ ìƒì„± - LLMì´ ë‹µì„ ì°¾ëŠ” ìµœì†Œ ë‹¨ìœ„"""
        
        result = []
        table_title = meta.get("table_title", "")
        
        for r in rows:
            people = r["people"]
            rank_max = r["rank_max"]
            idx = r["row_index"]
            
            # ìì—°ì–´ ë¬¸ì¥ ìƒì„±
            content = f"ì„ìš© ì¸ì› {people}ëª…ì¼ ê²½ìš° ìŠ¹ì§„í›„ë³´ì ë²”ìœ„ëŠ” ì„œì—´ {rank_max}ë²ˆê¹Œì§€ì´ë‹¤."
            
            chunk = TableRowChunk(
                chunk_type="table_row",
                table_id=table_id,
                row_index=idx,
                columns={
                    "people": people,
                    "rank_max": rank_max,
                },
                content=content,
                metadata={
                    "annex_no": 1,
                    "table_title": table_title,
                    "column_names": ["ì„ìš© ì¸ì›ìˆ˜", "ì„œì—´ëª…ë¶€ ìƒí•œ"],
                },
            )
            result.append(chunk)
        
        return result
    
    def _build_note_chunks(
        self, 
        table_id: str, 
        text: str, 
        meta: Dict[str, Any]
    ) -> List[TableNoteChunk]:
        """í‘œ ì£¼ì„ ì²­í¬ ìƒì„±"""
        
        notes: List[TableNoteChunk] = []
        
        # ê³„ì‚° ê·œì¹™ ê°ì§€
        # "5ëª…ê¹Œì§€ëŠ” ì„œì—´ëª…ë¶€ìˆœìœ„ì˜ 5ë°°ìˆ˜, 5ëª…ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ëŠ” ì´ˆê³¼ì¸ì›ì˜ 3ë°°ìˆ˜"
        if "3ë°°ìˆ˜" in text or "5ë°°ìˆ˜" in text:
            notes.append(
                TableNoteChunk(
                    chunk_type="table_note",
                    table_id=table_id,
                    note_index=1,
                    content="ì„ìš©í•˜ê³ ìí•˜ëŠ” ì¸ì›ìˆ˜ê°€ 5ëª…ê¹Œì§€ëŠ” ì„œì—´ëª…ë¶€ìˆœìœ„ì˜ 5ë°°ìˆ˜, "
                            "5ëª…ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ëŠ” ì´ˆê³¼ì¸ì›ì˜ 3ë°°ìˆ˜ë¥¼ ì‹¬ì‚¬ëŒ€ìƒì— í¬í•¨.",
                    note_type="calculation_rule",
                    metadata={"annex_no": 1},
                )
            )
        
        # â€» ì£¼ì„ íŒ¨í„´ íƒì§€
        note_pattern = re.compile(r'â€»\s*(.+?)(?:\n|$)')
        for i, match in enumerate(note_pattern.finditer(text), start=len(notes) + 1):
            note_text = match.group(1).strip()
            if note_text and len(note_text) > 10:
                notes.append(
                    TableNoteChunk(
                        chunk_type="table_note",
                        table_id=table_id,
                        note_index=i,
                        content=note_text,
                        note_type="general",
                        metadata={"annex_no": 1},
                    )
                )
        
        return notes


# ==============================
# í—¬í¼ í•¨ìˆ˜
# ==============================

def serialize_table_chunks(parsed: Dict[str, Any]) -> Dict[str, Any]:
    """
    íŒŒì‹± ê²°ê³¼ë¥¼ dictë¡œ ë³€í™˜ (chunks.json ì €ì¥ìš©)
    
    Args:
        parsed: parse_annex_table() ë°˜í™˜ê°’
    
    Returns:
        JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dict
    """
    return {
        "table_overview": asdict(parsed["table_overview"]),
        "table_rows": [asdict(r) for r in parsed["table_rows"]],
        "table_notes": [asdict(n) for n in parsed["table_notes"]],
    }


def validate_with_golden(
    parsed_rows: List[TableRowChunk],
    golden_data: List[Dict[str, int]]
) -> Dict[str, Any]:
    """
    Golden Setê³¼ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê²€ì¦
    
    Args:
        parsed_rows: TableParserê°€ ìƒì„±í•œ row chunks
        golden_data: Golden Set ë°ì´í„° [{people: 1, rank_max: 5}, ...]
    
    Returns:
        ê²€ì¦ ê²°ê³¼ {accuracy: float, mismatches: [...]}
    """
    mismatches = []
    matched = 0
    
    for golden in golden_data:
        people = golden["people"]
        rank_max = golden["rank_max"]
        
        # ë§¤ì¹­ë˜ëŠ” row ì°¾ê¸°
        found = False
        for row in parsed_rows:
            if row.columns["people"] == people:
                found = True
                if row.columns["rank_max"] == rank_max:
                    matched += 1
                else:
                    mismatches.append({
                        "people": people,
                        "expected": rank_max,
                        "actual": row.columns["rank_max"]
                    })
                break
        
        if not found:
            mismatches.append({
                "people": people,
                "expected": rank_max,
                "actual": None,
                "error": "not_found"
            })
    
    total = len(golden_data)
    accuracy = matched / total if total > 0 else 0.0
    
    return {
        "accuracy": accuracy,
        "matched": matched,
        "total": total,
        "mismatches": mismatches
    }


# ==============================
# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸
# ==============================

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_chunk = {
        "content": """1
2
3
4
5
5ë²ˆê¹Œì§€
10ë²ˆê¹Œì§€
15ë²ˆê¹Œì§€
20ë²ˆê¹Œì§€
25ë²ˆê¹Œì§€
â€» ì„ìš©í•˜ê³ ìí•˜ëŠ”ì¸ì›ìˆ˜ê°€ 5ëª…ê¹Œì§€ëŠ” ì„œì—´ëª…ë¶€ìˆœìœ„ì˜ 5ë°°ìˆ˜""",
        "metadata": {
            "type": "annex_table_rows",
            "table_title": "3ê¸‰ìŠ¹ì§„ì œì™¸",
            "section_id": "annex_1_table_1"
        }
    }
    
    # íŒŒì‹± í…ŒìŠ¤íŠ¸
    parser = TableParser()
    result = parser.parse_annex_table(test_chunk)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== íŒŒì‹± ê²°ê³¼ ===")
    print(f"Overview: {result['table_overview'].title}")
    print(f"Rows: {len(result['table_rows'])}ê°œ")
    print(f"Notes: {len(result['table_notes'])}ê°œ")
    
    # ì§ë ¬í™” í…ŒìŠ¤íŠ¸
    serialized = serialize_table_chunks(result)
    print("\n=== JSON ì¶œë ¥ ===")
    import json
    print(json.dumps(serialized, ensure_ascii=False, indent=2))
    
    # ë§ˆí¬ë‹¤ìš´ í‘œ í…ŒìŠ¤íŠ¸
    print("\n=== ë§ˆí¬ë‹¤ìš´ í‘œ ===")
    md_table = parser.to_markdown_table(result)
    print(md_table)