"""
app.py
PRISM Phase 0.3.3 - Enhanced Application

âœ… Phase 0.3.3 ì§€ì›:
1. ë²„ì „ ì²´í¬ ë¡œì§ ì—…ë°ì´íŠ¸ (0.3.3 ì§€ì›)
2. Safe ëª¨ë“ˆ ìë™ ë¡œë“œ
3. Fallback ë¡œì§ ê°•í™”

Author: ìµœë™í˜„ (Frontend Lead)
Date: 2025-11-08
Version: Phase 0.3.3
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import os
import time
import importlib
import json

# âœ… ë¡œê±° ì´ˆê¸°í™” (ìµœìƒë‹¨)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# âš ï¸ ìºì‹œ ë¬´íš¨í™”
importlib.invalidate_caches()

# âœ… core ëª¨ë“ˆ import
try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.pipeline import Phase53Pipeline
    
    logger.info("âœ… ê¸°ë³¸ core ëª¨ë“ˆ import ì„±ê³µ")
    
    # âœ… Safe ëª¨ë“ˆ ì²´í¬
    try:
        from core.typo_normalizer_safe import TypoNormalizer
        from core.post_merge_normalizer_safe import PostMergeNormalizer
        from core.semantic_chunker import SemanticChunker
        
        tn_version = getattr(TypoNormalizer, 'VERSION', 'UNKNOWN')
        
        # âœ… Safe/OCR íŒ¨í„´ ê°œìˆ˜ í™•ì¸
        safe_patterns = getattr(TypoNormalizer, 'SAFE_PATTERNS', {})
        ocr_patterns = getattr(TypoNormalizer, 'OCR_PATTERNS', {})
        tn_dict_size = len(safe_patterns) + len(ocr_patterns)
        tn_block_size = len(getattr(TypoNormalizer, 'BLOCKED_REPLACEMENTS', set()))
        
        pm_version = getattr(PostMergeNormalizer, 'VERSION', 'UNKNOWN')
        sc_version = getattr(SemanticChunker, 'VERSION', 'UNKNOWN')
        
        logger.info(f"ğŸ” TypoNormalizer: {tn_version}")
        logger.info(f"   ğŸ“– Safe: {len(safe_patterns)}ê°œ")
        logger.info(f"   ğŸ“– OCR: {len(ocr_patterns)}ê°œ")
        logger.info(f"   ğŸ“– í•©ê³„: {tn_dict_size}ê°œ")
        logger.info(f"   ğŸš« ê¸ˆì§€: {tn_block_size}ê°œ")
        logger.info(f"ğŸ” PostMergeNormalizer: {pm_version}")
        logger.info(f"ğŸ” SemanticChunker: {sc_version}")
        
        # âœ… ë²„ì „ íŒì • (0.3.3 ìš°ì„ )
        if "0.3.3" in tn_version:
            logger.info("âœ… Phase 0.3.3 í™•ì¸ë¨!")
            phase_version = "Phase 0.3.3"
            safe_mode_enabled = True
        elif "0.3.2" in tn_version:
            logger.info("âœ… Phase 0.3.2 í™•ì¸ë¨")
            phase_version = "Phase 0.3.2"
            safe_mode_enabled = True
        elif "0.3.1" in tn_version:
            logger.info("âœ… Phase 0.3.1 í™•ì¸ë¨")
            phase_version = "Phase 0.3.1"
            safe_mode_enabled = True
        else:
            logger.warning(f"âš ï¸ Phase ë¯¸í™•ì¸: version={tn_version}")
            phase_version = "Unknown"
            safe_mode_enabled = False
            
    except ImportError as ie:
        logger.error(f"âŒ Safe Normalizers import ì‹¤íŒ¨: {ie}")
        st.error(f"âŒ Safe ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ie}")
        st.error("core/ í´ë”ì— typo_normalizer_safe.pyì™€ post_merge_normalizer_safe.pyê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
        
except ImportError as e:
    logger.error(f"âŒ core ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.error("core í´ë”ì˜ ëª¨ë“  íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()


def main():
    # âœ… ì œëª© (ë²„ì „ë³„ í‘œì‹œ)
    if phase_version == "Phase 0.3.3":
        st.title("ğŸ¯ PRISM Phase 0.3.3 - ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œ âœ¨")
        st.success("âœ… Phase 0.3.3 í™œì„±í™” (ë ˆì´ì–´ ë¶„ë¦¬ ì •ê·œí™”, ê³¨ë“  diff ê¸°ë°˜)")
        
        with st.expander("âœ¨ Phase 0.3.3 ê°œì„ ì‚¬í•­", expanded=False):
            st.markdown("""
            **ğŸ¯ Phase 0.3.3 ì£¼ìš” ê°œì„ :**
            1. âœ… **ë ˆì´ì–´ ë¶„ë¦¬ ì„¤ê³„**: Safe/OCR/Domain 3ë‹¨ê³„ ë¶„ë¦¬
            2. âœ… **ê³¨ë“  diff ê¸°ë°˜**: ì‹¤ì œ ì˜¤ë¥˜ë§Œ êµì • (29ê°œ)
            3. âœ… **ì˜ë¯¸ ë³€ê²½ ì œê±°**: ì›ë³¸ ì¶©ì‹¤ë„ ìµœìš°ì„ 
            4. âœ… **ë¦¬í¬íŠ¸-ì½”ë“œ ë™ê¸°í™”**: ë¬¸ì„œì™€ ì½”ë“œ 100% ì¼ì¹˜
            
            **ğŸ”§ ê¸°ìˆ  ìŠ¤í™:**
            - Safe Layer: 7ê°œ (ê³µë°±/ì „ê°ë°˜ê° ì •ê·œí™”)
            - OCR Layer: 29ê°œ (ê³¨ë“  diff ì¶”ì¶œ)
            - Blocked: 3ê°œ (ì˜ë¯¸ ë³€ê²½ ë°©ì§€)
            - ì¡°ë¬¸ í—¤ë”: ìë™ ì •ê·œí™”
            """)
    else:
        st.title("ğŸ¯ PRISM - ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œ")
        st.warning(f"âš ï¸ ë²„ì „: {phase_version}")
    
    # ë²„ì „ ì •ë³´ í‘œì‹œ
    with st.expander("â„¹ï¸ ë²„ì „ ì •ë³´", expanded=False):
        st.write(f"**í˜„ì¬ ë²„ì „**: {phase_version}")
        st.write(f"**Safe Mode**: {'âœ… í™œì„±í™”' if safe_mode_enabled else 'âŒ ë¹„í™œì„±í™”'}")
        st.write(f"**TypoNormalizer**: {tn_version}")
        st.write(f"**PostMergeNormalizer**: {pm_version}")
        st.write(f"**SemanticChunker**: {sc_version}")
        st.write(f"**ì‚¬ì „ í¬ê¸°**: {tn_dict_size}ê°œ")
        st.write(f"**ê¸ˆì§€ ì¹˜í™˜**: {tn_block_size}ê°œ")
    
    # ì´ˆê¸°í™”
    try:
        pdf_processor = PDFProcessor()
        vlm_service = VLMServiceV50(provider="azure_openai")
        logger.info("âœ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ", type=['pdf'])
    
    if uploaded_file is not None:
        # session_stateë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬ ê²°ê³¼ ìºì‹±
        file_key = f"{uploaded_file.name}_{uploaded_file.size}"
        
        if 'last_processed_file' not in st.session_state or st.session_state['last_processed_file'] != file_key:
            # ìƒˆ íŒŒì¼ì´ê±°ë‚˜ ì•„ì§ ì²˜ë¦¬ ì•ˆ í–ˆìœ¼ë©´ ì²˜ë¦¬
            status_text = f"ğŸ”„ PDF ì²˜ë¦¬ ì¤‘... ({phase_version})"
            
            with st.spinner(status_text):
                temp_path = None
                
                try:
                    # ì„ì‹œ íŒŒì¼ ì €ì¥
                    temp_filename = f"temp_{int(time.time())}_{uploaded_file.name}"
                    temp_path = Path(temp_filename)
                    
                    with open(temp_path, 'wb') as f:
                        f.write(uploaded_file.getvalue())
                    
                    logger.info(f"âœ… ì„ì‹œ íŒŒì¼ ì €ì¥: {temp_path}")
                    
                    # Pipeline ì´ˆê¸°í™” ë° ì²˜ë¦¬
                    pipeline = Phase53Pipeline(pdf_processor, vlm_service)
                    result = pipeline.process_pdf(str(temp_path))
                    
                    # ê²°ê³¼ë¥¼ session_stateì— ì €ì¥
                    st.session_state['last_processed_file'] = file_key
                    st.session_state['result'] = result
                    
                    logger.info("âœ… ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                    st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    return
                    
                finally:
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    if temp_path and temp_path.exists():
                        try:
                            temp_path.unlink()
                            logger.info(f"âœ… ì„ì‹œ íŒŒì¼ ì‚­ì œ: {temp_path}")
                        except Exception as e:
                            logger.warning(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        # session_stateì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        result = st.session_state.get('result')
        
        if result and result.get('success'):
            st.success("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
            
            # ì²˜ë¦¬ ì‹œê°„ í‘œì‹œ
            elapsed = result.get('elapsed_time', 0)
            st.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
            
            # ì²´í¬ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
            st.subheader("ğŸ“Š í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸")
            checklist = result.get('checklist', {})
            
            if checklist:
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    fidelity = checklist.get('fidelity', 0)
                    st.metric("ğŸ“„ ì›ë³¸ ì¶©ì‹¤ë„", f"{fidelity}/100")
                    
                    chunking = checklist.get('chunking', 0)
                    st.metric("âœ‚ï¸ ì²­í‚¹ í’ˆì§ˆ", f"{chunking}/100")
                
                with col2:
                    rag = checklist.get('rag_readiness', 0)
                    st.metric("ğŸ¯ RAG ì í•©ë„", f"{rag}/100")
                    
                    generality = checklist.get('generality', 0)
                    st.metric("ğŸ”„ ë²”ìš©ì„±", f"{generality}/100")
                
                with col3:
                    competitive = checklist.get('competitive_edge', 0)
                    st.metric("ğŸ† ê²½ìŸë ¥", f"{competitive}/100")
                    
                    overall = checklist.get('overall', 0)
                    st.metric("ğŸ¯ ì¢…í•©", f"{overall}/100")
                
                # Markdown ë¯¸ë¦¬ë³´ê¸°
                st.subheader("ğŸ“ Markdown ë¯¸ë¦¬ë³´ê¸°")
                markdown = result.get('markdown', '')
                
                if markdown:
                    preview = markdown[:1000]
                    if len(markdown) > 1000:
                        preview += "\n\n... (ìƒëµ) ..."
                    
                    st.text_area("", preview, height=300, disabled=True)
                    
                    with st.expander("ğŸ“„ ì „ì²´ Markdown ë³´ê¸°"):
                        st.markdown(markdown)
                
                # ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
                st.subheader("âœ‚ï¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°")
                chunks = result.get('chunks', [])
                
                if chunks:
                    for i, chunk in enumerate(chunks[:3], 1):
                        with st.expander(f"ì²­í¬ {i}: {chunk.get('id', '')}"):
                            st.write("**ë©”íƒ€ë°ì´í„°:**")
                            st.json(chunk.get('metadata', {}))
                            st.write("**ë‚´ìš©:**")
                            st.text(chunk.get('content', ''))
                    
                    if len(chunks) > 3:
                        st.info(f"ğŸ“‹ ì´ {len(chunks)}ê°œ ì²­í¬")
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                st.subheader("ğŸ“¥ ë‹¤ìš´ë¡œë“œ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if markdown:
                        timestamp = time.strftime("%Y%m%d_%H%M%S")
                        filename = f"{uploaded_file.name.replace('.pdf', '')}_{timestamp}_markdown.md"
                        
                        st.download_button(
                            label="ğŸ“ Markdown ë‹¤ìš´ë¡œë“œ",
                            data=markdown,
                            file_name=filename,
                            mime="text/markdown"
                        )
                
                with col2:
                    if chunks:
                        timestamp = time.strftime("%Y%m%d_%H%M%S")
                        filename = f"{uploaded_file.name.replace('.pdf', '')}_{timestamp}_chunks.json"
                        
                        chunks_json = json.dumps(chunks, ensure_ascii=False, indent=2)
                        
                        st.download_button(
                            label="ğŸ“¦ JSON ë‹¤ìš´ë¡œë“œ",
                            data=chunks_json,
                            file_name=filename,
                            mime="application/json"
                        )
        
        elif result:
            st.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")


if __name__ == "__main__":
    main()