"""
app.py - PRISM Phase 0.4.0 P0-3 ì™„ì „íŒ
GPT í”¼ë“œë°± 100% ë°˜ì˜ + DualQA í†µí•©

âœ… Phase 0.4.0 P0-3 ê°œì„  ì‚¬í•­:
1. QA í—¤ë” ì¶”ì¶œ ì •êµí™” (ì¸ë¼ì¸ ì°¸ì¡° ë…¸ì´ì¦ˆ ì œê±°)
2. DualQAGate ì´ì¤‘ ê²€ì¦ (PDF vs VLM)
3. ê´€ì°° ëª¨ë“œ (í•˜ë“œ fail ê¸ˆì§€)
4. UIì— QA ê²½ê³  í‘œì‹œ

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT ë³´ì •
Date: 2025-11-13
Version: Phase 0.4.0 P0-3
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import time
import json
import gc
import base64
import uuid
import re
from PIL import Image
from datetime import datetime

# ============================================
# ë¡œê¹… ì„¤ì •
# ============================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ============================================
# ëª¨ë“ˆ Import
# ============================================
try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.hybrid_extractor import HybridExtractor
    from core.typo_normalizer_safe import TypoNormalizer
    from core.post_merge_normalizer_safe import PostMergeNormalizer
    from core.semantic_chunker import SemanticChunker
    from core.dual_qa_gate import DualQAGate, extract_pdf_text_layer
    from core.utils_fs import safe_temp_path, safe_remove
    
    logger.info("âœ… ëª¨ë“ˆ import ì„±ê³µ (Phase 0.4.0 P0-3)")
    
except Exception as e:
    logger.error(f"âŒ Import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()


# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def image_to_base64(image_data):
    """ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ base64ë¡œ ë³€í™˜"""
    if isinstance(image_data, tuple):
        image_data = image_data[0]
    
    if isinstance(image_data, Image.Image):
        from io import BytesIO
        buffered = BytesIO()
        image_data.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()
    
    if isinstance(image_data, str):
        return image_data
    
    if isinstance(image_data, bytes):
        return base64.b64encode(image_data).decode()
    
    raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_data)}")


def to_review_md(chunks: list, markdown: str) -> str:
    """
    ë¦¬ë·°ìš© Markdown ìƒì„± (ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í˜•ì‹)
    """
    lines = []
    
    # ê°œì • ì´ë ¥ (ìƒë‹¨ì— ìœ ì§€)
    if 'ì œ37ì°¨' in markdown or 'ê°œì •' in markdown[:200]:
        first_section = markdown.split('\n\n')[0]
        lines.append("# ì¸ì‚¬ê·œì •\n")
        lines.append("## ê°œì • ì´ë ¥")
        lines.append(first_section + "\n")
    
    # ì²­í¬ë³„ ë³€í™˜
    for chunk in chunks:
        content = chunk['content']
        chunk_type = chunk['metadata']['type']
        title = chunk['metadata'].get('title')
        
        # ê¸°ë³¸ì •ì‹ 
        if chunk_type == 'basic':
            lines.append("\n## ê¸°ë³¸ì •ì‹ \n")
            lines.append(content.replace('ê¸°ë³¸ì •ì‹ ', '', 1).strip())
        
        # ì¥
        elif chunk_type == 'chapter':
            chapter_match = re.search(r'ì œ\s*\d+\s*ì¥', content)
            if chapter_match:
                lines.append(f"\n## {chapter_match.group()}\n")
                rest = content[chapter_match.end():].strip()
                if rest:
                    lines.append(rest)
        
        # ì¡°ë¬¸
        elif chunk_type in ['article', 'article_loose']:
            header_match = re.search(r'(ì œ\s*\d+ì¡°(?:ì˜\d+)?)\s*\(([^)]+)\)', content)
            if header_match:
                article_num = header_match.group(1)
                article_title = header_match.group(2)
                lines.append(f"\n### {article_num}({article_title})\n")
                
                # ë³¸ë¬¸ (í•­ëª© ì•ì— ì¤„ë°”ê¿ˆ)
                rest = content[header_match.end():].strip()
                rest = re.sub(r'([ã€‚\.])(\s*)(â‘ )', r'\1\n\3', rest)
                rest = re.sub(r'([ã€‚\.])(\s*)(â‘¡)', r'\1\n\3', rest)
                rest = re.sub(r'([ã€‚\.])(\s*)(â‘¢)', r'\1\n\3', rest)
                rest = re.sub(r'([ã€‚\.])(\s*)(â‘£)', r'\1\n\3', rest)
                
                lines.append(rest)
            else:
                lines.append(f"\n### {content[:30]}...\n")
                lines.append(content)
    
    return '\n'.join(lines)


# ============================================
# ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ============================================

def process_document(pdf_path: str, max_pages: int = 20, provider: str = 'azure_openai'):
    """
    ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    
    âœ… Phase 0.4.0 P0-3: DualQA í†µí•©
    """
    
    # 0. DualQA ì¤€ë¹„: PDF í…ìŠ¤íŠ¸ ë ˆì´ì–´ ì¶”ì¶œ
    st.info("ğŸ“„ PDF ì›ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    pdf_text = extract_pdf_text_layer(pdf_path)
    logger.info(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(pdf_text)}ì")
    
    # 1. PDF ì²˜ë¦¬
    st.info("ğŸ“„ PDF ì´ë¯¸ì§€ ë³€í™˜ ì¤‘...")
    pdf_processor = PDFProcessor()
    images = pdf_processor.pdf_to_images(pdf_path, max_pages=max_pages)
    logger.info(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ì¶”ì¶œ")
    st.success(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")
    
    # 2. VLM ì´ˆê¸°í™”
    vlm_service = VLMServiceV50(provider=provider)
    logger.info("âœ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # 3. Hybrid ì¶”ì¶œ (í˜ì´ì§€ë³„ ì²˜ë¦¬)
    st.info("ğŸ¤– VLM ê¸°ë°˜ ì¶”ì¶œ ì¤‘...")
    extractor = HybridExtractor(
        vlm_service=vlm_service,
        pdf_path=pdf_path
    )
    logger.info("âœ… HybridExtractor ì´ˆê¸°í™”")
    
    # í˜ì´ì§€ë³„ ì¶”ì¶œ
    all_pages = []
    for i, image_item in enumerate(images, 1):
        # ë””ë²„ê¹…: íƒ€ì… í™•ì¸
        logger.info(f"   ğŸ” Page {i} image type: {type(image_item)}")
        
        # ì—¬ëŸ¬ ì¼€ì´ìŠ¤ ì²˜ë¦¬
        if isinstance(image_item, tuple):
            # Case 1: (image_data, metadata) íŠœí”Œ
            image_data = image_item[0]
            logger.info(f"   ğŸ“¦ íŠœí”Œì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (ìš”ì†Œ íƒ€ì…: {type(image_data)})")
        elif isinstance(image_item, dict):
            # Case 2: {'image': ..., 'metadata': ...} ë”•ì…”ë„ˆë¦¬
            image_data = image_item.get('image', image_item)
            logger.info(f"   ğŸ“¦ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ")
        else:
            # Case 3: ì§ì ‘ ì´ë¯¸ì§€ ë°ì´í„°
            image_data = image_item
            logger.info(f"   ğŸ“¦ ì§ì ‘ ì´ë¯¸ì§€ ì‚¬ìš©")
        
        # ìµœì¢… í™•ì¸: ì—¬ì „íˆ íŠœí”Œì´ë©´ ì¬ê·€ì ìœ¼ë¡œ ì¶”ì¶œ
        while isinstance(image_data, tuple):
            logger.warning(f"   âš ï¸ ì¤‘ì²© íŠœí”Œ ê°ì§€! ì¬ê·€ ì¶”ì¶œ")
            image_data = image_data[0]
        
        logger.info(f"   âœ… ìµœì¢… image_data íƒ€ì…: {type(image_data)}")
        
        page_result = extractor.extract(image_data, page_num=i)
        all_pages.append(page_result)
        
        st.info(f"   âœ… í˜ì´ì§€ {i}: {len(page_result['content'])}ì")
        logger.info(f"   âœ… í˜ì´ì§€ {i}: {len(page_result['content'])}ì")
    
    # Markdown ë³‘í•©
    markdown = '\n\n'.join([p['content'] for p in all_pages])
    logger.info(f"âœ… Markdown ë³‘í•© ì™„ë£Œ: {len(markdown)}ì")
    st.success(f"âœ… VLM ì¶”ì¶œ ì™„ë£Œ: {len(markdown)}ì")
    
    # 4. ì˜¤íƒˆì ì •ê·œí™”
    st.info("ğŸ”§ ì˜¤íƒˆì ì •ê·œí™” ì¤‘...")
    normalizer = TypoNormalizer()
    normalized_md = normalizer.normalize(markdown)
    logger.info(f"âœ… ì •ê·œí™” ì™„ë£Œ: {len(normalized_md)}ì")
    st.success(f"âœ… ì˜¤íƒˆì êµì • ì™„ë£Œ")
    
    # 5. í›„ì²˜ë¦¬ ì •ê·œí™”
    post_normalizer = PostMergeNormalizer()
    final_md = post_normalizer.normalize(normalized_md)
    logger.info(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ: {len(final_md)}ì")
    
    # 6. ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹
    st.info("âœ‚ï¸ ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì¤‘...")
    chunker = SemanticChunker()
    chunks = chunker.chunk(final_md)
    logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
    st.success(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
    
    # âœ… 7. DualQA ê²€ì¦ (Phase 0.4.0 P0-3 ì‹ ê·œ)
    st.info("ğŸ”¬ DualQA ì´ì¤‘ ê²€ì¦ ì¤‘...")
    dual_qa = DualQAGate()
    qa_result = dual_qa.validate(pdf_text, final_md)
    logger.info("âœ… DualQA ê²€ì¦ ì™„ë£Œ")
    
    # QA ê²°ê³¼ UI í‘œì‹œ
    if qa_result['qa_flags']:
        st.warning(f"âš ï¸ QA ê²½ê³ : {', '.join(qa_result['qa_flags'])}")
        
        with st.expander("ğŸ”¬ DualQA ìƒì„¸ ê²°ê³¼"):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("PDF ì¡°ë¬¸", qa_result['pdf_count'])
            with col2:
                st.metric("VLM ì¡°ë¬¸", qa_result['vlm_count'])
            with col3:
                st.metric("ë§¤ì¹­ë¥ ", f"{qa_result['match_rate']:.1%}")
            
            if qa_result['missing_in_vlm']:
                st.error(f"âŒ VLM ëˆ„ë½: {qa_result['missing_in_vlm']}")
            
            if qa_result['extra_in_vlm']:
                st.warning(f"âš ï¸ VLM ì¶”ê°€: {qa_result['extra_in_vlm']}")
    else:
        st.success("âœ… DualQA ê²€ì¦ í†µê³¼ (ì›ë³¸ê³¼ ì¼ì¹˜)")
    
    return {
        'markdown': final_md,
        'chunks': chunks,
        'qa_result': qa_result,
        'metadata': {
            'total_pages': len(images),
            'total_chars': len(final_md),
            'total_chunks': len(chunks),
            'processing_time': datetime.now().isoformat(),
            'qa_flags': qa_result['qa_flags']
        }
    }


# ============================================
# Streamlit UI
# ============================================

def main():
    """ë©”ì¸ UI"""
    
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="PRISM Phase 0.4.0 P0-3",
        page_icon="ğŸ”·",
        layout="wide"
    )
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'last_result' not in st.session_state:
        st.session_state.last_result = None
    if 'last_filename' not in st.session_state:
        st.session_state.last_filename = None
    
    # í—¤ë”
    st.title("ğŸ”· PRISM Phase 0.4.0 P0-3")
    st.caption("ì°¨ì„¸ëŒ€ ì§€ëŠ¥í˜• ë¬¸ì„œ ì´í•´ í”Œë«í¼ - DualQA ì™„ì „íŒ")
    
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        st.subheader("ğŸ“Š ë²„ì „ ì •ë³´")
        st.info("""
**Phase 0.4.0 P0-3 (QA-Stable)**

âœ… **P0-3a: QA í—¤ë” ì •êµí™”**
- ì¸ë¼ì¸ ì°¸ì¡° ë…¸ì´ì¦ˆ ì œê±°
- ì²­í‚¹ ê²½ê³„ íŒ¨í„´ í†µí•©

âœ… **P0-3b: DualQA ì´ì¤‘ ê²€ì¦**
- PDF ì›ë³¸ vs VLM ê²°ê³¼
- ê´€ì°° ëª¨ë“œ (í•˜ë“œ fail ê¸ˆì§€)
- ì›ë¬¸ ë¶ˆì¼ì¹˜ ìë™ ê°ì§€

**GPT í”¼ë“œë°± 100% ë°˜ì˜**
**ë§ˆì°½ìˆ˜ì‚°íŒ€ ì£¼ë„ ì„¤ê³„**
        """)
        
        st.markdown("---")
        
        st.subheader("ğŸ”§ VLM ì„¤ì •")
        provider = st.selectbox(
            "VLM Provider",
            ["azure_openai"],
            index=0
        )
        
        max_pages = st.slider(
            "ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€",
            min_value=1,
            max_value=20,
            value=20
        )
        
        st.markdown("---")
        
        st.subheader("ğŸ“– ì‚¬ìš© ë°©ë²•")
        st.markdown("""
1. PDF íŒŒì¼ ì—…ë¡œë“œ
2. 'ì²˜ë¦¬ ì‹œì‘' ë²„íŠ¼ í´ë¦­
3. DualQA ê²€ì¦ ê²°ê³¼ í™•ì¸
4. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
        """)
    
    # ë©”ì¸ ì˜ì—­
    st.header("ğŸ“„ PDF ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type=['pdf'],
        help="ìµœëŒ€ 10MB, 20í˜ì´ì§€ê¹Œì§€ ì§€ì›"
    )
    
    if uploaded_file is not None:
        # íŒŒì¼ ì •ë³´
        file_size = len(uploaded_file.getvalue()) / (1024 * 1024)
        st.info(f"ğŸ“ íŒŒì¼ëª…: {uploaded_file.name} ({file_size:.2f} MB)")
        
        if file_size > 10:
            st.error("âŒ íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤!")
            return
        
        # ì²˜ë¦¬ ë²„íŠ¼
        if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary"):
            # ìºì‹œ ë¬´íš¨í™”
            file_id = f"{uploaded_file.name}_{uuid.uuid4().hex[:8]}"
            
            # ì„ì‹œ íŒŒì¼ ì €ì¥
            pdf_path = safe_temp_path(".pdf")
            with open(pdf_path, 'wb') as f:
                f.write(uploaded_file.getvalue())
            
            logger.info(f"âœ… ì„ì‹œ íŒŒì¼ ì €ì¥: {pdf_path}")
            
            try:
                # ì§„í–‰ í‘œì‹œ
                with st.spinner("â³ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... (ìµœëŒ€ 2ë¶„ ì†Œìš”)"):
                    start_time = time.time()
                    
                    # ì²˜ë¦¬ ì‹¤í–‰
                    result = process_document(
                        pdf_path=pdf_path,
                        max_pages=max_pages,
                        provider=provider
                    )
                    
                    elapsed = time.time() - start_time
                    logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {elapsed:.1f}ì´ˆ")
                
                # ì„¸ì…˜ ìƒíƒœ ì €ì¥
                st.session_state.last_result = result
                st.session_state.last_filename = uploaded_file.name
                
                st.success(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! ({elapsed:.1f}ì´ˆ)")
                
            except Exception as e:
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                st.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                safe_remove(pdf_path)
                gc.collect()
    
    # ê²°ê³¼ í‘œì‹œ
    if st.session_state.last_result is not None:
        st.markdown("---")
        st.header("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
        
        result = st.session_state.last_result
        filename = st.session_state.last_filename
        
        # ë©”íƒ€ë°ì´í„°
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ì „ì²´ ë¬¸ì", f"{result['metadata']['total_chars']:,}")
        with col2:
            st.metric("ì²­í¬ ê°œìˆ˜", result['metadata']['total_chunks'])
        with col3:
            qa_status = "âš ï¸ ê²½ê³ " if result['metadata']['qa_flags'] else "âœ… í†µê³¼"
            st.metric("QA ìƒíƒœ", qa_status)
        
        # íƒ­
        tab1, tab2, tab3, tab4 = st.tabs([
            "ğŸ“ Markdown",
            "ğŸ“¦ JSON",
            "ğŸ“– Review",
            "ğŸ”¬ QA ìƒì„¸"
        ])
        
        with tab1:
            st.text_area(
                "Markdown ê²°ê³¼",
                result['markdown'],
                height=400
            )
            
            # ë‹¤ìš´ë¡œë“œ
            filename_base = Path(filename).stem
            st.download_button(
                "â¬‡ï¸ Markdown ë‹¤ìš´ë¡œë“œ",
                result['markdown'],
                file_name=f"{filename_base}_{uuid.uuid4().hex[:8]}.md",
                mime="text/markdown"
            )
        
        with tab2:
            json_str = json.dumps(result['chunks'], ensure_ascii=False, indent=2)
            st.text_area(
                "JSON ê²°ê³¼",
                json_str,
                height=400
            )
            
            st.download_button(
                "â¬‡ï¸ JSON ë‹¤ìš´ë¡œë“œ",
                json_str,
                file_name=f"{filename_base}_{uuid.uuid4().hex[:8]}.json",
                mime="application/json"
            )
        
        with tab3:
            review_md = to_review_md(result['chunks'], result['markdown'])
            st.text_area(
                "ë¦¬ë·°ìš© Markdown",
                review_md,
                height=400
            )
            
            st.download_button(
                "â¬‡ï¸ ë¦¬ë·°ìš© ë‹¤ìš´ë¡œë“œ",
                review_md,
                file_name=f"{filename_base}_review_{uuid.uuid4().hex[:8]}.md",
                mime="text/markdown"
            )
        
        with tab4:
            qa_result = result['qa_result']
            
            st.subheader("ğŸ”¬ DualQA ê²€ì¦ ìƒì„¸")
            
            # ë©”íŠ¸ë¦­
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("PDF ì¡°ë¬¸", qa_result['pdf_count'])
            with col2:
                st.metric("VLM ì¡°ë¬¸", qa_result['vlm_count'])
            with col3:
                st.metric("ë§¤ì¹­ë¥ ", f"{qa_result['match_rate']:.1%}")
            
            # ë¶ˆì¼ì¹˜ ìƒì„¸
            if qa_result['missing_in_vlm']:
                st.error("âŒ VLM ëˆ„ë½ (PDFì—ëŠ” ìˆì§€ë§Œ VLMì´ ëª» ì°¾ìŒ)")
                st.json(qa_result['missing_in_vlm'])
            
            if qa_result['extra_in_vlm']:
                st.warning("âš ï¸ VLM ì¶”ê°€ (VLMì´ ë§Œë“¤ì–´ë‚¸ ì¡°ë¬¸)")
                st.json(qa_result['extra_in_vlm'])
            
            if not qa_result['qa_flags']:
                st.success("âœ… ì›ë³¸ê³¼ ì™„ì „ ì¼ì¹˜!")
            else:
                st.warning(f"âš ï¸ QA í”Œë˜ê·¸: {qa_result['qa_flags']}")
                st.info("â†’ ìˆ˜ë™ ê²€ìˆ˜ ê¶Œì¥")


if __name__ == '__main__':
    main()