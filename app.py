"""
PRISM POC - í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ì§€ì›
VLM ì´ë¯¸ì§€ ë¶„ì„ + OCR ì›ë¬¸ ì²­í‚¹
"""

import streamlit as st
import os
import json
import base64
import logging
from datetime import datetime
from typing import Dict, List, Any
from io import BytesIO

# ========== í™˜ê²½ë³€ìˆ˜ ê°•ì œ ë¡œë“œ (ìµœìš°ì„ ) ==========
from pathlib import Path
from dotenv import load_dotenv

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ .env ë¡œë“œ
current_dir = Path(__file__).parent
env_path = current_dir / '.env'

if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=True)
    print(f"âœ… .env íŒŒì¼ ë¡œë“œ: {env_path}")
else:
    print(f"âš ï¸ .env íŒŒì¼ ì—†ìŒ: {env_path}")

# API í‚¤ í™•ì¸
api_key = os.getenv('ANTHROPIC_API_KEY')
if api_key:
    print(f"âœ… Claude API í‚¤ ë¡œë“œ ì„±ê³µ: {api_key[:20]}...")
else:
    print("âŒ Claude API í‚¤ ë¡œë“œ ì‹¤íŒ¨!")
# ==================================================

# Core ëª¨ë“ˆ
from core.pdf_processor import PDFProcessor
from core.multi_vlm_service import MultiVLMService

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ========== Streamlit ì„¤ì • ==========
st.set_page_config(
    page_title="PRISM - Document Intelligence Platform",
    page_icon="ğŸ”·",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ========== CSS (ì „ì²´ ë„ˆë¹„ ì‚¬ìš©) ==========
st.markdown("""
<style>
    /* ì „ì²´ ì»¨í…Œì´ë„ˆ ë„ˆë¹„ */
    .main .block-container {
        max-width: 100%;
        padding-left: 2rem;
        padding-right: 2rem;
    }
    
    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
    .sidebar-title {
        font-size: 1.8rem;
        font-weight: bold;
        color: #1E88E5;
        margin-bottom: 0.5rem;
    }
    
    .sidebar-subtitle {
        font-size: 0.9rem;
        color: #666;
        margin-bottom: 1.5rem;
    }
    
    /* í”„ë¡œë°”ì´ë” ì¹´ë“œ */
    .provider-card {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 0.5rem;
        border: 2px solid #e0e0e0;
        background-color: #f9f9f9;
    }
    
    .provider-available {
        border-color: #4CAF50;
        background-color: #E8F5E9;
    }
    
    /* ì²­í¬ ì¹´ë“œ */
    .chunk-card {
        border: 1px solid #ddd;
        border-radius: 8px;
        padding: 1rem;
        margin-bottom: 1rem;
        background: white;
    }
    
    .chunk-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 0.5rem;
    }
    
    .chunk-type-image {
        background: #E3F2FD;
        color: #1976D2;
        padding: 4px 12px;
        border-radius: 12px;
        font-size: 0.85rem;
        font-weight: bold;
    }
    
    .chunk-type-text {
        background: #F3E5F5;
        color: #7B1FA2;
        padding: 4px 12px;
        border-radius: 12px;
        font-size: 0.85rem;
        font-weight: bold;
    }
    
    /* ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì˜ì—­ */
    .download-section {
        margin-top: 2rem;
        padding: 1.5rem;
        background: #f8f9fa;
        border-radius: 8px;
        border: 1px solid #dee2e6;
    }
</style>
""", unsafe_allow_html=True)

# ========== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ==========

def text_chunking(text: str, min_length: int = 100, max_length: int = 500, overlap: int = 50) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        min_length: ìµœì†Œ ì²­í¬ ê¸¸ì´
        max_length: ìµœëŒ€ ì²­í¬ ê¸¸ì´
        overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© ê¸¸ì´
    
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    if not text or len(text.strip()) == 0:
        return []
    
    # ë‹¨ë½ ë¶„ë¦¬
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ max_lengthë¥¼ ì´ˆê³¼í•˜ë©´
        if len(current_chunk) + len(para) > max_length:
            if len(current_chunk) >= min_length:
                chunks.append(current_chunk.strip())
                # ì˜¤ë²„ë© ì ìš©
                current_chunk = current_chunk[-overlap:] + " " + para
            else:
                current_chunk += " " + para
        else:
            current_chunk += " " + para if current_chunk else para
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk.strip() and len(current_chunk.strip()) >= min_length:
        chunks.append(current_chunk.strip())
    
    return chunks

def convert_to_json(chunks: List[Dict]) -> Dict:
    """JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    data = {
        'metadata': {
            'processed_at': datetime.now().isoformat(),
            'total_chunks': len(chunks),
            'chunk_types': {
                'image': len([c for c in chunks if c['type'] == 'image']),
                'text': len([c for c in chunks if c['type'] == 'text'])
            }
        },
        'chunks': chunks
    }
    return data

def convert_to_markdown(chunks: List[Dict]) -> str:
    """Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    md_lines = []
    md_lines.append("# PRISM ë¬¸ì„œ ì¶”ì¶œ ê²°ê³¼\n")
    md_lines.append(f"**ì²˜ë¦¬ ì¼ì‹œ:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # í†µê³„
    md_lines.append("## ğŸ“Š í†µê³„\n")
    md_lines.append(f"- ì´ ì²­í¬: {len(chunks)}")
    md_lines.append(f"- ì´ë¯¸ì§€ ì²­í¬: {len([c for c in chunks if c['type'] == 'image'])}")
    md_lines.append(f"- í…ìŠ¤íŠ¸ ì²­í¬: {len([c for c in chunks if c['type'] == 'text'])}\n")
    
    # ì²­í¬ë³„ ë‚´ìš©
    current_page = None
    
    for chunk in chunks:
        page_num = chunk.get('page_num', 0)
        
        # í˜ì´ì§€ í—¤ë”
        if page_num != current_page:
            current_page = page_num
            md_lines.append(f"\n## ğŸ“„ í˜ì´ì§€ {page_num}\n")
        
        chunk_id = chunk.get('chunk_id', '')
        chunk_type = chunk.get('type', 'unknown')
        content = chunk.get('content', '')
        
        # ì²­í¬ í—¤ë”
        if chunk_type == 'image':
            md_lines.append(f"### ğŸ–¼ï¸ {chunk_id} (VLM ë¶„ì„)\n")
        else:
            md_lines.append(f"### ğŸ“ {chunk_id} (ì›ë¬¸)\n")
        
        md_lines.append(content)
        md_lines.append("\n---\n")
    
    return "\n".join(md_lines)

# ========== ì„¸ì…˜ ì´ˆê¸°í™” ==========

if 'vlm_service' not in st.session_state:
    try:
        default_provider = os.getenv('DEFAULT_VLM_PROVIDER', 'claude')
        st.session_state.vlm_service = MultiVLMService(default_provider=default_provider)
    except Exception as e:
        st.session_state.vlm_service = None

if 'pdf_processor' not in st.session_state:
    try:
        st.session_state.pdf_processor = PDFProcessor()
    except Exception as e:
        st.session_state.pdf_processor = None

if 'processing_results' not in st.session_state:
    st.session_state.processing_results = None

# ========== ë©”ì¸ UI ==========

def main():
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown('<div class="sidebar-title">PRISM</div>', unsafe_allow_html=True)
        st.markdown('<div class="sidebar-subtitle">Document Intelligence Platform</div>', unsafe_allow_html=True)
        
        st.markdown("### ğŸ¤– ëª¨ë¸ ì„¤ì •")
        
        if st.session_state.vlm_service is not None:
            vlm_service = st.session_state.vlm_service
            
            current_provider = getattr(vlm_service, 'current_provider_key', None)
            if current_provider is None:
                current_provider = os.getenv('DEFAULT_VLM_PROVIDER', 'claude')
            
            providers_status = vlm_service.get_available_providers()
            
            provider_display_names = {
                'claude': 'ğŸŸ£ Claude Sonnet 4',
                'azure_openai': 'ğŸ”µ Azure OpenAI GPT-4',
                'ollama': 'ğŸŸ¢ Ollama'
            }
            
            provider_options = []
            provider_mapping = {}
            
            for key in ['claude', 'azure_openai', 'ollama']:
                if key in providers_status:
                    info = providers_status[key]
                    is_available = info.get('available', False)
                    
                    display_name = provider_display_names.get(key, key)
                    if not is_available:
                        display_name += " (ì„¤ì • í•„ìš”)"
                    
                    provider_options.append(display_name)
                    provider_mapping[display_name] = {
                        'key': key,
                        'available': is_available
                    }
            
            selected_display = st.selectbox(
                "VLM í”„ë¡œë°”ì´ë”",
                options=provider_options,
                index=[i for i, opt in enumerate(provider_options) 
                      if provider_mapping[opt]['key'] == current_provider][0] if provider_options else 0
            )
            
            if selected_display and selected_display in provider_mapping:
                selected_key = provider_mapping[selected_display]['key']
                is_available = provider_mapping[selected_display]['available']
                
                if is_available and selected_key != current_provider:
                    vlm_service.set_provider(selected_key)
                    st.rerun()
                
                if is_available:
                    st.success(f"âœ… {selected_display.split('(')[0].strip()} ì‚¬ìš© ê°€ëŠ¥")
                else:
                    st.error(f"âš ï¸ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤")
            
            st.divider()
            
            # ì²˜ë¦¬ ì„¤ì •
            st.markdown("### âš™ï¸ ì²˜ë¦¬ ì„¤ì •")
            
            max_pages = st.slider(
                "ìµœëŒ€ í˜ì´ì§€ ìˆ˜",
                min_value=1,
                max_value=20,
                value=3,
                help="ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜"
            )
            
            use_text_chunking = st.checkbox(
                "ì›ë¬¸ ì²­í‚¹ í™œì„±í™”",
                value=True,
                help="OCR í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• "
            )
            
            if use_text_chunking:
                st.caption("ğŸ“ ì›ë¬¸ì„ 100-500ì ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ RAGì— ìµœì í™”")
    
    # ë©”ì¸ ì˜ì—­
    st.title("ğŸ“„ PRISM")
    st.caption("ì§€ëŠ¥í˜• ë¬¸ì„œ ì´í•´ í”Œë«í¼")
    
    # ì´ì „ ì²˜ë¦¬ ê²°ê³¼ í‘œì‹œ
    if st.session_state.processing_results is not None:
        st.success("âœ… ì´ì „ ì²˜ë¦¬ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. ìƒˆ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ê¸°ì¡´ ê²°ê³¼ê°€ ëŒ€ì²´ë©ë‹ˆë‹¤.")
        display_results(st.session_state.processing_results)
        st.markdown("---")
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['pdf'],
        help="ìµœëŒ€ 200MB, ìµœëŒ€ 20í˜ì´ì§€"
    )
    
    if uploaded_file:
        file_size_mb = uploaded_file.size / (1024 * 1024)
        
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            st.info(f"ğŸ“„ **{uploaded_file.name}** ({file_size_mb:.1f} MB)")
        with col2:
            use_ocr = st.checkbox("OCR ì‚¬ìš©", value=True)
        with col3:
            vlm_service = st.session_state.vlm_service
            if vlm_service:
                current_provider_key = getattr(vlm_service, 'current_provider_key', None)
                
                # í˜„ì¬ ì„ íƒëœ í”„ë¡œë°”ì´ë”ê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
                if current_provider_key:
                    providers_status = vlm_service.get_available_providers()
                    is_available = providers_status.get(current_provider_key, {}).get('available', False)
                    
                    if is_available:
                        if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary", use_container_width=True):
                            process_file(uploaded_file, max_pages, use_ocr, use_text_chunking)
                    else:
                        st.button("âš ï¸ ëª¨ë¸ ì„¤ì • í•„ìš”", disabled=True, use_container_width=True)
                        st.caption("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„¤ì •í•˜ì„¸ìš”")
                else:
                    st.button("âš ï¸ í”„ë¡œë°”ì´ë” ì„ íƒ í•„ìš”", disabled=True, use_container_width=True)
            else:
                st.button("âš ï¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨", disabled=True, use_container_width=True)

def process_file(uploaded_file, max_pages, use_ocr, use_text_chunking):
    """íŒŒì¼ ì²˜ë¦¬"""
    pdf_processor = st.session_state.pdf_processor
    vlm_service = st.session_state.vlm_service
    
    if vlm_service is None:
        st.error("VLM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    progress_bar = st.progress(0)
    status = st.empty()
    
    try:
        # PDF ì²˜ë¦¬
        status.text("ğŸ“„ PDF ì²˜ë¦¬ ì¤‘...")
        file_bytes = uploaded_file.read()
        
        elements = pdf_processor.process_pdf(pdf_data=file_bytes)
        
        logger.info(f"ì¶”ì¶œëœ Elements ìˆ˜: {len(elements)}")
        
        if not elements:
            st.warning("ì¶”ì¶œëœ Elementê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìµœëŒ€ í˜ì´ì§€ ì œí•œ
        elements = [e for e in elements if e['page_num'] <= max_pages]
        
        if not elements:
            st.warning(f"í˜ì´ì§€ {max_pages} ì´í•˜ì— Elementê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_elements = len(elements)
        st.info(f"ì´ {total_elements}ê°œ Element ì²˜ë¦¬ ì˜ˆì • (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ì²˜ë¦¬
        all_chunks = []
        chunk_counter = 1
        
        for idx, element in enumerate(elements):
            progress = (idx + 1) / total_elements
            progress_bar.progress(progress)
            status.text(f"ì²˜ë¦¬ ì¤‘... ({idx + 1}/{total_elements})")
            
            page_num = element['page_num']
            ocr_text = element.get('ocr_text', '')
            image_base64 = element.get('image_base64', '')
            
            # 1. IMAGE ì²­í¬ (VLM ë¶„ì„)
            status.text(f"ğŸ¤– í˜ì´ì§€ {page_num} VLM ë¶„ì„ ì¤‘...")
            
            try:
                if image_base64:
                    vlm_response = vlm_service.analyze_image(
                        image_base64=image_base64,
                        prompt=f"ì´ ë¬¸ì„œ í˜ì´ì§€ì˜ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. OCR í…ìŠ¤íŠ¸: {ocr_text[:200] if ocr_text else 'ì—†ìŒ'}"
                    )
                    
                    image_chunk = {
                        'chunk_id': f"chunk_{chunk_counter:03d}",
                        'type': 'image',
                        'page_num': page_num,
                        'content': vlm_response,
                        'provider': vlm_service.current_provider_key,
                        'ocr_text_preview': ocr_text[:100] if ocr_text else None
                    }
                    all_chunks.append(image_chunk)
                    chunk_counter += 1
            
            except Exception as e:
                logger.error(f"VLM ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                st.warning(f"âš ï¸ í˜ì´ì§€ {page_num} VLM ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # 2. TEXT ì²­í¬ (OCR ì›ë¬¸ ì²­í‚¹)
            if use_text_chunking and use_ocr and ocr_text:
                status.text(f"ğŸ“ í˜ì´ì§€ {page_num} ì›ë¬¸ ì²­í‚¹ ì¤‘...")
                
                text_chunks = text_chunking(ocr_text)
                
                for sub_idx, text_chunk in enumerate(text_chunks, 1):
                    text_chunk_obj = {
                        'chunk_id': f"chunk_{chunk_counter:03d}",
                        'type': 'text',
                        'page_num': page_num,
                        'sub_index': sub_idx,
                        'content': text_chunk,
                        'length': len(text_chunk)
                    }
                    all_chunks.append(text_chunk_obj)
                    chunk_counter += 1
        
        # ì²˜ë¦¬ ì™„ë£Œ
        progress_bar.progress(1.0)
        status.text("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        
        st.session_state.processing_results = all_chunks
        
        st.success(f"âœ¨ ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ!")
        
        # ê²°ê³¼ í‘œì‹œ
        display_results(all_chunks)
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
        st.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

def display_results(chunks: List[Dict]):
    """ì²˜ë¦¬ ê²°ê³¼ í‘œì‹œ"""
    
    st.markdown("---")
    st.markdown("## ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
    
    # í†µê³„
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ì²­í¬", len(chunks))
    
    with col2:
        image_chunks = len([c for c in chunks if c['type'] == 'image'])
        st.metric("ì´ë¯¸ì§€ ì²­í¬", image_chunks)
    
    with col3:
        text_chunks = len([c for c in chunks if c['type'] == 'text'])
        st.metric("í…ìŠ¤íŠ¸ ì²­í¬", text_chunks)
    
    with col4:
        pages = len(set(c['page_num'] for c in chunks))
        st.metric("í˜ì´ì§€ ìˆ˜", pages)
    
    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    st.markdown('<div class="download-section">', unsafe_allow_html=True)
    st.markdown("### ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        json_data = convert_to_json(chunks)
        json_str = json.dumps(json_data, ensure_ascii=False, indent=2)
        
        st.download_button(
            label="ğŸ“¥ JSON ë‹¤ìš´ë¡œë“œ",
            data=json_str,
            file_name=f"prism_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json",
            use_container_width=True
        )
    
    with col2:
        md_str = convert_to_markdown(chunks)
        
        st.download_button(
            label="ğŸ“¥ Markdown ë‹¤ìš´ë¡œë“œ",
            data=md_str,
            file_name=f"prism_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown",
            use_container_width=True
        )
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ì²­í¬ ìƒì„¸ í‘œì‹œ
    st.markdown("---")
    st.markdown("### ğŸ“‹ ì²­í¬ ëª©ë¡")
    
    # í•„í„°
    col1, col2 = st.columns([1, 3])
    
    with col1:
        filter_type = st.selectbox(
            "ì²­í¬ íƒ€ì…",
            options=['ì „ì²´', 'ì´ë¯¸ì§€', 'í…ìŠ¤íŠ¸'],
            index=0
        )
    
    # í•„í„°ë§
    filtered_chunks = chunks
    if filter_type == 'ì´ë¯¸ì§€':
        filtered_chunks = [c for c in chunks if c['type'] == 'image']
    elif filter_type == 'í…ìŠ¤íŠ¸':
        filtered_chunks = [c for c in chunks if c['type'] == 'text']
    
    st.info(f"ğŸ“‹ {len(filtered_chunks)}ê°œ ì²­í¬ í‘œì‹œ ì¤‘")
    
    # ì²­í¬ ì¹´ë“œ í‘œì‹œ
    for chunk in filtered_chunks:
        chunk_id = chunk['chunk_id']
        chunk_type = chunk['type']
        page_num = chunk['page_num']
        content = chunk['content']
        
        # ì²­í¬ ì¹´ë“œ
        with st.expander(
            f"{'ğŸ–¼ï¸' if chunk_type == 'image' else 'ğŸ“'} {chunk_id} | í˜ì´ì§€ {page_num} | {chunk_type.upper()}",
            expanded=False
        ):
            # ë©”íƒ€ë°ì´í„°
            st.markdown(f"**í˜ì´ì§€:** {page_num}")
            st.markdown(f"**íƒ€ì…:** {chunk_type}")
            
            if chunk_type == 'image':
                provider = chunk.get('provider', 'unknown')
                st.markdown(f"**í”„ë¡œë°”ì´ë”:** {provider}")
            elif chunk_type == 'text':
                sub_index = chunk.get('sub_index', 1)
                length = chunk.get('length', 0)
                st.markdown(f"**ì„œë¸Œ ì¸ë±ìŠ¤:** {sub_index}")
                st.markdown(f"**ê¸¸ì´:** {length}ì")
            
            st.markdown("---")
            
            # ë‚´ìš©
            st.markdown("**ë‚´ìš©:**")
            st.text_area(
                label="",
                value=content,
                height=200,
                key=f"content_{chunk_id}",
                label_visibility="collapsed"
            )

# ì‹¤í–‰
if __name__ == "__main__":
    main()