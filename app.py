"""
app.py - PRISM Phase 0.3.4 P2.4 ìµœì¢… ì™„ì„± ë²„ì „
GPT í”¼ë“œë°± 4ê°€ì§€ ê°œì„ ì‚¬í•­ 100% ë°˜ì˜

âœ… ê°œì„  ì‚¬í•­:
1. ê²½ê³„ ì •ë°€ë„ ê°•í™” (SemanticChunker)
2. íŒŒí¸ ìë™ ë³‘í•© (200ì ë¯¸ë§Œ ì²­í¬)
3. OCR ì˜¤íƒˆì êµì • í™•ëŒ€ (13ê°€ì§€ ë„ë©”ì¸ íŒ¨í„´)
4. ì•ˆì „ íŒŒì¼ ì‚­ì œ (utils_fs.py í†µí•©)

Author: ìµœë™í˜„ (Frontend Lead) + ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-12
Version: Phase 0.3.4 P2.4
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import time
import json
import gc
import base64
from PIL import Image
from datetime import datetime

# ============================================
# ë¡œê¹… ì„¤ì •
# ============================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ============================================
# ëª¨ë“ˆ Import
# ============================================
try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.hybrid_extractor import HybridExtractor
    from core.typo_normalizer_safe import TypoNormalizer
    from core.post_merge_normalizer_safe import PostMergeNormalizer
    from core.semantic_chunker import SemanticChunker
    from core.utils_fs import safe_temp_path, safe_remove  # âœ… ì‹ ê·œ ì¶”ê°€
    
    logger.info("âœ… ëª¨ë“ˆ import ì„±ê³µ (Phase 0.3.4 P2.4)")
    
except Exception as e:
    logger.error(f"âŒ Import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()


# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def image_to_base64(image_data):
    """ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ base64ë¡œ ë³€í™˜"""
    if isinstance(image_data, tuple):
        image_data = image_data[0]
    
    if isinstance(image_data, Image.Image):
        from io import BytesIO
        buffered = BytesIO()
        image_data.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()
    
    if isinstance(image_data, str):
        return image_data
    
    if isinstance(image_data, bytes):
        return base64.b64encode(image_data).decode()
    
    raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_data)}")


def process_pdf_direct(pdf_path, pdf_processor, vlm_service):
    """
    PDF ì§ì ‘ ì²˜ë¦¬ (Phase 0.3.4 P2.4)
    
    í”Œë¡œìš°:
    1. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    2. HybridExtractorë¡œ í˜ì´ì§€ë³„ ì²˜ë¦¬
    3. Markdown ë³‘í•©
    4. ì˜¤íƒˆì ì •ê·œí™” (13ê°€ì§€ ë„ë©”ì¸ íŒ¨í„´)
    5. í›„ì²˜ë¦¬ ì •ê·œí™”
    6. ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (íŒŒí¸ ë³‘í•©)
    """
    
    # 1. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    st.info("ğŸ“„ PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘...")
    images = pdf_processor.pdf_to_images(pdf_path)
    logger.info(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ì¶”ì¶œ")
    st.success(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
    
    # 2. HybridExtractor ì´ˆê¸°í™”
    extractor = HybridExtractor(vlm_service, pdf_path)
    logger.info(f"âœ… HybridExtractor ì´ˆê¸°í™”")
    
    # 3. í˜ì´ì§€ë³„ ì²˜ë¦¬
    st.info("ğŸ”„ VLMìœ¼ë¡œ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
    progress_bar = st.progress(0)
    all_markdown = []
    
    for page_num, image_data in enumerate(images, 1):
        logger.info(f"ğŸ”„ í˜ì´ì§€ {page_num}/{len(images)} ì²˜ë¦¬ ì¤‘...")
        progress_bar.progress(page_num / len(images))
        
        try:
            image_b64 = image_to_base64(image_data)
            result = extractor.extract(image_b64, page_num)
            
            # í‚¤ëŠ” 'content' (GPT í”¼ë“œë°± ë°˜ì˜)
            page_md = result.get('content', '').strip()
            
            if page_md:
                all_markdown.append(page_md)
                logger.info(f"   âœ… í˜ì´ì§€ {page_num}: {len(page_md)}ì ì¶”ê°€")
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            st.warning(f"âš ï¸ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    progress_bar.progress(1.0)
    
    # í˜ì´ì§€ ë³‘í•©
    markdown = "\n\n".join(all_markdown)
    logger.info(f"âœ… ë³‘í•© ì™„ë£Œ: {len(markdown)}ì (í˜ì´ì§€ {len(all_markdown)}ê°œ)")
    st.success(f"âœ… Markdown ì¶”ì¶œ: {len(markdown):,}ì")
    
    if len(markdown) < 100:
        raise ValueError(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(markdown)}ì)")
    
    # 4. ì˜¤íƒˆì ì •ê·œí™” (13ê°€ì§€ ë„ë©”ì¸ íŒ¨í„´)
    st.info("ğŸ”§ ì˜¤íƒˆì ì •ê·œí™” ì¤‘...")
    normalizer = TypoNormalizer()
    normalized_md = normalizer.normalize(markdown)
    logger.info(f"âœ… ì •ê·œí™” ì™„ë£Œ: {len(normalized_md)}ì")
    st.success(f"âœ… ì˜¤íƒˆì êµì • ì™„ë£Œ")
    
    # 5. í›„ì²˜ë¦¬ ì •ê·œí™”
    post_normalizer = PostMergeNormalizer()
    final_md = post_normalizer.normalize(normalized_md)
    logger.info(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ: {len(final_md)}ì")
    
    # 6. ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (íŒŒí¸ ë³‘í•© ì ìš©)
    st.info("âœ‚ï¸ ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì¤‘...")
    chunker = SemanticChunker()
    chunks = chunker.chunk(final_md)
    logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
    st.success(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
    
    return {
        'markdown': final_md,
        'chunks': chunks,
        'metadata': {
            'total_pages': len(images),
            'total_chars': len(final_md),
            'total_chunks': len(chunks),
            'processing_time': datetime.now().isoformat()
        }
    }


# ============================================
# Streamlit UI
# ============================================

def main():
    """ë©”ì¸ UI"""
    
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="PRISM Phase 0.3.4 P2.4",
        page_icon="ğŸ”·",
        layout="wide"
    )
    
    # í—¤ë”
    st.title("ğŸ”· PRISM Phase 0.3.4 P2.4")
    st.caption("ì°¨ì„¸ëŒ€ ì§€ëŠ¥í˜• ë¬¸ì„œ ì´í•´ í”Œë«í¼ - ìµœì¢… ì™„ì„± ë²„ì „")
    
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” - ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        st.subheader("ğŸ“Š ë²„ì „ ì •ë³´")
        st.info("""
**Phase 0.3.4 P2.4**
- âœ… ê²½ê³„ ì •ë°€ë„ ê°•í™”
- âœ… íŒŒí¸ ìë™ ë³‘í•©
- âœ… OCR ì˜¤íƒˆì êµì • í™•ëŒ€
- âœ… ì•ˆì „ íŒŒì¼ ì‚­ì œ
        """)
        
        st.markdown("---")
        
        st.subheader("ğŸ”§ VLM ì„¤ì •")
        provider = st.selectbox(
            "VLM Provider",
            ["azure_openai"],
            index=0
        )
        
        max_pages = st.slider(
            "ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€",
            min_value=1,
            max_value=20,
            value=20,
            help="í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜"
        )
        
        st.markdown("---")
        
        st.subheader("ğŸ“– ì‚¬ìš© ë°©ë²•")
        st.markdown("""
1. PDF íŒŒì¼ ì—…ë¡œë“œ (ìµœëŒ€ 10MB)
2. 'ì²˜ë¦¬ ì‹œì‘' ë²„íŠ¼ í´ë¦­
3. ê²°ê³¼ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
        """)
    
    # ë©”ì¸ ì˜ì—­
    st.header("ğŸ“„ PDF ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type=['pdf'],
        help="ìµœëŒ€ 10MB, 20í˜ì´ì§€ê¹Œì§€ ì§€ì›"
    )
    
    if uploaded_file is not None:
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        file_size = len(uploaded_file.getvalue()) / (1024 * 1024)
        st.info(f"ğŸ“ íŒŒì¼ëª…: {uploaded_file.name} ({file_size:.2f} MB)")
        
        # íŒŒì¼ í¬ê¸° ì²´í¬
        if file_size > 10:
            st.error("âŒ íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤!")
            return
        
        # ì²˜ë¦¬ ì‹œì‘ ë²„íŠ¼
        if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary"):
            
            # âœ… ì•ˆì „í•œ ì„ì‹œ íŒŒì¼ ìƒì„±
            pdf_path = safe_temp_path(".pdf")
            
            try:
                # ì„ì‹œ íŒŒì¼ ì €ì¥
                with open(pdf_path, 'wb') as f:
                    f.write(uploaded_file.getvalue())
                
                logger.info(f"âœ… ì„ì‹œ íŒŒì¼ ì €ì¥: {pdf_path}")
                
                # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
                with st.spinner("ğŸ”§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘..."):
                    pdf_processor = PDFProcessor()
                    vlm_service = VLMServiceV50(provider=provider)
                    logger.info("âœ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # PDF ì²˜ë¦¬
                start_time = time.time()
                
                with st.spinner("ğŸ”„ PDF ì²˜ë¦¬ ì¤‘..."):
                    result = process_pdf_direct(pdf_path, pdf_processor, vlm_service)
                
                processing_time = time.time() - start_time
                
                # ì„±ê³µ ë©”ì‹œì§€
                st.success(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! ({processing_time:.1f}ì´ˆ)")
                
                # ê²°ê³¼ í‘œì‹œ
                st.markdown("---")
                st.header("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
                
                # ë©”íƒ€ë°ì´í„°
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("ì´ í˜ì´ì§€", result['metadata']['total_pages'])
                
                with col2:
                    st.metric("ì¶”ì¶œ ë¬¸ì", f"{result['metadata']['total_chars']:,}ì")
                
                with col3:
                    st.metric("ìƒì„± ì²­í¬", result['metadata']['total_chunks'])
                
                with col4:
                    st.metric("ì²˜ë¦¬ ì‹œê°„", f"{processing_time:.1f}ì´ˆ")
                
                # íƒ­ìœ¼ë¡œ ê²°ê³¼ êµ¬ë¶„
                tab1, tab2, tab3 = st.tabs(["ğŸ“ Markdown", "âœ‚ï¸ ì²­í¬", "ğŸ“Š ì²­í¬ ë¶„ì„"])
                
                with tab1:
                    st.subheader("ì¶”ì¶œëœ Markdown")
                    st.text_area(
                        "Markdown ë‚´ìš©",
                        result['markdown'],
                        height=400,
                        key="markdown_display"
                    )
                    
                    # Markdown ë‹¤ìš´ë¡œë“œ
                    st.download_button(
                        label="ğŸ“¥ Markdown ë‹¤ìš´ë¡œë“œ",
                        data=result['markdown'],
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_markdown.md",
                        mime="text/markdown"
                    )
                
                with tab2:
                    st.subheader(f"ìƒì„±ëœ ì²­í¬ ({len(result['chunks'])}ê°œ)")
                    
                    for i, chunk in enumerate(result['chunks'], 1):
                        with st.expander(f"ì²­í¬ {i} - {chunk['metadata']['type']} ({chunk['metadata']['char_count']}ì)"):
                            st.markdown(f"**ê²½ê³„:** `{chunk['metadata']['boundary']}`")
                            if chunk['metadata'].get('title'):
                                st.markdown(f"**ì œëª©:** {chunk['metadata']['title']}")
                            st.text_area(
                                "ë‚´ìš©",
                                chunk['content'],
                                height=200,
                                key=f"chunk_{i}"
                            )
                    
                    # ì²­í¬ JSON ë‹¤ìš´ë¡œë“œ
                    chunks_json = json.dumps(result['chunks'], ensure_ascii=False, indent=2)
                    st.download_button(
                        label="ğŸ“¥ ì²­í¬ JSON ë‹¤ìš´ë¡œë“œ",
                        data=chunks_json,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_chunks.json",
                        mime="application/json"
                    )
                
                with tab3:
                    st.subheader("ì²­í¬ í’ˆì§ˆ ë¶„ì„")
                    
                    # íƒ€ì…ë³„ ë¶„í¬
                    type_counts = {}
                    for chunk in result['chunks']:
                        chunk_type = chunk['metadata']['type']
                        type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
                    
                    st.markdown("**íƒ€ì…ë³„ ë¶„í¬**")
                    for chunk_type, count in type_counts.items():
                        percentage = (count / len(result['chunks'])) * 100
                        st.progress(percentage / 100, text=f"{chunk_type}: {count}ê°œ ({percentage:.1f}%)")
                    
                    # í¬ê¸° ë¶„í¬
                    st.markdown("---")
                    st.markdown("**í¬ê¸° ë¶„í¬**")
                    
                    size_ranges = {
                        "50~150ì": 0,
                        "150~300ì": 0,
                        "300~600ì": 0,
                        "600ì ì´ìƒ": 0
                    }
                    
                    for chunk in result['chunks']:
                        size = chunk['metadata']['char_count']
                        if size < 150:
                            size_ranges["50~150ì"] += 1
                        elif size < 300:
                            size_ranges["150~300ì"] += 1
                        elif size < 600:
                            size_ranges["300~600ì"] += 1
                        else:
                            size_ranges["600ì ì´ìƒ"] += 1
                    
                    for range_name, count in size_ranges.items():
                        percentage = (count / len(result['chunks'])) * 100
                        st.progress(percentage / 100, text=f"{range_name}: {count}ê°œ ({percentage:.1f}%)")
                    
                    # í‰ê·  í¬ê¸°
                    avg_size = sum(c['metadata']['char_count'] for c in result['chunks']) / len(result['chunks'])
                    st.metric("í‰ê·  ì²­í¬ í¬ê¸°", f"{avg_size:.0f}ì")
                
            except Exception as e:
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            finally:
                # âœ… ì•ˆì „í•œ íŒŒì¼ ì‚­ì œ
                safe_remove(pdf_path)
                gc.collect()
    
    else:
        st.info("ğŸ‘† PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”")
        
        # ìƒ˜í”Œ ê²°ê³¼ í‘œì‹œ
        st.markdown("---")
        st.header("ğŸ“– Phase 0.3.4 P2.4 ì£¼ìš” ê°œì„ ì‚¬í•­")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ¯ ê²½ê³„ ì •ë°€ë„ ê°•í™”")
            st.code("""
# Before
'ì œ28ì¡°ì œ2í•­' â†’ ê²½ê³„ë¡œ ì˜¤ì¸ âŒ

# After  
'ì œ28ì¡°ì œ2í•­' â†’ ì¡°ë¬¸ ì°¸ì¡°ë¡œ ì¸ì‹ âœ…
            """, language="python")
            
            st.subheader("ğŸ§© íŒŒí¸ ìë™ ë³‘í•©")
            st.code("""
# Before
18ê°œ ì²­í¬ (50~150ì: 8ê°œ)

# After
12~14ê°œ ì²­í¬ (200~600ì ì§‘ì¤‘)
            """, language="python")
        
        with col2:
            st.subheader("ğŸ”§ OCR ì˜¤íƒˆì êµì •")
            st.code("""
# 13ê°€ì§€ ë„ë©”ì¸ íŒ¨í„´ ì¶”ê°€
- "ê¸° ë³¸ ì • ì‹ " â†’ "ê¸°ë³¸ì •ì‹ "
- "ìš©ìƒ" â†’ "í†µìƒ"
- "ì „ì¡±" â†’ "ì „ì†"
- "í•´íŒŒêµ°ì§ì±„ìš©" â†’ "ì˜ˆë¹„êµ°ì§€íœ˜ê´€"
            """, language="python")
            
            st.subheader("ğŸ”’ ì•ˆì „ íŒŒì¼ ì‚­ì œ")
            st.code("""
# Before
WinError 32 ë°œìƒ âŒ

# After
ì¬ì‹œë„ + GC â†’ ì•ˆì „ ì‚­ì œ âœ…
            """, language="python")
    
    # í‘¸í„°
    st.markdown("---")
    st.caption("ğŸ”· PRISM Phase 0.3.4 P2.4 | Developed by ë§ˆì°½ìˆ˜ì‚°íŒ€")


if __name__ == "__main__":
    main()