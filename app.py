"""
app.py - PRISM Phase 0.9
Annex ì„œë¸Œì²­í‚¹ + Promotion Lookup ê³„ì‚°ê¸° í†µí•©

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€
Date: 2025-11-18
Version: Phase 0.9.0
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import json
import os
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ëª¨ë“ˆ Import
try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.hybrid_extractor import HybridExtractor
    from core.semantic_chunker import SemanticChunker
    from core.dual_qa_gate import DualQAGate, extract_pdf_text_layer
    from core.utils_fs import safe_temp_path, safe_remove
    
    logger.info("âœ… ëª¨ë“ˆ import ì„±ê³µ")
    
except Exception as e:
    logger.error(f"âŒ Import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()

# LawParser Import
try:
    from core.law_parser import LawParser
    LAW_MODE_AVAILABLE = True
    logger.info("âœ… LawParser ë¡œë“œ ì„±ê³µ")
except ImportError:
    LAW_MODE_AVAILABLE = False
    logger.warning("âš ï¸ LawParser ë¯¸ì„¤ì¹˜")

# DocumentProfile Import
try:
    from core.document_profile import auto_detect_profile
    PROFILE_AVAILABLE = True
    logger.info("âœ… DocumentProfile ë¡œë“œ ì„±ê³µ")
except ImportError:
    PROFILE_AVAILABLE = False
    logger.warning("âš ï¸ DocumentProfile ë¯¸ì„¤ì¹˜")

# âœ… Phase 0.9: Promotion Lookup Import
try:
    sys.path.insert(0, str(Path(__file__).parent / 'research'))
    from promotion_lookup import PromotionRangeLookup
    PROMOTION_LOOKUP_AVAILABLE = True
    logger.info("âœ… PromotionLookup ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    PROMOTION_LOOKUP_AVAILABLE = False
    logger.warning(f"âš ï¸ PromotionLookup ë¯¸ì„¤ì¹˜: {e}")


LAW_SPACING_KEYWORDS = [
    "ì„ìš©", "ìŠ¹ì§„", "ë³´ìˆ˜", "ë³µë¬´", "ì§•ê³„", "í‡´ì§",
    "ì±„ìš©", "ì¸ì‚¬", "ì§ì›", "ê³µì‚¬", "ìˆ˜ìŠµ", "ê²°ê²©ì‚¬ìœ ",
    "ê·œì •", "ì¡°ì§", "ë¬¸í™”", "ì—­ëŸ‰", "íƒœë„", "ê°œì„ "
]


def apply_law_spacing(text: str) -> str:
    """Phase 0.7 ë£° ê¸°ë°˜ ë„ì–´ì“°ê¸° (ë¯¸ì„¸ì¡°ì •)"""
    
    logger.info("   âœ… ì¡°ë¬¸/í‘œ ì œëª© íŒ¨í„´ ë³´ì • ì‹œì‘")
    text = re.sub(r"ì œ\s*(\d+)\s*ì¡°\s*ì˜\s*(\d+)", r"ì œ\1ì¡°ì˜\2", text)
    text = re.sub(r"ì œ\s*(\d+)\s*ì¡°", r"ì œ\1ì¡°", text)
    text = re.sub(r"í‘œ\s*(\d+)", r"í‘œ\1", text)
    text = re.sub(r"\[ë³„í‘œ\s*(\d+)\]", r"[ë³„í‘œ\1]", text)
    logger.info("   âœ… ì¡°ë¬¸/í‘œ ì œëª© íŒ¨í„´ ë³´ì • ì™„ë£Œ")
    
    logger.info("   âœ… ìˆ«ì/ë‹¨ìœ„ ê³µë°± ìµœì í™” ì‹œì‘")
    text = re.sub(r"(\d+)\s*(ë§Œì›|ì–µì›|ì²œì›|ì›)", r"\1\2", text)
    text = re.sub(r"(\d+)\s*(ëª…|ê°œ|ê±´|íšŒ|ë…„|ì›”|ì¼)", r"\1\2", text)
    text = re.sub(r"(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})", r"\1.\2.\3", text)
    logger.info("   âœ… ìˆ«ì/ë‹¨ìœ„ ê³µë°± ìµœì í™” ì™„ë£Œ")
    
    logger.info("   âœ… ì¡°ì‚¬ ì• ê³µë°± ì œê±° ì‹œì‘")
    josa_list = ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ê³¼", "ì™€", "ì—", "ì—ì„œ", "ì—ê²Œ", "ë¡œ", "ìœ¼ë¡œ"]
    for josa in josa_list:
        text = re.sub(rf"([ê°€-í£]+)\s?{josa}\s?([ê°€-í£])", rf"\1{josa} \2", text)
    logger.info("   âœ… ì¡°ì‚¬ ì• ê³µë°± ì œê±° ì™„ë£Œ")
    
    logger.info("   âœ… í‘œ ì£¼ì„ ì¤„ë°”ê¿ˆ ì•ˆì •í™” ì‹œì‘")
    comment_starters = ["â€»", "ë¹„ê³ :", "ì£¼:", "ë‹¨,", "ë‹¤ë§Œ,"]
    for starter in comment_starters:
        escaped = re.escape(starter)
        text = re.sub(rf"([^\n]){escaped}", rf"\1\n{starter}", text)
    logger.info("   âœ… í‘œ ì£¼ì„ ì¤„ë°”ê¿ˆ ì•ˆì •í™” ì™„ë£Œ")
    
    for kw in LAW_SPACING_KEYWORDS:
        text = re.sub(rf"([ê°€-í£0-9]){kw}", rf"\1 {kw}", text)
    
    text = re.sub(r"([\.!?])([ê°€-í£0-9])", r"\1 \2", text)
    text = re.sub(r"[ ]{2,}", " ", text)
    
    lines = []
    for line in text.splitlines():
        cleaned = line.strip()
        if cleaned:
            lines.append(cleaned)
    
    text = "\n".join(lines)
    
    logger.info("   âœ… Phase 0.7 ë£° ê¸°ë°˜ ë„ì–´ì“°ê¸° ì ìš© ì™„ë£Œ")
    
    return text


def to_review_md_basic(
    chunks: list,
    parsed_result: dict = None,
    base_markdown: str = None
) -> str:
    """ì²­í¬/íŒŒì‹± ê²°ê³¼ â†’ ë¦¬ë·°ìš© Markdown"""
    
    if base_markdown:
        logger.info("   ğŸ“‹ base_markdown ì‚¬ìš©")
        return base_markdown
    
    if parsed_result is not None:
        logger.info("   ğŸ“‹ LawParser ë§ˆí¬ë‹¤ìš´ ìƒì„±")
        parser = LawParser()
        return parser.to_markdown(parsed_result)
    
    logger.info("   ğŸ“‹ chunks ì¡°í•© (ë°±ì—…)")
    lines = []
    
    for chunk in chunks:
        content = chunk['content']
        meta = chunk['metadata']
        chunk_type = meta.get('type', '')
        
        if chunk_type == 'title':
            lines.append(f"# {content}")
            lines.append("")
        
        elif chunk_type == 'amendment_history':
            lines.append("## ê°œì • ì´ë ¥")
            lines.append("")
            lines.append(f"- {content}")
            lines.append("")
        
        elif chunk_type == 'basic':
            lines.append("## ê¸°ë³¸ì •ì‹ ")
            lines.append("")
            lines.append(content)
            lines.append("")
        
        elif chunk_type == 'chapter':
            chapter_num = meta.get('chapter_number', '')
            chapter_title = meta.get('chapter_title', '')
            lines.append(f"## {chapter_num} {chapter_title}")
            lines.append("")
        
        elif chunk_type == 'article':
            article_num = meta.get('article_number', '')
            article_title = meta.get('article_title', '')
            lines.append(f"### {article_num}({article_title})")
            lines.append("")
            
            body = content.split('\n', 1)[-1] if '\n' in content else content
            lines.append(body)
            lines.append("")
        
        elif chunk_type.startswith('annex'):
            # Phase 0.8: ì„œë¸Œì²­í¬ íƒ€ì… ì²˜ë¦¬
            if 'header' in chunk_type:
                lines.append(f"## {content.split(chr(10))[0]}")
            elif 'note' in chunk_type:
                lines.append(content)
            else:
                lines.append(content)
            lines.append("")
    
    return "\n".join(lines)


def process_document_vlm_mode(pdf_path: str, pdf_text: str):
    """VLM Mode íŒŒì´í”„ë¼ì¸"""
    
    st.info("ğŸ–¼ï¸ VLM Mode: ì´ë¯¸ì§€ ê¸°ë°˜ ì²˜ë¦¬ ì¤‘...")
    progress_bar = st.progress(0)
    
    try:
        processor = PDFProcessor()
        pages = processor.process(pdf_path)
        max_pages = 20
        if len(pages) > max_pages:
            st.warning(f"âš ï¸ í˜ì´ì§€ ìˆ˜ ì œí•œ: {len(pages)} â†’ {max_pages}")
            pages = pages[:max_pages]
        
        vlm_service = VLMServiceV50(provider='azure_openai')
        extractor = HybridExtractor(vlm_service)
        markdown_text = extractor.extract(pages)
        progress_bar.progress(50)
        
        st.info("ğŸ§© ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì¤‘...")
        chunker = SemanticChunker()
        chunks = chunker.chunk(markdown_text)
        st.success(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        st.info("ğŸ”¬ DualQA ê²€ì¦ ì¤‘...")
        qa_gate = DualQAGate()
        qa_result = qa_gate.validate(
            pdf_text=pdf_text,
            processed_text=markdown_text,
            source="vlm"
        )
        
        progress_bar.progress(100)
        
        return {
            'rag_markdown': markdown_text,
            'chunks': chunks,
            'qa_result': qa_result,
            'is_qa_pass': qa_result.get('is_pass', False),
            'mode': 'VLM Mode'
        }
    
    except Exception as e:
        logger.error(f"âŒ VLM ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


def process_document_law_mode(pdf_path: str, pdf_text: str, document_title: str):
    """LawMode íŒŒì´í”„ë¼ì¸ (Phase 0.8)"""
    
    st.info("ğŸ“œ LawMode: ê·œì •/ë²•ë ¹ íŒŒì‹± ì¤‘...")
    progress_bar = st.progress(0)
    
    if PROFILE_AVAILABLE:
        profile = auto_detect_profile(pdf_text, document_title)
        st.info(f"ğŸ“ ë¬¸ì„œ í”„ë¡œíŒŒì¼: {profile.name}")
    
    parser = LawParser()
    
    # âœ… Phase 0.8: parser.parse() ì§ì ‘ í˜¸ì¶œ
    parsed_result = parser.parse(
        pdf_text=pdf_text,
        document_title=document_title,
        clean_artifacts=True,
        normalize_linebreaks=True
    )
    
    progress_bar.progress(50)
    
    # âœ… Phase 0.8: ì„œë¸Œì²­í‚¹ ì ìš©ëœ chunks
    chunks = parser.to_chunks(parsed_result)
    progress_bar.progress(75)
    
    rag_markdown = parser.to_markdown(parsed_result)
    
    st.info("ğŸ”¬ DualQA ê²€ì¦ ì¤‘...")
    qa_gate = DualQAGate()
    qa_result = qa_gate.validate(
        pdf_text=pdf_text,
        processed_text=rag_markdown,
        source="law"
    )
    
    progress_bar.progress(100)
    
    return {
        'rag_markdown': rag_markdown,
        'chunks': chunks,
        'qa_result': qa_result,
        'is_qa_pass': qa_result.get('is_pass', False),
        'parsed_result': parsed_result,
        'mode': 'LawMode'
    }


# ============================================
# âœ… Phase 0.9: Promotion Lookup ê³„ì‚°ê¸°
# ============================================

def render_promotion_calculator():
    """ìŠ¹ì§„í›„ë³´ì ë²”ìœ„ ê³„ì‚°ê¸° UI"""
    
    st.sidebar.header("ğŸ§® ìŠ¹ì§„í›„ë³´ì ë²”ìœ„ ê³„ì‚°ê¸°")
    st.sidebar.markdown("**Phase 0.9 - Golden Set ê¸°ë°˜**")
    
    if not PROMOTION_LOOKUP_AVAILABLE:
        st.sidebar.error("âŒ Promotion Lookup ëª¨ë“ˆ ì—†ìŒ")
        st.sidebar.info("research/promotion_lookup.py í™•ì¸ í•„ìš”")
        return
    
    try:
        # Lookup ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        lookup = PromotionRangeLookup()
        
        # ë©”íƒ€ë°ì´í„° í‘œì‹œ
        metadata = lookup.get_metadata()
        
        with st.sidebar.expander("ğŸ“Š Golden Set ì •ë³´", expanded=False):
            st.write(f"**í‘œ ID:** {metadata['table_id']}")
            st.write(f"**ë“±ê¸‰:** {metadata['grade_type']}")
            st.write(f"**ê´€ë ¨ ì¡°ë¬¸:** {metadata['related_article']}")
            st.write(f"**ì „ì²´ í–‰:** {metadata['total_rows']}ê°œ")
            st.write(f"**ì¶œì²˜:** {metadata['source']}")
        
        # ì…ë ¥
        st.sidebar.subheader("ğŸ“¥ ì…ë ¥")
        people = st.sidebar.number_input(
            "ì„ìš©í•˜ê³ ì í•˜ëŠ” ì¸ì›ìˆ˜",
            min_value=1,
            max_value=100,
            value=47,
            step=1,
            help="1~75ëª… ë²”ìœ„ì—ì„œ ì…ë ¥"
        )
        
        # ì¡°íšŒ ë²„íŠ¼
        if st.sidebar.button("ğŸ” ì¡°íšŒ", type="primary"):
            result = lookup.query(people)
            
            if result:
                st.sidebar.success("âœ… ì¡°íšŒ ì„±ê³µ!")
                st.sidebar.markdown("---")
                st.sidebar.subheader("ğŸ“‹ ê²°ê³¼")
                st.sidebar.metric("ì„ìš© ì¸ì›", f"{result['people']}ëª…")
                st.sidebar.metric("ìŠ¹ì§„í›„ë³´ì ë²”ìœ„", f"ì„œì—´ {result['rank_max']}ë²ˆê¹Œì§€")
                st.sidebar.info(f"**ì¶œì²˜:** {result['source']}")
                st.sidebar.info(f"**ì‹ ë¢°ë„:** {result['confidence']*100:.0f}%")
                
                # JSON ë‹¤ìš´ë¡œë“œ
                result_json = json.dumps(result, ensure_ascii=False, indent=2)
                st.sidebar.download_button(
                    label="ğŸ“¥ ê²°ê³¼ JSON ë‹¤ìš´ë¡œë“œ",
                    data=result_json,
                    file_name=f"promotion_result_{people}ëª….json",
                    mime="application/json"
                )
            else:
                st.sidebar.error(f"âŒ ì¡°íšŒ ì‹¤íŒ¨: {people}ëª…ì€ Golden Set ë²”ìœ„(1-75) ë°–ì…ë‹ˆë‹¤.")
        
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        st.sidebar.markdown("---")
        st.sidebar.subheader("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸")
        test_cases = [1, 5, 10, 20, 47, 50, 75]
        
        for test_people in test_cases:
            result = lookup.query(test_people)
            if result:
                st.sidebar.write(f"â€¢ {test_people}ëª… â†’ {result['rank_max']}ë²ˆê¹Œì§€")
    
    except Exception as e:
        st.sidebar.error(f"âŒ ê³„ì‚°ê¸° ì˜¤ë¥˜: {e}")
        logger.error(f"Promotion Calculator ì˜¤ë¥˜: {e}", exc_info=True)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    st.set_page_config(
        page_title="PRISM Phase 0.9",
        page_icon="ğŸ”·",
        layout="wide"
    )
    
    st.title("ğŸ”· PRISM Phase 0.9")
    st.markdown("**Progressive Reasoning & Intelligence for Structured Materials**")
    st.markdown("**Annex ì„œë¸Œì²­í‚¹ + Promotion Lookup ê³„ì‚°ê¸°**")
    
    # âœ… Phase 0.9: ì‚¬ì´ë“œë°” ê³„ì‚°ê¸°
    render_promotion_calculator()
    
    # ë©”ì¸ ì˜ì—­: ë¬¸ì„œ ì²˜ë¦¬
    st.header("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬")
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['pdf'],
        help="ì¸ì‚¬ê·œì •, ë²•ë ¹ ë“± ê·œì • ë¬¸ì„œ"
    )
    
    if not uploaded_file:
        st.info("ğŸ‘† PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì²˜ë¦¬ê°€ ì‹œì‘ë©ë‹ˆë‹¤.")
        
        # Phase 0.9 ì•ˆë‚´
        st.markdown("---")
        st.subheader("ğŸ†• Phase 0.9 ì‹ ê¸°ëŠ¥")
        st.success("**âœ… ìŠ¹ì§„í›„ë³´ì ë²”ìœ„ ê³„ì‚°ê¸°** (ì™¼ìª½ ì‚¬ì´ë“œë°”)")
        st.info("""
        **Golden Set ê¸°ë°˜ 100% ì •í™•ë„ ë³´ì¥**
        - ì„ìš© ì¸ì›ìˆ˜ ì…ë ¥ â†’ ìŠ¹ì§„í›„ë³´ì ë²”ìœ„ ì¦‰ì‹œ ì¡°íšŒ
        - ë³„í‘œ1 (3ê¸‰ ìŠ¹ì§„ ì œì™¸) ê¸°ì¤€
        - JSON ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì§€ì›
        """)
        
        return
    
    # ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ
    mode = st.radio(
        "ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ",
        ["LawMode (ê·œì •/ë²•ë ¹)", "VLM Mode (ì¼ë°˜ ë¬¸ì„œ)"],
        help="LawMode: ì¡°ë¬¸ êµ¬ì¡° íŒŒì‹± | VLM Mode: ì´ë¯¸ì§€ ê¸°ë°˜ ì²˜ë¦¬"
    )
    
    process_mode = "law" if "LawMode" in mode else "vlm"
    
    if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary"):
        try:
            # ì„ì‹œ íŒŒì¼ ì €ì¥
            temp_pdf = safe_temp_path(uploaded_file.name)
            with open(temp_pdf, 'wb') as f:
                f.write(uploaded_file.getbuffer())
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            pdf_text = extract_pdf_text_layer(str(temp_pdf))
            
            # ì²˜ë¦¬ ëª¨ë“œ ë¶„ê¸°
            if process_mode == "law":
                result = process_document_law_mode(
                    str(temp_pdf),
                    pdf_text,
                    uploaded_file.name
                )
            else:
                result = process_document_vlm_mode(
                    str(temp_pdf),
                    pdf_text
                )
            
            # ê²°ê³¼ í‘œì‹œ
            st.success(f"âœ… ì²˜ë¦¬ ì™„ë£Œ ({result['mode']})")
            
            # DualQA ê²°ê³¼
            qa_result = result['qa_result']
            if result['is_qa_pass']:
                st.success(f"âœ… DualQA í†µê³¼ (ì»¤ë²„ë¦¬ì§€: {qa_result.get('text_coverage', 0)*100:.1f}%)")
            else:
                st.warning(f"âš ï¸ DualQA ê²½ê³  (ì»¤ë²„ë¦¬ì§€: {qa_result.get('text_coverage', 0)*100:.1f}%)")
            
            # ì²­í¬ í†µê³„
            st.subheader("ğŸ“Š ì²­í¬ í†µê³„")
            chunks = result['chunks']
            st.write(f"- ì´ ì²­í¬: {len(chunks)}ê°œ")
            
            # Phase 0.8: Annex ì„œë¸Œì²­í¬ ê°•ì¡°
            annex_chunks = [c for c in chunks if 'annex' in c.get('metadata', {}).get('type', '')]
            if annex_chunks:
                st.success(f"âœ… Annex ì„œë¸Œì²­í¬: {len(annex_chunks)}ê°œ")
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.download_button(
                    label="ğŸ“¥ engine.md",
                    data=result['rag_markdown'],
                    file_name="engine.md",
                    mime="text/markdown"
                )
            
            with col2:
                st.download_button(
                    label="ğŸ“¥ chunks.json",
                    data=json.dumps(chunks, ensure_ascii=False, indent=2),
                    file_name="chunks.json",
                    mime="application/json"
                )
            
            with col3:
                review_md = to_review_md_basic(
                    chunks,
                    result.get('parsed_result'),
                    result['rag_markdown']
                )
                st.download_button(
                    label="ğŸ“¥ review.md",
                    data=review_md,
                    file_name="review.md",
                    mime="text/markdown"
                )
            
            # ë¯¸ë¦¬ë³´ê¸°
            with st.expander("ğŸ“„ engine.md ë¯¸ë¦¬ë³´ê¸°"):
                st.markdown(result['rag_markdown'][:2000] + "..." if len(result['rag_markdown']) > 2000 else result['rag_markdown'])
            
            with st.expander("ğŸ” chunks.json ë¯¸ë¦¬ë³´ê¸°"):
                st.json(chunks[:3])
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            safe_remove(temp_pdf)
        
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            st.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")


if __name__ == '__main__':
    main()