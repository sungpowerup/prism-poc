"""
app.py - PRISM Phase 0.9.5.2
ë¬¸ì„œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

Phase 0.9.5.2 ìˆ˜ì •:
- âœ… annex_paragraph íƒ€ì… review.md ë Œë”ë§ ì¶”ê°€
- âœ… Phase 0.9.5.1 ì•ˆì •ì„± 100% ìœ ì§€

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT ë¯¸ì†¡ë‹˜ ê°€ì´ë“œ
Date: 2025-11-24
Version: Phase 0.9.5.2
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import json
import os
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ëª¨ë“ˆ Import
try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.hybrid_extractor import HybridExtractor
    from core.semantic_chunker import SemanticChunker
    from core.dual_qa_gate import DualQAGate, extract_pdf_text_layer
    from core.utils_fs import safe_temp_path, safe_remove
    
    logger.info("âœ… ëª¨ë“ˆ import ì„±ê³µ")
    
except Exception as e:
    logger.error(f"âŒ Import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()

# LawParser Import
try:
    from core.law_parser import LawParser
    LAW_MODE_AVAILABLE = True
    logger.info("âœ… LawParser ë¡œë“œ ì„±ê³µ")
except ImportError:
    LAW_MODE_AVAILABLE = False
    logger.warning("âš ï¸ LawParser ë¯¸ì„¤ì¹˜")

# DocumentProfile Import
try:
    from core.document_profile import auto_detect_profile
    PROFILE_AVAILABLE = True
    logger.info("âœ… DocumentProfile ë¡œë“œ ì„±ê³µ")
except ImportError:
    PROFILE_AVAILABLE = False
    logger.warning("âš ï¸ DocumentProfile ë¯¸ì„¤ì¹˜")

# TableParser Import
try:
    from research.table_parser import TableParser
    TABLE_PARSER_AVAILABLE = True
    logger.info("âœ… TableParser ë¡œë“œ ì„±ê³µ (Phase 0.9.5.2)")
except ImportError:
    TABLE_PARSER_AVAILABLE = False
    logger.warning("âš ï¸ TableParser ë¯¸ì„¤ì¹˜ - í…Œì´ë¸” êµ¬ì¡°í™” ë¹„í™œì„±í™”")


# âœ… Phase 0.9.8.4: DocumentClassifier Import
try:
    from core.document_classifier import DocumentClassifier
    CLASSIFIER_AVAILABLE = True
    logger.info("âœ… DocumentClassifier ë¡œë“œ ì„±ê³µ (Phase 0.9.8.4)")
except ImportError:
    CLASSIFIER_AVAILABLE = False
    logger.warning("âš ï¸ DocumentClassifier ë¯¸ì„¤ì¹˜ - ìë™ ë¶„ë¥˜ ë¹„í™œì„±í™”")



LAW_SPACING_KEYWORDS = [
    "ì„ìš©", "ìŠ¹ì§„", "ë³´ìˆ˜", "ë³µë¬´", "ì§•ê³„", "í‡´ì§",
    "ì±„ìš©", "ì¸ì‚¬", "ì§ì›", "ê³µì‚¬", "ìˆ˜ìŠµ", "ê²°ê²©ì‚¬ìœ ",
    "ê·œì •", "ì¡°ì§", "ë¬¸í™”", "ì—­ëŸ‰", "íƒœë„", "ê°œì„ "
]


def apply_law_spacing(text: str) -> str:
    """Phase 0.7 ë£° ê¸°ë°˜ ë„ì–´ì“°ê¸° (ë¯¸ì„¸ì¡°ì •)"""
    
    text = re.sub(r"ì œ\s*(\d+)\s*ì¡°\s*ì˜\s*(\d+)", r"ì œ\1ì¡°ì˜\2", text)
    text = re.sub(r"ì œ\s*(\d+)\s*ì¡°", r"ì œ\1ì¡°", text)
    text = re.sub(r"í‘œ\s*(\d+)", r"í‘œ\1", text)
    text = re.sub(r"\[ë³„í‘œ\s*(\d+)\]", r"[ë³„í‘œ\1]", text)
    
    text = re.sub(r"(\d+)\s*(ë§Œì›|ì–µì›|ì²œì›|ì›)", r"\1\2", text)
    text = re.sub(r"(\d+)\s*(ëª…|ê°œ|ê±´|íšŒ|ë…„|ì›”|ì¼)", r"\1\2", text)
    text = re.sub(r"(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})", r"\1.\2.\3", text)
    
    josa_list = ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ê³¼", "ì™€", "ì—", "ì—ì„œ", "ì—ê²Œ", "ë¡œ", "ìœ¼ë¡œ"]
    for josa in josa_list:
        text = re.sub(rf"([ê°€-í£]+)\s?{josa}\s?([ê°€-í£]+)", rf"\1{josa} \2", text)
    
    return text


def generate_qa_summary(
    document_title: str,
    pdf_text_len: int,
    processed_text_len: int,
    parsed_result: dict,
    qa_result: dict,
    chunks: list,
    table_stats: dict = None
) -> str:
    """QA Summary ìƒì„±"""
    
    lines = []
    
    lines.extend([
        "=" * 60,
        f"ë¬¸ì„œëª…: {document_title}",
        "=" * 60,
        "",
        "[ì²˜ë¦¬ ê²°ê³¼]",
        f"- PDF í…ìŠ¤íŠ¸: {pdf_text_len:,}ì",
        f"- ì²˜ë¦¬ í…ìŠ¤íŠ¸: {processed_text_len:,}ì",
        f"- ì´ ì¥: {parsed_result.get('total_chapters', 0)}ê°œ",
        f"- ì´ ì¡°ë¬¸: {parsed_result.get('total_articles', 0)}ê°œ",
        f"- ê°œì •ì´ë ¥: {len(parsed_result.get('amendment_history', []))}ê±´",
        f"- ì²­í¬: {len(chunks)}ê°œ",
        ""
    ])
    
    # í…Œì´ë¸” í†µê³„
    if table_stats:
        lines.append("[í…Œì´ë¸” êµ¬ì¡°í™”]")
        if table_stats:
            for table_id, count in table_stats.items():
                lines.append(f"  Â· {table_id}: {count}í–‰")
    
    # QA ê²°ê³¼
    match_rate = qa_result.get('match_rate', 0) * 100
    is_pass = qa_result.get('is_pass', False)
    qa_flags = qa_result.get('qa_flags', [])
    
    lines.extend([
        "",
        "[QA ê²°ê³¼]",
        f"- ì¡°ë¬¸ í—¤ë” ë§¤ì¹­ë¥ : {match_rate:.0f}% ({parsed_result.get('total_articles', 0)}/{parsed_result.get('total_articles', 0)})",
        f"- ì´ìƒ ì§•í›„: {', '.join(qa_flags) if qa_flags else 'ì—†ìŒ'}",
        f"- íŒì •: {'âœ… PASS' if is_pass else 'âš ï¸ WARNING'}",
    ])
    
    return "\n".join(lines)


def to_review_md_basic(
    chunks: list,
    parsed_result: dict,
    base_markdown: str,
    qa_summary: str = None
) -> str:
    """
    review.md ìƒì„± (ì‚¬ëŒ ê²€ìˆ˜ìš©)
    
    âœ… Phase 0.9.5.2 ìˆ˜ì •:
    - annex_paragraph íƒ€ì… ì²˜ë¦¬ ì¶”ê°€
    - ë³„í‘œ ë¬¸ë‹¨ì´ review.mdì— í‘œì‹œë˜ë„ë¡ ê°œì„ 
    """
    
    lines = []
    
    # QA Summary ë¸”ë¡ ì¶”ê°€
    if qa_summary:
        lines.append("```")
        lines.append(qa_summary)
        lines.append("```")
        lines.append("")
        lines.append("---")
        lines.append("")
    
    # ë¬¸ì„œ ì œëª©
    if parsed_result.get('document_title'):
        lines.append(f"# {parsed_result['document_title']}")
        lines.append("")
    
    # ë³¸ë¬¸ ì²­í¬
    for chunk in chunks:
        content = chunk.get('content', '')
        chunk_type = chunk.get('metadata', {}).get('type', '')
        
        if chunk_type == 'title':
            continue
        elif chunk_type == 'chapter':
            lines.append(f"## {content}")
        elif chunk_type == 'article':
            article_num = chunk.get('metadata', {}).get('article_number', '')
            article_title = chunk.get('metadata', {}).get('article_title', '')
            if article_num:
                lines.append(f"### {article_num}({article_title})")
            lines.append(content)
        elif chunk_type == 'table_row':
            # Phase 0.9.1: í…Œì´ë¸” í–‰ í‘œì‹œ
            table_id = chunk.get('metadata', {}).get('table_id', '')
            row_num = chunk.get('metadata', {}).get('ì„ìš©ì¸ì›ìˆ˜', '')
            rank = chunk.get('metadata', {}).get('ì„œì—´ëª…ë¶€ìˆœìœ„', '')
            lines.append(f"- [{table_id}] {row_num}ëª… â†’ {rank}ë²ˆê¹Œì§€")
        elif chunk_type == 'annex_paragraph':
            # âœ… Phase 0.9.5.2: annex_paragraph íƒ€ì… ì²˜ë¦¬ ì¶”ê°€
            lines.append("")
            lines.append(content)
            lines.append("")
        elif 'header' in chunk_type:
            lines.append(f"## {content.split(chr(10))[0]}")
        elif 'note' in chunk_type:
            lines.append(content)
        else:
            lines.append(content)
        lines.append("")
    
    return '\n'.join(lines)


def process_document_vlm_mode(pdf_path: str, pdf_text: str):
    """VLM Mode íŒŒì´í”„ë¼ì¸"""
    
    st.info("ğŸ–¼ï¸ VLM Mode: ì´ë¯¸ì§€ ê¸°ë°˜ ì²˜ë¦¬ ì¤‘...")
    progress_bar = st.progress(0)
    
    try:
        processor = PDFProcessor()
        pages = processor.process(pdf_path)
        max_pages = 20
        if len(pages) > max_pages:
            st.warning(f"âš ï¸ í˜ì´ì§€ ìˆ˜ ì œí•œ: {len(pages)} â†’ {max_pages}")
            pages = pages[:max_pages]
        
        vlm_service = VLMServiceV50(provider='azure_openai')
        extractor = HybridExtractor(vlm_service)
        markdown_text = extractor.extract(pages)
        progress_bar.progress(50)
        
        st.info("ğŸ§© ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì¤‘...")
        chunker = SemanticChunker()
        chunks = chunker.chunk(markdown_text)
        st.success(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        st.info("ğŸ”¬ DualQA ê²€ì¦ ì¤‘...")
        qa_gate = DualQAGate()
        qa_result = qa_gate.validate(
            pdf_text=pdf_text,
            processed_text=markdown_text,
            source="vlm"
        )
        
        progress_bar.progress(100)
        
        return {
            'rag_markdown': markdown_text,
            'chunks': chunks,
            'qa_result': qa_result,
            'is_qa_pass': qa_result.get('is_pass', False),
            'mode': 'VLM Mode'
        }
    
    except Exception as e:
        logger.error(f"âŒ VLM ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


def process_document_law_mode(pdf_path: str, pdf_text: str, document_title: str):
    """
    LawMode íŒŒì´í”„ë¼ì¸ (Phase 0.9.5.2)
    
    âœ… Phase 0.9.5.2:
    - annex_paragraph review ë Œë”ë§ ì¶”ê°€
    - Phase 0.9.5.1 ì•ˆì •ì„± 100% ìœ ì§€
    """
    
    st.info("ğŸ“œ LawMode: ê·œì •/ë²•ë ¹ íŒŒì‹± ì¤‘...")
    progress_bar = st.progress(0)
    
    if PROFILE_AVAILABLE:
        profile = auto_detect_profile(pdf_text, document_title)
        st.info(f"ğŸ“ ë¬¸ì„œ í”„ë¡œíŒŒì¼: {profile.name}")
    
    parser = LawParser()
    
    parsed_result = parser.parse(
        pdf_text=pdf_text,
        document_title=document_title,
        clean_artifacts=True,
        normalize_linebreaks=True
    )
    
    progress_bar.progress(40)
    
    chunks = parser.to_chunks(parsed_result)
    
    # TableParser í†µí•©
    table_structured = False
    table_stats = {}
    
    if TABLE_PARSER_AVAILABLE:
        try:
            st.info("ğŸ“Š TableParser êµ¬ì¡°í™” ì‹œë„ ì¤‘...")
            table_parser = TableParser()
            
            # annex_table_rows ì²­í¬ ì°¾ê¸°
            table_chunks = [c for c in chunks if c.get('metadata', {}).get('type') == 'annex_table_rows']
            
            if table_chunks:
                logger.info(f"   ğŸ“Š {len(table_chunks)}ê°œ í‘œ ì²­í¬ ë°œê²¬")
                
                new_chunks = []
                for chunk in chunks:
                    if chunk.get('metadata', {}).get('type') == 'annex_table_rows':
                        # TableParser ì‹œë„
                        raw_text = chunk.get('content', '')
                        structured = table_parser.parse(raw_text)
                        
                        if structured and len(structured) > 0:
                            # êµ¬ì¡°í™” ì„±ê³µ
                            logger.info(f"      âœ… êµ¬ì¡°í™” ì„±ê³µ: {len(structured)}í–‰")
                            
                            for row in structured:
                                new_chunks.append({
                                    'content': f"{row.get('ì„ìš©ì¸ì›ìˆ˜', '')}ëª… â†’ {row.get('ì„œì—´ëª…ë¶€ìˆœìœ„', '')}ë²ˆê¹Œì§€",
                                    'metadata': {
                                        'type': 'table_row',
                                        'table_id': row.get('table_id', ''),
                                        **row
                                    }
                                })
                            
                            table_id = structured[0].get('table_id', 'unknown')
                            table_stats[table_id] = len(structured)
                            table_structured = True
                        else:
                            # êµ¬ì¡°í™” ì‹¤íŒ¨ - ê¸°ì¡´ ìœ ì§€
                            logger.warning("      âš ï¸ êµ¬ì¡°í™” ì‹¤íŒ¨ - ê¸°ì¡´ í˜•ì‹ ìœ ì§€")
                            new_chunks.append(chunk)
                    else:
                        new_chunks.append(chunk)
                
                if table_structured:
                    chunks = new_chunks
                    st.success(f"âœ… TableParser êµ¬ì¡°í™” ì™„ë£Œ")
            else:
                logger.info("   â„¹ï¸ í‘œ ì²­í¬ ì—†ìŒ - TableParser ê±´ë„ˆëœ€")
        
        except Exception as e:
            logger.error(f"âŒ TableParser ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            st.warning(f"âš ï¸ TableParser ì²˜ë¦¬ ì‹¤íŒ¨ - ê¸°ì¡´ í˜•ì‹ ìœ ì§€")
    
    progress_bar.progress(60)
    
    rag_markdown = parser.to_markdown(parsed_result)
    
    st.info("ğŸ”¬ DualQA ê²€ì¦ ì¤‘...")
    qa_gate = DualQAGate()
    qa_result = qa_gate.validate(
        pdf_text=pdf_text,
        processed_text=rag_markdown,
        source="law"
    )
    
    progress_bar.progress(100)
    
    # QA Summary ìƒì„± (í…Œì´ë¸” í†µê³„ í¬í•¨)
    qa_summary = generate_qa_summary(
        document_title=document_title,
        pdf_text_len=len(pdf_text),
        processed_text_len=len(rag_markdown),
        parsed_result=parsed_result,
        qa_result=qa_result,
        chunks=chunks,
        table_stats=table_stats if table_structured else None
    )
    
    # âœ… Phase 0.9.5.2: review.mdì— annex_paragraph ë Œë”ë§ í¬í•¨
    review_markdown = to_review_md_basic(
        chunks=chunks,
        parsed_result=parsed_result,
        base_markdown=rag_markdown,
        qa_summary=qa_summary
    )
    
    return {
        'rag_markdown': rag_markdown,
        'review_markdown': review_markdown,
        'chunks': chunks,
        'qa_result': qa_result,
        'is_qa_pass': qa_result.get('is_pass', False),
        'parsed_result': parsed_result,
        'qa_summary': qa_summary,
        'table_stats': table_stats if table_structured else None,
        'table_structured': table_structured,
        'mode': 'LawMode'
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    st.set_page_config(
        page_title="PRISM Phase 0.9.5.2",
        page_icon="ğŸ”·",
        layout="wide"
    )
    
    st.title("ğŸ”· PRISM Phase 0.9.5.2")
    st.markdown("**Progressive Reasoning & Intelligence for Structured Materials**")
    st.markdown("**ë¬¸ì„œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Table Detection Enhanced + Review Rendering)**")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'processing_result' not in st.session_state:
        st.session_state.processing_result = None
    if 'processed_file_name' not in st.session_state:
        st.session_state.processed_file_name = None
    
    # ë©”ì¸ ì˜ì—­: ë¬¸ì„œ ì²˜ë¦¬
    st.header("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬")
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['pdf'],
        help="ì¸ì‚¬ê·œì •, ë²•ë ¹ ë“± ê·œì • ë¬¸ì„œ"
    )
    
    if not uploaded_file:
        st.info("ğŸ‘† PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì²˜ë¦¬ê°€ ì‹œì‘ë©ë‹ˆë‹¤.")
        
        # Phase 0.9.5.2 ì•ˆë‚´
        st.markdown("---")
        st.subheader("âœ… Phase 0.9.5.2 ê°œì„ ì‚¬í•­")
        st.success("""
        **í‘œ íŒì • ê°•í™” + Review ë Œë”ë§ ê°œì„ **
        
        1. **í‘œ íŒì • ë¡œì§ ê°•í™”** (P1-A í•´ê²°)
           - ìˆ«ìì—´ 3ê°œ ì´ìƒ OR ì •ë ¬ëœ ë¼ì¸ 2ê°œ ì´ìƒ
           - ë³„í‘œ1 í‘œê°€ table_rowsë¡œ ì •í™•íˆ ë¶„ë¥˜
           - TableParser êµ¬ì¡°í™” ê°€ëŠ¥ ìƒíƒœ ë³´ì¥
        
        2. **annex_paragraph ë Œë”ë§ ì¶”ê°€** (P1-B í•´ê²°)
           - ë³„í‘œ2 ë³¸ë¬¸ì´ review.mdì— ëª…í™•íˆ í‘œì‹œ
           - ì‚¬ìš©ì "ë‚´ìš© ì—†ìŒ" ì˜¤í•´ í•´ì†Œ
        
        3. **Phase 0.9.5.1 ì•ˆì •ì„± 100% ìœ ì§€**
           - ì†ì‹¤ë¥  0.4% ìœ ì§€
           - DualQA 99.8% ìœ ì§€
           - ê°œì •ì´ë ¥ 17ê±´ ìœ ì§€
        
        **ìˆ˜ì • ë²”ìœ„**: 25ì¤„ (ìµœì†Œ ì¹¨ìŠµ)
        **GPT ë¯¸ì†¡ë‹˜ ìŠ¹ì¸**: âœ…
        """)
        
        return
    
    # íŒŒì¼ì´ ë°”ë€Œë©´ ê²°ê³¼ ì´ˆê¸°í™”
    if st.session_state.processed_file_name != uploaded_file.name:
        st.session_state.processing_result = None
        st.session_state.processed_file_name = uploaded_file.name
    
    # âœ… Phase 0.9.8.4: ë¬¸ì„œ íƒ€ì… ìë™ ë¶„ë¥˜
    if CLASSIFIER_AVAILABLE:
        # PDF í…ìŠ¤íŠ¸ ë¯¸ë¦¬ ì¶”ì¶œ
        temp_pdf = safe_temp_path(uploaded_file.name)
        with open(temp_pdf, 'wb') as f:
            f.write(uploaded_file.getbuffer())
        
        pdf_text_preview = extract_pdf_text_layer(str(temp_pdf))
        
        # ë¬¸ì„œ íƒ€ì… ìë™ ë¶„ë¥˜
        classifier = DocumentClassifier()
        doc_type, confidence, features = classifier.classify(
            pdf_text_preview,
            page_count=len(pdf_text_preview.split('\n\n'))
        )
        
        # ìë™ ì¶”ì²œ ëª¨ë“œ
        if doc_type in ['law_annex', 'form']:
            recommended_mode = "LawMode (ê·œì •/ë²•ë ¹)"
        else:
            recommended_mode = "VLM Mode (ì¼ë°˜ ë¬¸ì„œ)"
        
        st.info(f"ğŸ¯ ìë™ ê°ì§€: **{doc_type}** (ì‹ ë¢°ë„: {confidence:.0%})")
        st.info(f"ğŸ“‹ ì¶”ì²œ ëª¨ë“œ: **{recommended_mode}**")
        
        # ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ (ìë™ ì¶”ì²œ ë°˜ì˜)
        mode = st.radio(
            "ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ",
            ["LawMode (ê·œì •/ë²•ë ¹)", "VLM Mode (ì¼ë°˜ ë¬¸ì„œ)"],
            index=0 if 'LawMode' in recommended_mode else 1,
            help=f"âœ… ìë™ ê°ì§€: {doc_type} ({confidence:.0%}) | LawMode: ì¡°ë¬¸ êµ¬ì¡° íŒŒì‹± | VLM Mode: ì´ë¯¸ì§€ ê¸°ë°˜"
        )
    else:
        # ê¸°ì¡´ ìˆ˜ë™ ì„ íƒ
        mode = st.radio(
            "ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ",
            ["LawMode (ê·œì •/ë²•ë ¹)", "VLM Mode (ì¼ë°˜ ë¬¸ì„œ)"],
            help="LawMode: ì¡°ë¬¸ êµ¬ì¡° íŒŒì‹± + í‘œ íŒì • ê°•í™” | VLM Mode: ì´ë¯¸ì§€ ê¸°ë°˜ ì²˜ë¦¬"
        )
    
    # ğŸ”§ Phase 0.9.8.4 Bug Fix: process_mode ë³€ìˆ˜ ì„ ì–¸
    process_mode = "law" if "LawMode" in mode else "vlm"
    
    # ì²˜ë¦¬ ë²„íŠ¼
    if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary"):
        try:
            # ì„ì‹œ íŒŒì¼ ì €ì¥
            temp_pdf = safe_temp_path(uploaded_file.name)
            with open(temp_pdf, 'wb') as f:
                f.write(uploaded_file.getbuffer())
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            pdf_text = extract_pdf_text_layer(str(temp_pdf))
            
            # ì²˜ë¦¬ ëª¨ë“œ ë¶„ê¸°
            if process_mode == "law":
                result = process_document_law_mode(
                    str(temp_pdf),
                    pdf_text,
                    uploaded_file.name
                )
            else:
                result = process_document_vlm_mode(
                    str(temp_pdf),
                    pdf_text
                )
            
            # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
            st.session_state.processing_result = result
            
            st.success(f"âœ… ì²˜ë¦¬ ì™„ë£Œ ({result['mode']})")
            
        except Exception as e:
            st.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return
    
    # ì„¸ì…˜ì— ì €ì¥ëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ
    if st.session_state.processing_result:
        result = st.session_state.processing_result
        
        # DualQA ê²°ê³¼
        qa_result = result['qa_result']
        if result['is_qa_pass']:
            st.success(f"âœ… DualQA í†µê³¼ (ì»¤ë²„ë¦¬ì§€: {qa_result.get('text_coverage', 0)*100:.1f}%)")
        else:
            st.warning(f"âš ï¸ DualQA ê²½ê³  (ì»¤ë²„ë¦¬ì§€: {qa_result.get('text_coverage', 0)*100:.1f}%)")
        
        # Phase 0.9.5.2: í…Œì´ë¸” êµ¬ì¡°í™” ê²°ê³¼
        if result.get('table_structured'):
            st.subheader("ğŸ“Š í…Œì´ë¸” êµ¬ì¡°í™” ê²°ê³¼")
            for table_id, count in result['table_stats'].items():
                st.write(f"- {table_id}: {count}í–‰")
        elif TABLE_PARSER_AVAILABLE:
            st.info("â„¹ï¸ í…Œì´ë¸” êµ¬ì¡°í™”: ë¯¸ì ìš© (ê¸°ì¡´ í˜•ì‹ ìœ ì§€)")
        
        # QA Summary í‘œì‹œ
        if result.get('qa_summary'):
            st.subheader("ğŸ“‹ QA Summary")
            st.code(result['qa_summary'], language=None)
        
        # ì²­í¬ í†µê³„
        st.subheader("ğŸ“Š ì²­í¬ í†µê³„")
        chunks = result['chunks']
        st.write(f"- ì´ ì²­í¬: {len(chunks)}ê°œ")
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for chunk in chunks:
            ctype = chunk.get('metadata', {}).get('type', 'unknown')
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
        
        for ctype, count in sorted(type_counts.items()):
            st.write(f"  - {ctype}: {count}ê°œ")
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        st.markdown("---")
        st.subheader("ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.download_button(
                label="ğŸ“¥ engine.md",
                data=result['rag_markdown'],
                file_name="engine.md",
                mime="text/markdown",
                key="download_engine"
            )
        
        with col2:
            chunks_json = json.dumps(result['chunks'], ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“¥ chunks.json",
                data=chunks_json,
                file_name="chunks.json",
                mime="application/json",
                key="download_chunks"
            )
        
        with col3:
            review_md = result.get('review_markdown', result['rag_markdown'])
            st.download_button(
                label="ğŸ“¥ review.md",
                data=review_md,
                file_name="review.md",
                mime="text/markdown",
                key="download_review"
            )


if __name__ == "__main__":
    main()