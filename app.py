"""
app.py - PRISM Phase 0.6.3 "Clean View"
GPT í”¼ë“œë°± 100% ë°˜ì˜: MD ë Œë”ë§ ê°œì„ 

âœ… Phase 0.6.3 í•«í”½ìŠ¤:
1. to_review_md() ì™„ì „ ì¬ì„¤ê³„ (íƒ€ì… ê¸°ë°˜ ë Œë”ë§)
2. íƒ€ì´í‹€/ê°œì •ì´ë ¥/ì¥/ì¡°ë¬¸ ëª…í™•í•œ í—¤ë” í‘œì‹œ
3. section_order ê¸°ì¤€ ì •ë ¬ ë³´ì¥

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT ìµœì¢… í”¼ë“œë°±
Date: 2025-11-14
Version: Phase 0.6.3
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import json
import uuid
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ëª¨ë“ˆ Import
try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.hybrid_extractor import HybridExtractor
    from core.semantic_chunker import SemanticChunker
    from core.dual_qa_gate import DualQAGate, extract_pdf_text_layer
    from core.utils_fs import safe_temp_path, safe_remove
    
    logger.info("âœ… ëª¨ë“ˆ import ì„±ê³µ (Phase 0.6.3)")
    
except Exception as e:
    logger.error(f"âŒ Import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()

# LawParser Import (Phase 0.6.3)
try:
    from core.law_parser import LawParser
    LAW_MODE_AVAILABLE = True
    logger.info("âœ… LawParser ë¡œë“œ ì„±ê³µ (Phase 0.6.3)")
except ImportError:
    LAW_MODE_AVAILABLE = False
    logger.warning("âš ï¸ LawParser ë¯¸ì„¤ì¹˜")

# DocumentProfile Import
try:
    from core.document_profile import auto_detect_profile, get_profile
    PROFILE_AVAILABLE = True
    logger.info("âœ… DocumentProfile ë¡œë“œ ì„±ê³µ")
except ImportError:
    PROFILE_AVAILABLE = False
    logger.warning("âš ï¸ DocumentProfile ë¯¸ì„¤ì¹˜")


# ============================================
# âœ… Phase 0.6.3: ì™„ì „ ì¬ì„¤ê³„ëœ to_review_md()
# ============================================

def to_review_md(chunks: list, markdown: str = None) -> str:
    """
    âœ… Phase 0.6.3: ë¦¬ë·°ìš© Markdown ìƒì„± (GPT ê¶Œì¥ ë°©ì‹)
    
    íƒ€ì…ë³„ë¡œ ì •í™•íˆ ë Œë”ë§:
    - title â†’ # íƒ€ì´í‹€
    - amendment_history â†’ ## ê°œì • ì´ë ¥ + ë¦¬ìŠ¤íŠ¸
    - basic â†’ ## ê¸°ë³¸ì •ì‹ 
    - chapter â†’ ## ì œNì¥ ì œëª©
    - article â†’ ### ì œNì¡°(ì œëª©)
    
    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        markdown: ì‚¬ìš© ì•ˆ í•¨ (í•˜ìœ„ í˜¸í™˜)
    
    Returns:
        Markdown ë¬¸ìì—´
    """
    lines = []
    
    # âœ… section_order ê¸°ì¤€ ì •ë ¬ (í•„ìˆ˜!)
    chunks_sorted = sorted(chunks, key=lambda c: c['metadata'].get('section_order', 999))
    
    for chunk in chunks_sorted:
        meta = chunk["metadata"]
        text = chunk["content"]
        chunk_type = meta["type"]
        
        # 1. íƒ€ì´í‹€
        if chunk_type == "title":
            title = meta.get('title', text)
            lines.append(f"# {title}\n")
        
        # 2. ê°œì •ì´ë ¥ (ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬)
        elif chunk_type == "amendment_history":
            lines.append("## ê°œì • ì´ë ¥\n")
            # "ì œ37ì°¨ê°œì •2019.05.27." ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            items = re.split(r'(?=ì œ\d+ì°¨)', text)
            for item in items:
                item = item.strip()
                if item:
                    lines.append(f"- {item}")
            lines.append("")  # ë¹ˆ ì¤„
        
        # 3. ê¸°ë³¸ì •ì‹ 
        elif chunk_type == "basic":
            lines.append("## ê¸°ë³¸ì •ì‹ \n")
            lines.append(text)
            lines.append("")
        
        # 4. ì¥ (Chapter)
        elif chunk_type == "chapter":
            ch_num = meta["chapter_number"]
            ch_title = meta["chapter_title"]
            lines.append(f"## {ch_num} {ch_title}\n")
        
        # 5. ì¡°ë¬¸ (Article)
        elif chunk_type == "article":
            art_num = meta["article_number"]
            art_title = meta["article_title"]
            lines.append(f"### {art_num}({art_title})\n")
            
            # ë³¸ë¬¸ì—ì„œ í—¤ë” ì œê±° (ì¤‘ë³µ ë°©ì§€)
            body = text
            header = f"{art_num}({art_title})"
            if header in body:
                body = body.replace(header, '', 1).strip()
            
            lines.append(body)
            lines.append("")
    
    return "\n".join(lines)


# VLM ëª¨ë“œ ì²˜ë¦¬
def process_document_vlm_mode(pdf_path: str, pdf_text: str, max_pages: int = 20):
    """VLM íŒŒì´í”„ë¼ì¸"""
    
    st.info("ğŸ”¬ VLM ëª¨ë“œ: Azure OpenAI GPT-4 Vision ì²˜ë¦¬ ì¤‘...")
    progress_bar = st.progress(0)
    
    try:
        # PDF ì²˜ë¦¬
        pdf_processor = PDFProcessor()
        pages = pdf_processor.process_pdf(pdf_path)
        progress_bar.progress(25)
        
        if len(pages) > max_pages:
            st.warning(f"âš ï¸ ìµœëŒ€ {max_pages}í˜ì´ì§€ê¹Œì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            pages = pages[:max_pages]
        
        # VLM ì²˜ë¦¬
        vlm_service = VLMServiceV50(provider='azure_openai')
        extractor = HybridExtractor(vlm_service)
        markdown_text = extractor.extract(pages)
        progress_bar.progress(50)
        
        # ì²­í‚¹
        st.info("ğŸ§© ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì¤‘...")
        chunker = SemanticChunker()
        chunks = chunker.chunk(markdown_text)
        st.success(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # DualQA ê²€ì¦
        st.info("ğŸ”¬ DualQA ê²€ì¦ ì¤‘...")
        qa_gate = DualQAGate()
        qa_result = qa_gate.validate(
            pdf_text=pdf_text,
            processed_text=markdown_text,
            source="vlm"
        )
        
        match_rate = qa_result.get('match_rate', 0.0)
        qa_flags = qa_result.get('qa_flags', [])
        is_qa_pass = qa_result.get('is_pass', False)
        
        progress_bar.progress(100)
        
        return {
            'markdown': markdown_text,
            'chunks': chunks,
            'qa_result': qa_result,
            'is_qa_pass': is_qa_pass,
            'mode': 'VLM'
        }
    
    except Exception as e:
        logger.error(f"âŒ VLM ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


# LawMode ì²˜ë¦¬ (Phase 0.6.3)
def process_document_law_mode(pdf_path: str, pdf_text: str, document_title: str):
    """
    âœ… Phase 0.6.3: LawMode íŒŒì´í”„ë¼ì¸
    """
    
    st.info("ğŸ“œ LawMode Phase 0.6.3: ê·œì •/ë²•ë ¹ íŒŒì‹± ì¤‘...")
    progress_bar = st.progress(0)
    
    # DocumentProfile ìë™ ê°ì§€ (ì˜µì…˜)
    if PROFILE_AVAILABLE:
        profile = auto_detect_profile(pdf_text, document_title)
        st.info(f"ğŸ“ ë¬¸ì„œ í”„ë¡œíŒŒì¼: {profile.name}")
    
    # LawParser íŒŒì‹±
    parser = LawParser()
    parsed_result = parser.parse(
        pdf_text=pdf_text,
        document_title=document_title,
        clean_artifacts=True,
        normalize_linebreaks=True
    )
    progress_bar.progress(50)
    
    # ì²­í¬ ë³€í™˜
    chunks = parser.to_chunks(parsed_result)
    progress_bar.progress(75)
    
    # Markdown ìƒì„±
    markdown_lines = []
    
    # ê¸°ë³¸ì •ì‹ 
    if parsed_result['basic_spirit']:
        markdown_lines.append("## ê¸°ë³¸ì •ì‹ \n")
        markdown_lines.append(parsed_result['basic_spirit'])
        markdown_lines.append("")
    
    # ì¥ê³¼ ì¡°ë¬¸ (section_order ê¸°ì¤€ ì •ë ¬)
    for chunk in chunks:
        chunk_type = chunk['metadata']['type']
        
        if chunk_type == 'chapter':
            chapter_num = chunk['metadata']['chapter_number']
            chapter_title = chunk['metadata']['chapter_title']
            markdown_lines.append(f"## {chapter_num} {chapter_title}\n")
        
        elif chunk_type == 'article':
            article_num = chunk['metadata']['article_number']
            article_title = chunk['metadata']['article_title']
            markdown_lines.append(f"### {article_num}({article_title})\n")
            
            # ë³¸ë¬¸ (ì¡°ë¬¸ ë²ˆí˜¸ ì œê±°)
            body = chunk['content']
            if f"{article_num}({article_title})" in body:
                body = body.replace(f"{article_num}({article_title})", '', 1).strip()
            
            markdown_lines.append(body)
            markdown_lines.append("")
    
    markdown_text = '\n'.join(markdown_lines)
    
    # DualQA ê²€ì¦
    st.info("ğŸ”¬ DualQA ê²€ì¦ ì¤‘...")
    qa_gate = DualQAGate()
    qa_result = qa_gate.validate(
        pdf_text=pdf_text,
        processed_text=markdown_text,
        source="lawmode"
    )
    
    match_rate = qa_result.get('match_rate', 0.0)
    qa_flags = qa_result.get('qa_flags', [])
    is_qa_pass = qa_result.get('is_pass', False)
    
    progress_bar.progress(100)
    
    return {
        'markdown': markdown_text,
        'chunks': chunks,
        'qa_result': qa_result,
        'is_qa_pass': is_qa_pass,
        'mode': 'LawMode',
        'parsed_result': parsed_result
    }


# Streamlit UI
def main():
    st.set_page_config(
        page_title="PRISM Phase 0.6.3",
        page_icon="ğŸ”·",
        layout="wide"
    )
    
    st.title("ğŸ”· PRISM Phase 0.6.3 \"Clean View\"")
    st.caption("GPT í”¼ë“œë°± ë°˜ì˜: MD ë Œë”ë§ ì™„ì „ ê°œì„ ")
    
    # ì‚¬ì´ë“œë°”: ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # LawMode í† ê¸€
        use_law_mode = st.checkbox(
            "ğŸ“œ LawMode ì‚¬ìš© (ê·œì •/ë²•ë ¹ ì „ìš©)",
            value=LAW_MODE_AVAILABLE,
            disabled=not LAW_MODE_AVAILABLE,
            help="PDF í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™•í•œ ì¡°ë¬¸ ì¶”ì¶œ"
        )
        
        if not LAW_MODE_AVAILABLE:
            st.warning("âš ï¸ LawParser ë¯¸ì„¤ì¹˜")
        
        st.divider()
        
        # Phase 0.6.3 ë³€ê²½ì‚¬í•­
        with st.expander("âœ¨ Phase 0.6.3 í•«í”½ìŠ¤"):
            st.markdown("""
            **GPT í”¼ë“œë°± 100% ë°˜ì˜:**
            
            ### 1. MD ë Œë”ë§ ì™„ì „ ì¬ì„¤ê³„
            
            **Before (0.6.2):**
            - íƒ€ì´í‹€/ê°œì •ì´ë ¥ì´ "ì•ˆ ë³´ì„"
            - í—¤ë” ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì­‰ ë‚˜ì—´
            
            **After (0.6.3):**
            ```markdown
            # ì¸ì‚¬ê·œì •
            
            ## ê°œì • ì´ë ¥
            - ì œ37ì°¨ê°œì •2019.05.27.
            - ì œ38ì°¨ê°œì •2019.07.01.
            ...
            
            ## ê¸°ë³¸ì •ì‹ 
            ì´ê·œì •ì€í•œêµ­ë†ì–´ì´Œê³µì‚¬ì§ì›ì˜...
            
            ## ì œ1ì¥ ì´ì¹™
            
            ### ì œ1ì¡°(ëª©ì )
            ì´ê·œì •ì€í•œêµ­ë†ì–´ì´Œê³µì‚¬ì§ì›ì—ê²Œ...
            ```
            
            ### 2. ì¥ ì œëª© ì •í™•íˆ íŒŒì‹±
            
            **Before:** `chapter_title: "ì´ì¹™ì œ"`  
            **After:** `chapter_title: "ì´ì¹™"` âœ…
            
            ---
            
            **ë‹¤ìŒ Phase 0.7**: ë„ì–´ì“°ê¸° ì „ìš© ëª¨ë“ˆ
            """)
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ",
        type=['pdf'],
        help="ê·œì •/ë²•ë ¹ ë¬¸ì„œ ê¶Œì¥ (LawMode)"
    )
    
    if not uploaded_file:
        st.info("ğŸ‘† PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”")
        return
    
    # ë¬¸ì„œ ì²˜ë¦¬
    try:
        # PDF ì„ì‹œ ì €ì¥
        pdf_path = safe_temp_path('.pdf')
        with open(pdf_path, 'wb') as f:
            f.write(uploaded_file.read())
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pdf_text = extract_pdf_text_layer(pdf_path)
        
        if not pdf_text:
            st.error("âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            return
        
        # ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ
        if use_law_mode:
            result = process_document_law_mode(
                pdf_path=pdf_path,
                pdf_text=pdf_text,
                document_title=uploaded_file.name
            )
        else:
            result = process_document_vlm_mode(
                pdf_path=pdf_path,
                pdf_text=pdf_text
            )
        
        # ê²°ê³¼ í‘œì‹œ
        st.success(f"âœ… {result['mode']} ì²˜ë¦¬ ì™„ë£Œ!")
        
        # íƒ­ìœ¼ë¡œ ê²°ê³¼ êµ¬ì„±
        tab1, tab2, tab3, tab4 = st.tabs([
            "ğŸ“Š í’ˆì§ˆ ê²€ì¦",
            "ğŸ“„ Markdown",
            "ğŸ§© ì²­í¬ (JSON)",
            "ğŸ“– ë¦¬ë·°ìš©"
        ])
        
        with tab1:
            st.subheader("ğŸ”¬ DualQA ê²€ì¦ ê²°ê³¼")
            
            qa_result = result['qa_result']
            match_rate = qa_result['match_rate']
            qa_flags = qa_result['qa_flags']
            is_pass = result['is_qa_pass']
            source_label = qa_result.get('source', result['mode'])
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    "ë§¤ì¹­ë¥ ",
                    f"{match_rate*100:.1f}%",
                    delta=f"{match_rate*100-95:.1f}%" if match_rate < 0.95 else None,
                    delta_color="normal" if match_rate >= 0.95 else "inverse"
                )
            
            with col2:
                st.metric("PDF ì¡°ë¬¸", len(qa_result['pdf_articles']))
            
            with col3:
                st.metric(f"{source_label} ì¡°ë¬¸", len(qa_result['processed_articles']))
            
            if is_pass:
                st.success("âœ… QA í†µê³¼ - ì›ë¬¸ ì¼ì¹˜")
            else:
                st.error("âŒ QA ì‹¤íŒ¨ - ì›ë¬¸ ë¶ˆì¼ì¹˜")
                
                if qa_flags:
                    st.warning(f"âš ï¸ QA í”Œë˜ê·¸: {qa_flags}")
            
            # LawMode ìƒì„¸ ì •ë³´
            if use_law_mode and 'parsed_result' in result:
                st.divider()
                st.subheader("ğŸ“œ LawMode Phase 0.6.3 íŒŒì‹± ìƒì„¸")
                
                parsed = result['parsed_result']
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì´ ì¥(Chapter)", parsed['total_chapters'])
                with col2:
                    st.metric("ì´ ì¡°ë¬¸", parsed['total_articles'])
                with col3:
                    st.metric("ê¸°ë³¸ì •ì‹ ", f"{len(parsed['basic_spirit'])}ì")
                
                # ì¥ ëª©ë¡ í‘œì‹œ
                if parsed['chapters']:
                    with st.expander("ğŸ“‚ ì¥(Chapter) ëª©ë¡"):
                        for chapter in parsed['chapters']:
                            st.write(f"- **{chapter.number}** {chapter.title}")
        
        with tab2:
            st.subheader("ğŸ“„ Markdown")
            st.code(result['markdown'], language='markdown')
            st.download_button(
                "ğŸ’¾ Markdown ë‹¤ìš´ë¡œë“œ",
                data=result['markdown'],
                file_name=f"{uploaded_file.name}_phase063.md",
                mime="text/markdown"
            )
        
        with tab3:
            st.subheader("ğŸ§© ì²­í¬ (JSON)")
            
            # ì²­í¬ í†µê³„
            chunk_types = {}
            for chunk in result['chunks']:
                chunk_type = chunk['metadata']['type']
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
            
            st.write(f"**ì´ ì²­í¬ ìˆ˜**: {len(result['chunks'])}ê°œ")
            st.write(f"**ì²­í¬ íƒ€ì… ë¶„í¬**: {chunk_types}")
            
            st.json(result['chunks'], expanded=False)
            
            st.download_button(
                "ğŸ’¾ JSON ë‹¤ìš´ë¡œë“œ",
                data=json.dumps(result['chunks'], ensure_ascii=False, indent=2),
                file_name=f"{uploaded_file.name}_chunks_phase063.json",
                mime="application/json"
            )
        
        with tab4:
            st.subheader("ğŸ“– ë¦¬ë·°ìš© Markdown (Phase 0.6.3 ê°œì„ )")
            
            # âœ… Phase 0.6.3: ì¬ì„¤ê³„ëœ to_review_md() ì‚¬ìš©
            review_md = to_review_md(result['chunks'])
            
            st.markdown(review_md)
            st.download_button(
                "ğŸ’¾ ë¦¬ë·°ìš© ë‹¤ìš´ë¡œë“œ",
                data=review_md,
                file_name=f"{uploaded_file.name}_review_phase063.md",
                mime="text/markdown"
            )
    
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        if 'pdf_path' in locals():
            safe_remove(pdf_path)


if __name__ == '__main__':
    main()