"""
PRISM POC - Main Application with OCR
PDF + ì´ë¯¸ì§€ ì²˜ë¦¬ ë° VLM ë³€í™˜ (OCR í†µí•©)
"""

import streamlit as st
import asyncio
from pathlib import Path
import json
from datetime import datetime
import logging
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from core.pdf_processor import PDFProcessor
from core.vlm_service import VLMService

# ElementClassifierëŠ” ê¸°ì¡´ í”„ë¡œì íŠ¸ì— ìˆìŒ (import ìœ ì§€)
try:
    from core.element_classifier import ElementClassifier
except ImportError:
    logger.warning("âš ï¸ ElementClassifierë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ë¥˜ ì‚¬ìš©")
    ElementClassifier = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PRISM POC",
    page_icon="ğŸ”·",
    layout="wide"
)


def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'processed_results' not in st.session_state:
        st.session_state.processed_results = None
    if 'pdf_processor' not in st.session_state:
        st.session_state.pdf_processor = None
    if 'vlm_service' not in st.session_state:
        st.session_state.vlm_service = None
    if 'classifier' not in st.session_state:
        st.session_state.classifier = None


def display_header():
    """í—¤ë” í‘œì‹œ"""
    st.title("ğŸ”· PRISM POC")
    st.markdown("**Progressive Reasoning & Intelligence for Structured Materials**")
    st.markdown("VLM ê¸°ë°˜ ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ (OCR í†µí•©)")
    st.divider()


def display_file_upload():
    """íŒŒì¼ ì—…ë¡œë“œ UI"""
    st.subheader("ğŸ“¤ Step 1: ë¬¸ì„œ ì—…ë¡œë“œ")
    
    file_type = st.radio(
        "íŒŒì¼ íƒ€ì… ì„ íƒ",
        ["PDF ë¬¸ì„œ", "ì´ë¯¸ì§€ íŒŒì¼"],
        horizontal=True
    )
    
    if file_type == "PDF ë¬¸ì„œ":
        uploaded_file = st.file_uploader(
            "PDF íŒŒì¼ ì„ íƒ",
            type=['pdf'],
            help="ìµœëŒ€ 10MB, 20í˜ì´ì§€ ì´í•˜"
        )
    else:
        uploaded_file = st.file_uploader(
            "ì´ë¯¸ì§€ íŒŒì¼ ì„ íƒ",
            type=['png', 'jpg', 'jpeg'],
            help="ì°¨íŠ¸, í‘œ, ë‹¤ì´ì–´ê·¸ë¨ ì´ë¯¸ì§€"
        )
    
    return uploaded_file, file_type


async def process_pdf_async(pdf_path: Path, use_ocr: bool = True):
    """PDF ë¹„ë™ê¸° ì²˜ë¦¬"""
    
    # ì´ˆê¸°í™”
    if st.session_state.pdf_processor is None:
        with st.spinner("ğŸš€ PDF í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì¤‘..."):
            st.session_state.pdf_processor = PDFProcessor()
    
    if st.session_state.vlm_service is None:
        with st.spinner("ğŸš€ VLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘..."):
            st.session_state.vlm_service = VLMService()
    
    if st.session_state.classifier is None:
        if ElementClassifier:
            with st.spinner("ğŸš€ ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì¤‘..."):
                st.session_state.classifier = ElementClassifier()
        else:
            st.session_state.classifier = None
    
    pdf_processor = st.session_state.pdf_processor
    vlm_service = st.session_state.vlm_service
    classifier = st.session_state.classifier
    
    # í˜ì´ì§€ ìˆ˜ í™•ì¸
    page_count = pdf_processor.get_page_count(pdf_path)
    
    st.info(f"ğŸ“„ ì´ {page_count}í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘...")
    
    # ì§„í–‰ ìƒíƒœ í‘œì‹œ
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    results = []
    
    for page_num in range(1, page_count + 1):
        try:
            status_text.text(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {page_num}/{page_count} í˜ì´ì§€...")
            
            # 1) PDF í˜ì´ì§€ ì²˜ë¦¬ (ì´ë¯¸ì§€ + OCR)
            page_data = pdf_processor.process_page(pdf_path, page_num, use_ocr)
            
            # 2) Element íƒ€ì… ë¶„ë¥˜
            if classifier:
                try:
                    # ElementClassifierì˜ ì‹¤ì œ ë©”ì„œë“œëª… í™•ì¸ í•„ìš”
                    if hasattr(classifier, 'classify_element'):
                        element_type = classifier.classify_element(page_data['image'])
                    elif hasattr(classifier, 'classify'):
                        element_type = classifier.classify(page_data['image'])
                    else:
                        element_type = 'image'  # ê¸°ë³¸ê°’
                except Exception as e:
                    logger.warning(f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
                    element_type = 'image'
            else:
                element_type = 'image'
            
            # 3) VLM ìº¡ì…˜ ìƒì„±
            vlm_result = await vlm_service.generate_caption(
                page_data['image_base64'],
                element_type,
                page_data.get('extracted_text', '')
            )
            
            # ê²°ê³¼ í†µí•©
            result = {
                'page_number': page_num,
                'element_type': element_type,
                'extracted_text': page_data.get('extracted_text', ''),
                'ocr_confidence': page_data.get('ocr_confidence', 0.0),
                'caption': vlm_result['caption'],
                'caption_confidence': vlm_result['confidence'],
                'image': page_data['image'],
                'usage': vlm_result['usage']
            }
            
            results.append(result)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = page_num / page_count
            progress_bar.progress(progress)
            
        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            st.error(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    progress_bar.empty()
    status_text.empty()
    
    return results


def display_results(results: list):
    """ê²°ê³¼ í‘œì‹œ"""
    if not results:
        return
    
    st.success(f"âœ… {len(results)}ê°œ í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ!")
    
    # ìš”ì•½ í†µê³„
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ í˜ì´ì§€", len(results))
    
    with col2:
        avg_ocr_conf = sum(r.get('ocr_confidence', 0) for r in results) / len(results)
        st.metric("í‰ê·  OCR ì‹ ë¢°ë„", f"{avg_ocr_conf:.2f}")
    
    with col3:
        avg_caption_conf = sum(r.get('caption_confidence', 0) for r in results) / len(results)
        st.metric("í‰ê·  VLM ì‹ ë¢°ë„", f"{avg_caption_conf:.2f}")
    
    with col4:
        total_tokens = sum(
            r.get('usage', {}).get('input_tokens', 0) + 
            r.get('usage', {}).get('output_tokens', 0)
            for r in results
        )
        st.metric("ì´ í† í°", f"{total_tokens:,}")
    
    st.divider()
    
    # í˜ì´ì§€ë³„ ìƒì„¸ ê²°ê³¼
    st.subheader("ğŸ“‘ Step 3: í˜ì´ì§€ë³„ ê²°ê³¼")
    
    for result in results:
        page_num = result['page_number']
        element_type = result.get('element_type', 'unknown')
        
        # Element íƒ€ì… ì•„ì´ì½˜
        type_icons = {
            'chart': 'ğŸ“Š',
            'table': 'ğŸ“‹',
            'diagram': 'ğŸ”·',
            'image': 'ğŸ–¼ï¸'
        }
        icon = type_icons.get(element_type, 'ğŸ“„')
        
        with st.expander(
            f"{icon} Page {page_num} - {element_type.upper()} "
            f"(OCR: {result.get('ocr_confidence', 0):.2f}, "
            f"VLM: {result.get('caption_confidence', 0):.2f})"
        ):
            # íƒ­ìœ¼ë¡œ êµ¬ë¶„
            tab1, tab2, tab3, tab4 = st.tabs([
                "ğŸ“ OCR í…ìŠ¤íŠ¸",
                "ğŸ¨ VLM ì„¤ëª…",
                "ğŸ–¼ï¸ ì›ë³¸ ì´ë¯¸ì§€",
                "â„¹ï¸ ë©”íƒ€ë°ì´í„°"
            ])
            
            with tab1:
                st.markdown("### ğŸ“ OCR ì¶”ì¶œ í…ìŠ¤íŠ¸")
                extracted_text = result.get('extracted_text', '')
                
                if extracted_text:
                    st.text_area(
                        "ì¶”ì¶œëœ í…ìŠ¤íŠ¸",
                        extracted_text,
                        height=300,
                        key=f"ocr_{page_num}"
                    )
                    st.caption(f"ğŸ“Š ì´ {len(extracted_text)}ì ì¶”ì¶œ")
                else:
                    st.warning("âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            with tab2:
                st.markdown("### ğŸ¨ VLM ìƒì„± ì„¤ëª…")
                st.markdown(result.get('caption', 'N/A'))
                
                # ì‚¬ìš©ëŸ‰ ì •ë³´
                usage = result.get('usage', {})
                if usage:
                    col1, col2 = st.columns(2)
                    with col1:
                        st.caption(f"ğŸ“¥ Input: {usage.get('input_tokens', 0):,} tokens")
                    with col2:
                        st.caption(f"ğŸ“¤ Output: {usage.get('output_tokens', 0):,} tokens")
            
            with tab3:
                st.markdown("### ğŸ–¼ï¸ ì›ë³¸ ì´ë¯¸ì§€")
                if 'image' in result:
                    st.image(result['image'], use_container_width=True)
                else:
                    st.warning("ì´ë¯¸ì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            with tab4:
                st.markdown("### â„¹ï¸ ë©”íƒ€ë°ì´í„°")
                metadata = {
                    "í˜ì´ì§€ ë²ˆí˜¸": page_num,
                    "Element íƒ€ì…": element_type,
                    "OCR ì‹ ë¢°ë„": f"{result.get('ocr_confidence', 0):.3f}",
                    "VLM ì‹ ë¢°ë„": f"{result.get('caption_confidence', 0):.3f}",
                    "OCR í…ìŠ¤íŠ¸ ê¸¸ì´": f"{len(result.get('extracted_text', ''))} ì",
                    "VLM ìº¡ì…˜ ê¸¸ì´": f"{len(result.get('caption', ''))} ì",
                    "Input Tokens": f"{result.get('usage', {}).get('input_tokens', 0):,}",
                    "Output Tokens": f"{result.get('usage', {}).get('output_tokens', 0):,}"
                }
                st.json(metadata)
    
    st.divider()
    
    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    st.subheader("ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # JSON ë‹¤ìš´ë¡œë“œ
        json_data = prepare_json_export(results)
        st.download_button(
            label="ğŸ“¥ JSON ë‹¤ìš´ë¡œë“œ",
            data=json_data,
            file_name=f"prism_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )
    
    with col2:
        # Markdown ë‹¤ìš´ë¡œë“œ
        markdown_data = prepare_markdown_export(results)
        st.download_button(
            label="ğŸ“¥ Markdown ë‹¤ìš´ë¡œë“œ",
            data=markdown_data,
            file_name=f"prism_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown"
        )


def prepare_json_export(results: list) -> str:
    """JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì¤€ë¹„"""
    export_data = {
        "metadata": {
            "exported_at": datetime.now().isoformat(),
            "total_pages": len(results),
            "version": "PRISM POC v1.0 (OCR)"
        },
        "summary": {
            "total_pages": len(results),
            "avg_ocr_confidence": sum(r.get('ocr_confidence', 0) for r in results) / len(results) if results else 0,
            "avg_vlm_confidence": sum(r.get('caption_confidence', 0) for r in results) / len(results) if results else 0,
            "total_tokens": sum(
                r.get('usage', {}).get('input_tokens', 0) + 
                r.get('usage', {}).get('output_tokens', 0)
                for r in results
            )
        },
        "pages": [
            {
                "page_number": r['page_number'],
                "element_type": r.get('element_type'),
                "extracted_text": r.get('extracted_text'),
                "ocr_confidence": r.get('ocr_confidence'),
                "caption": r.get('caption'),
                "caption_confidence": r.get('caption_confidence'),
                "usage": r.get('usage')
            }
            for r in results
        ]
    }
    
    return json.dumps(export_data, ensure_ascii=False, indent=2)


def prepare_markdown_export(results: list) -> str:
    """Markdown í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì¤€ë¹„"""
    md = f"""# PRISM POC ë¶„ì„ ê²°ê³¼ (OCR í†µí•©)

**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**ì´ í˜ì´ì§€**: {len(results)}  
**í‰ê·  OCR ì‹ ë¢°ë„**: {sum(r.get('ocr_confidence', 0) for r in results) / len(results):.3f}  
**í‰ê·  VLM ì‹ ë¢°ë„**: {sum(r.get('caption_confidence', 0) for r in results) / len(results):.3f}  

---

"""
    
    for result in results:
        page_num = result['page_number']
        element_type = result.get('element_type', 'unknown')
        
        md += f"""## ğŸ“„ Page {page_num} - {element_type.upper()}

### ğŸ“ OCR ì¶”ì¶œ í…ìŠ¤íŠ¸

```
{result.get('extracted_text', 'N/A')}
```

### ğŸ¨ VLM ìƒì„± ì„¤ëª…

{result.get('caption', 'N/A')}

**ë©”íƒ€ë°ì´í„°**:
- OCR ì‹ ë¢°ë„: {result.get('ocr_confidence', 0):.3f}
- VLM ì‹ ë¢°ë„: {result.get('caption_confidence', 0):.3f}
- Input Tokens: {result.get('usage', {}).get('input_tokens', 0):,}
- Output Tokens: {result.get('usage', {}).get('output_tokens', 0):,}

---

"""
    
    return md


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    init_session_state()
    display_header()
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file, file_type = display_file_upload()
    
    if uploaded_file:
        # OCR ì‚¬ìš© ì—¬ë¶€
        use_ocr = st.checkbox(
            "ğŸ“ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‚¬ìš©",
            value=True,
            help="PaddleOCRë¡œ ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
        )
        
        # ì²˜ë¦¬ ì‹œì‘ ë²„íŠ¼
        if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary"):
            # ì„ì‹œ íŒŒì¼ ì €ì¥
            temp_dir = Path("temp")
            temp_dir.mkdir(exist_ok=True)
            
            file_path = temp_dir / uploaded_file.name
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            try:
                if file_type == "PDF ë¬¸ì„œ":
                    # PDF ì²˜ë¦¬
                    with st.spinner("ğŸ“„ PDF ì²˜ë¦¬ ì¤‘..."):
                        results = asyncio.run(
                            process_pdf_async(file_path, use_ocr)
                        )
                    
                    st.session_state.processed_results = results
                    display_results(results)
                
                else:
                    # ì´ë¯¸ì§€ ì²˜ë¦¬ (ë‹¨ì¼)
                    st.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ëŠ¥ì€ ê³§ ì¶”ê°€ë©ë‹ˆë‹¤.")
            
            except Exception as e:
                st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                logger.exception("Processing failed")
            
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if file_path.exists():
                    file_path.unlink()
    
    # ì´ì „ ê²°ê³¼ í‘œì‹œ
    elif st.session_state.processed_results:
        st.info("ğŸ’¡ ì´ì „ ì²˜ë¦¬ ê²°ê³¼ë¥¼ í‘œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        display_results(st.session_state.processed_results)


if __name__ == "__main__":
    main()