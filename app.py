"""
app.py - PRISM Final Version (GPT Feedback Applied)
GPT 6ê°€ì§€ í•«í”½ìŠ¤ ë°˜ì˜
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import time
import json
import gc
import base64
from PIL import Image

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.hybrid_extractor import HybridExtractor
    from core.typo_normalizer_safe import TypoNormalizer
    from core.post_merge_normalizer_safe import PostMergeNormalizer
    from core.semantic_chunker import SemanticChunker
    
    logger.info("âœ… ëª¨ë“ˆ import ì„±ê³µ")
    
except Exception as e:
    logger.error(f"âŒ Import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()


def image_to_base64(image_data):
    """ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ base64ë¡œ ë³€í™˜"""
    if isinstance(image_data, tuple):
        image_data = image_data[0]
    
    if isinstance(image_data, Image.Image):
        from io import BytesIO
        buffered = BytesIO()
        image_data.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()
    
    if isinstance(image_data, str):
        return image_data
    
    if isinstance(image_data, bytes):
        return base64.b64encode(image_data).decode()
    
    raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_data)}")


def process_pdf_direct(pdf_path, pdf_processor, vlm_service):
    """ì§ì ‘ PDF ì²˜ë¦¬"""
    
    # 1. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    images = pdf_processor.pdf_to_images(pdf_path)
    logger.info(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ì¶”ì¶œ")
    
    # 2. HybridExtractor ì´ˆê¸°í™”
    extractor = HybridExtractor(vlm_service, pdf_path)
    logger.info(f"âœ… HybridExtractor ì´ˆê¸°í™”")
    
    # 3. í˜ì´ì§€ë³„ ì²˜ë¦¬
    all_markdown = []
    
    for page_num, image_data in enumerate(images, 1):
        logger.info(f"ğŸ”„ í˜ì´ì§€ {page_num}/{len(images)} ì²˜ë¦¬ ì¤‘...")
        
        try:
            image_b64 = image_to_base64(image_data)
            result = extractor.extract(image_b64, page_num)
            
            # GPT í”¼ë“œë°±: í‚¤ëŠ” 'content'
            page_md = result.get('content', '').strip()
            
            if page_md:
                all_markdown.append(page_md)
                logger.info(f"   âœ… í˜ì´ì§€ {page_num}: {len(page_md)}ì ì¶”ê°€")
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
    
    # í˜ì´ì§€ ë³‘í•©
    markdown = "\n\n".join(all_markdown)
    logger.info(f"âœ… ë³‘í•© ì™„ë£Œ: {len(markdown)}ì (í˜ì´ì§€ {len(all_markdown)}ê°œ)")
    
    if len(markdown) < 100:
        raise ValueError(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(markdown)}ì)")
    
    # 4. ì •ê·œí™”
    normalizer = TypoNormalizer()
    markdown = normalizer.normalize(markdown)
    
    post_normalizer = PostMergeNormalizer()
    markdown = post_normalizer.normalize(markdown)
    logger.info(f"âœ… ì •ê·œí™” ì™„ë£Œ: {len(markdown)}ì")
    
    # 5. ì²­í‚¹
    chunker = SemanticChunker()
    chunks = chunker.chunk(markdown)
    logger.info(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # 6. GPT í”¼ë“œë°±: í’ˆì§ˆ ì ìˆ˜ ì œê±° (Golden File ë¯¸ê²€ì¦)
    checklist = None  # í’ˆì§ˆ ì ìˆ˜ ì—†ìŒ
    
    return {
        'success': True,
        'markdown': markdown,
        'chunks': chunks,
        'checklist': checklist,
        'elapsed_time': 0
    }


def main():
    st.title("ğŸ”· PRISM - ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œ")
    
    st.warning("""
    âš ï¸ **Phase 0.3.4 P0 (ì‹¤í—˜ìš© PoC)**
    - Golden File ë¯¸ê²€ì¦ ìƒíƒœì…ë‹ˆë‹¤
    - í’ˆì§ˆ ì ìˆ˜ëŠ” í‘œì‹œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
    """)
    
    try:
        pdf_processor = PDFProcessor()
        vlm_service = VLMServiceV50(provider="azure_openai")
        logger.info("âœ… ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    uploaded_file = st.file_uploader("ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ", type=['pdf'])
    
    if uploaded_file:
        file_key = f"{uploaded_file.name}_{uploaded_file.size}"
        
        if 'last_file' not in st.session_state or st.session_state['last_file'] != file_key:
            with st.spinner("ğŸ”„ ì²˜ë¦¬ ì¤‘... (VLM í˜¸ì¶œ)"):
                temp_path = None
                try:
                    temp_path = Path(f"temp_{int(time.time())}_{uploaded_file.name}")
                    temp_path.write_bytes(uploaded_file.getvalue())
                    
                    result = process_pdf_direct(str(temp_path), pdf_processor, vlm_service)
                    
                    st.session_state['last_file'] = file_key
                    st.session_state['result'] = result
                    
                except Exception as e:
                    logger.error(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
                    st.error(f"âŒ ì˜¤ë¥˜: {e}")
                    st.session_state['result'] = None
                    
                finally:
                    gc.collect()
                    
                    # GPT í”¼ë“œë°±: Windows íŒŒì¼ ë½ ì¬ì‹œë„
                    if temp_path and temp_path.exists():
                        for attempt in range(3):
                            try:
                                time.sleep(0.2)
                                temp_path.unlink()
                                logger.info("âœ… ì„ì‹œ íŒŒì¼ ì‚­ì œ")
                                break
                            except PermissionError as e:
                                if attempt == 2:
                                    logger.warning(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        result = st.session_state.get('result')
        
        if result and result.get('success'):
            st.success("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
            
            # GPT í”¼ë“œë°±: í’ˆì§ˆ ì ìˆ˜ ì œê±°
            st.info("ğŸ’¡ í’ˆì§ˆ ì ìˆ˜ëŠ” Golden File ì—°ë™ í›„ í‘œì‹œë©ë‹ˆë‹¤")
            
            # ê²°ê³¼
            markdown = result.get('markdown', '')
            chunks = result.get('chunks', [])
            
            if markdown:
                st.subheader("ğŸ“ Markdown ê²°ê³¼")
                preview = markdown[:1000]
                if len(markdown) > 1000:
                    preview += "\n\n... (ìƒëµ)"
                # GPT í”¼ë“œë°±: label ë¹„ì–´ìˆìŒ ê²½ê³  ì œê±°
                st.text_area(
                    "ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°",
                    preview,
                    height=300,
                    label_visibility="collapsed"
                )
            
            if chunks:
                st.subheader(f"âœ‚ï¸ ì²­í¬ ê²°ê³¼ (ì´ {len(chunks)}ê°œ)")
                for i, chunk in enumerate(chunks[:3], 1):
                    with st.expander(f"ì²­í¬ {i}"):
                        st.json(chunk.get('metadata', {}))
                        st.text(chunk.get('content', ''))
            
            # ë‹¤ìš´ë¡œë“œ
            st.subheader("ğŸ“¥ ë‹¤ìš´ë¡œë“œ")
            col1, col2 = st.columns(2)
            
            with col1:
                if markdown:
                    st.download_button(
                        "ğŸ“ Markdown",
                        markdown,
                        f"{uploaded_file.name.replace('.pdf', '')}_markdown.md",
                        mime="text/markdown"
                    )
            
            with col2:
                if chunks:
                    chunks_json = json.dumps(chunks, ensure_ascii=False, indent=2)
                    st.download_button(
                        "ğŸ“¦ JSON",
                        chunks_json,
                        f"{uploaded_file.name.replace('.pdf', '')}_chunks.json",
                        mime="application/json"
                    )
        
        elif result is not None:
            st.error("âŒ ì²˜ë¦¬ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()