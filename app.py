"""
app.py - PRISM Phase 0.3.4 P2.5.3 ìµœì¢… ì™„ì„± (ìƒìš© ë°°í¬ ë²„ì „)
GPT í”¼ë“œë°± 100% ë°˜ì˜ + ë§ˆì°½ìˆ˜ì‚°íŒ€ ì£¼ë„ ì„¤ê³„

âœ… ê°œì„  ì‚¬í•­:
1. ì œ4ì¡° ëˆ„ë½ ë°©ì§€ (í—¤ë” ì ˆëŒ€ ë³´í˜¸ + ìë™ QA)
2. OCR ì˜¤íƒˆì 23ê°œ íŒ¨í„´ (ìœ ì—°í•œ ì •ê·œì‹)
3. ë¦¬ë·°ìš© íŒŒì¼ ìƒì„± (*_review.md)
4. ì •ê·œì‹ ê²½ê³  ì™„ì „ ì œê±° (raw string)
5. ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ + UUID ìºì‹œ ë¬´íš¨í™”

Author: ë§ˆì°½ìˆ˜ì‚°íŒ€ (ìµœë™í˜„ Frontend Lead) + GPT ë³´ì •
Date: 2025-11-13
Version: Phase 0.3.4 P2.5.3 (Production Ready)
"""

import streamlit as st
import logging
import sys
from pathlib import Path
import time
import json
import gc
import base64
import uuid
import re
from PIL import Image
from datetime import datetime

# ============================================
# ë¡œê¹… ì„¤ì •
# ============================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('prism.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ============================================
# ëª¨ë“ˆ Import
# ============================================
try:
    from core.pdf_processor import PDFProcessor
    from core.vlm_service import VLMServiceV50
    from core.hybrid_extractor import HybridExtractor
    from core.typo_normalizer_safe import TypoNormalizer
    from core.post_merge_normalizer_safe import PostMergeNormalizer
    from core.semantic_chunker import SemanticChunker
    from core.utils_fs import safe_temp_path, safe_remove
    
    logger.info("âœ… ëª¨ë“ˆ import ì„±ê³µ (Phase 0.3.4 P2.5.3)")
    
except Exception as e:
    logger.error(f"âŒ Import ì‹¤íŒ¨: {e}")
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()


# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def image_to_base64(image_data):
    """ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ base64ë¡œ ë³€í™˜"""
    if isinstance(image_data, tuple):
        image_data = image_data[0]
    
    if isinstance(image_data, Image.Image):
        from io import BytesIO
        buffered = BytesIO()
        image_data.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()
    
    if isinstance(image_data, str):
        return image_data
    
    if isinstance(image_data, bytes):
        return base64.b64encode(image_data).decode()
    
    raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_data)}")


def to_review_md(chunks: list, markdown: str) -> str:
    """
    âœ… GPT í”¼ë“œë°±: ë¦¬ë·°ìš© Markdown ìƒì„±
    
    ëª©ì : ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    - ì¡°ë¬¸ë§ˆë‹¤ ### í—¤ë”
    - í•­ëª©(â‘ â‘¡) ì•ì— ì¤„ë°”ê¿ˆ
    - 80ì ì†Œí”„íŠ¸ ë˜í•‘
    """
    lines = []
    
    # ê°œì • ì´ë ¥ (ìƒë‹¨ì— ìœ ì§€)
    if 'ì œ37ì°¨ ê°œì •' in markdown or 'ì œì •' in markdown:
        header_end = markdown.find('ê¸°ë³¸ì •ì‹ ')
        if header_end > 0:
            header = markdown[:header_end].strip()
            lines.append("# ì¸ì‚¬ê·œì •")
            lines.append("")
            lines.append("## ê°œì • ì´ë ¥")
            lines.append(header)
            lines.append("")
    
    for ch in chunks:
        # âœ… None ì²˜ë¦¬ ì¶”ê°€
        t = ch["metadata"].get("title") or ""
        t = t.strip() if t else ""
        
        b = ch["metadata"].get("boundary") or ""
        b = b.strip() if b else ""
        
        btype = ch["metadata"].get("type", "")
        
        # í—¤ë” ìƒì„±
        if btype == 'chapter':
            head = f"## {b}"
        elif btype == 'basic':
            head = "## ê¸°ë³¸ì •ì‹ "
        elif b:
            if t:
                head = f"### {b}{t})"
            else:
                head = f"### {b}"
        else:
            head = "### ë‚´ìš©"
        
        # ë³¸ë¬¸ ì²˜ë¦¬
        body = ch.get("content", "")
        body = body.strip() if body else ""
        
        # âœ… í•­ëª© ì¤„ë°”ê¿ˆ ë³´ì •
        body = re.sub(r'\s*(â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©)', r'\n\1', body)
        body = re.sub(r'\s*(?=^\d+\.)', r'\n', body, flags=re.M)
        
        lines += [head, "", body, ""]
    
    return "\n".join(lines).strip()


def process_pdf_direct(pdf_path, pdf_processor, vlm_service):
    """
    PDF ì§ì ‘ ì²˜ë¦¬ (Phase 0.3.4 P2.5.3)
    
    í”Œë¡œìš°:
    1. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    2. HybridExtractorë¡œ í˜ì´ì§€ë³„ ì²˜ë¦¬
    3. Markdown ë³‘í•©
    4. ì˜¤íƒˆì ì •ê·œí™” (33ê°€ì§€ íŒ¨í„´)
    5. í›„ì²˜ë¦¬ ì •ê·œí™”
    6. ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (ì œ4ì¡° ëˆ„ë½ ë°©ì§€ + ìë™ QA)
    """
    
    # 1. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    st.info("ğŸ“„ PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘...")
    images = pdf_processor.pdf_to_images(pdf_path)
    logger.info(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ì¶”ì¶œ")
    st.success(f"âœ… {len(images)}ê°œ í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
    
    # 2. HybridExtractor ì´ˆê¸°í™”
    extractor = HybridExtractor(vlm_service, pdf_path)
    logger.info(f"âœ… HybridExtractor ì´ˆê¸°í™”")
    
    # 3. í˜ì´ì§€ë³„ ì¶”ì¶œ ë° ë³‘í•©
    st.info(f"ğŸ” {len(images)}ê°œ í˜ì´ì§€ ì¶”ì¶œ ì¤‘...")
    
    markdown_parts = []
    progress_bar = st.progress(0)
    
    for idx, image_data in enumerate(images, 1):
        try:
            # ì´ë¯¸ì§€ â†’ Base64
            if not isinstance(image_data, str):
                image_base64 = image_to_base64(image_data)
            else:
                image_base64 = image_data
            
            # í˜ì´ì§€ ì¶”ì¶œ
            page_result = extractor.extract(image_base64, idx)
            
            # Markdown ë³‘í•©
            if page_result and 'content' in page_result:
                markdown_parts.append(page_result['content'])
                logger.info(f"   âœ… í˜ì´ì§€ {idx}: {len(page_result['content'])}ì")
            else:
                logger.warning(f"   âš ï¸ í˜ì´ì§€ {idx}: ë‚´ìš© ì—†ìŒ")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.progress(idx / len(images))
            
        except Exception as e:
            logger.error(f"   âŒ í˜ì´ì§€ {idx} ì˜¤ë¥˜: {e}")
            st.warning(f"âš ï¸ í˜ì´ì§€ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    progress_bar.empty()
    
    # Markdown ë³‘í•©
    markdown = '\n\n'.join(markdown_parts)
    logger.info(f"âœ… Markdown ë³‘í•© ì™„ë£Œ: {len(markdown)}ì")
    st.success(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(markdown):,}ì")
    
    # 4. ì˜¤íƒˆì ì •ê·œí™” (33ê°€ì§€ íŒ¨í„´)
    st.info("ğŸ”§ ì˜¤íƒˆì ì •ê·œí™” ì¤‘...")
    normalizer = TypoNormalizer()
    normalized_md = normalizer.normalize(markdown)
    logger.info(f"âœ… ì •ê·œí™” ì™„ë£Œ: {len(normalized_md)}ì")
    st.success(f"âœ… ì˜¤íƒˆì êµì • ì™„ë£Œ")
    
    # 5. í›„ì²˜ë¦¬ ì •ê·œí™”
    post_normalizer = PostMergeNormalizer()
    final_md = post_normalizer.normalize(normalized_md)
    logger.info(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ: {len(final_md)}ì")
    
    # 6. ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (ì œ4ì¡° ëˆ„ë½ ë°©ì§€ + ìë™ QA)
    st.info("âœ‚ï¸ ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì¤‘...")
    chunker = SemanticChunker()
    chunks = chunker.chunk(final_md)
    logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
    st.success(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
    
    return {
        'markdown': final_md,
        'chunks': chunks,
        'metadata': {
            'total_pages': len(images),
            'total_chars': len(final_md),
            'total_chunks': len(chunks),
            'processing_time': datetime.now().isoformat()
        }
    }


# ============================================
# Streamlit UI
# ============================================

def main():
    """ë©”ì¸ UI"""
    
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="PRISM Phase 0.3.4 P2.5.3",
        page_icon="ğŸ”·",
        layout="wide"
    )
    
    # âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'last_result' not in st.session_state:
        st.session_state.last_result = None
    if 'last_filename' not in st.session_state:
        st.session_state.last_filename = None
    
    # í—¤ë”
    st.title("ğŸ”· PRISM Phase 0.3.4 P2.5.3")
    st.caption("ì°¨ì„¸ëŒ€ ì§€ëŠ¥í˜• ë¬¸ì„œ ì´í•´ í”Œë«í¼ - ìµœì¢… ì™„ì„± (ìƒìš© ë°°í¬ ë²„ì „)")
    
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” - ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        st.subheader("ğŸ“Š ë²„ì „ ì •ë³´")
        st.info("""
**Phase 0.3.4 P2.5.3 (Production Ready)**
- âœ… ì œ4ì¡° ëˆ„ë½ ë°©ì§€ (í—¤ë” ì ˆëŒ€ ë³´í˜¸)
- âœ… OCR ì˜¤íƒˆì 23ê°œ íŒ¨í„´
- âœ… ë¦¬ë·°ìš© íŒŒì¼ ìƒì„± (ì‚¬ëŒ ëˆˆ ì¹œí™”)
- âœ… ìë™ QA ê²Œì´íŠ¸ (ëˆ„ë½ ì¡°ë¬¸ ê°ì§€)
- âœ… ì •ê·œì‹ ê²½ê³  ì™„ì „ ì œê±°

**GPT ë³´ì • + ë§ˆì°½ìˆ˜ì‚°íŒ€ ì£¼ë„ ì„¤ê³„**
        """)
        
        st.markdown("---")
        
        st.subheader("ğŸ”§ VLM ì„¤ì •")
        provider = st.selectbox(
            "VLM Provider",
            ["azure_openai"],
            index=0
        )
        
        max_pages = st.slider(
            "ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€",
            min_value=1,
            max_value=20,
            value=20,
            help="í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜"
        )
        
        st.markdown("---")
        
        st.subheader("ğŸ“– ì‚¬ìš© ë°©ë²•")
        st.markdown("""
1. PDF íŒŒì¼ ì—…ë¡œë“œ (ìµœëŒ€ 10MB)
2. 'ì²˜ë¦¬ ì‹œì‘' ë²„íŠ¼ í´ë¦­
3. ê²°ê³¼ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
   - RAGìš©: Markdown + JSON
   - ê²€ìˆ˜ìš©: Review Markdown
        """)
    
    # ë©”ì¸ ì˜ì—­
    st.header("ğŸ“„ PDF ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type=['pdf'],
        help="ìµœëŒ€ 10MB, 20í˜ì´ì§€ê¹Œì§€ ì§€ì›"
    )
    
    if uploaded_file is not None:
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        file_size = len(uploaded_file.getvalue()) / (1024 * 1024)
        st.info(f"ğŸ“ íŒŒì¼ëª…: {uploaded_file.name} ({file_size:.2f} MB)")
        
        # íŒŒì¼ í¬ê¸° ì²´í¬
        if file_size > 10:
            st.error("âŒ íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤!")
            return
        
        # ì²˜ë¦¬ ì‹œì‘ ë²„íŠ¼
        if st.button("ğŸš€ ì²˜ë¦¬ ì‹œì‘", type="primary"):
            
            # ì•ˆì „í•œ ì„ì‹œ íŒŒì¼ ìƒì„±
            pdf_path = safe_temp_path(".pdf")
            
            try:
                # ì„ì‹œ íŒŒì¼ ì €ì¥
                with open(pdf_path, 'wb') as f:
                    f.write(uploaded_file.getvalue())
                
                logger.info(f"âœ… ì„ì‹œ íŒŒì¼ ì €ì¥: {pdf_path}")
                
                # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
                with st.spinner("ğŸ”§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘..."):
                    pdf_processor = PDFProcessor()
                    vlm_service = VLMServiceV50(provider=provider)
                    logger.info("âœ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # PDF ì²˜ë¦¬
                start_time = time.time()
                
                with st.spinner("ğŸ”„ PDF ì²˜ë¦¬ ì¤‘..."):
                    result = process_pdf_direct(pdf_path, pdf_processor, vlm_service)
                
                processing_time = time.time() - start_time
                
                # âœ… ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
                st.session_state.last_result = result
                st.session_state.last_filename = uploaded_file.name
                
                # ì„±ê³µ ë©”ì‹œì§€
                st.success(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! ({processing_time:.1f}ì´ˆ)")
                
            except Exception as e:
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            finally:
                # ì•ˆì „í•œ íŒŒì¼ ì‚­ì œ
                safe_remove(pdf_path)
                gc.collect()
    
    # âœ… ê²°ê³¼ í‘œì‹œ (ì„¸ì…˜ ìƒíƒœì—ì„œ)
    if st.session_state.last_result is not None:
        result = st.session_state.last_result
        filename = st.session_state.last_filename
        base_name = filename.replace('.pdf', '')
        
        # ê²°ê³¼ í‘œì‹œ
        st.markdown("---")
        st.header("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
        
        # ë©”íƒ€ë°ì´í„°
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ì´ í˜ì´ì§€", result['metadata']['total_pages'])
        
        with col2:
            st.metric("ì¶”ì¶œ ë¬¸ì", f"{result['metadata']['total_chars']:,}ì")
        
        with col3:
            st.metric("ìƒì„± ì²­í¬", result['metadata']['total_chunks'])
        
        # íƒ­ìœ¼ë¡œ ê²°ê³¼ êµ¬ë¶„
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ RAG Markdown", "âœ‚ï¸ ì²­í¬ JSON", "ğŸ“„ ë¦¬ë·°ìš© MD", "ğŸ“Š ë¶„ì„"])
        
        with tab1:
            st.subheader("RAGìš© Markdown (ì„ë² ë”© ìµœì í™”)")
            st.text_area(
                "Markdown ë‚´ìš©",
                result['markdown'],
                height=400,
                key="markdown_display"
            )
            
            # âœ… UUIDë¡œ ìºì‹œ ë¬´íš¨í™”
            download_id = uuid.uuid4().hex[:8]
            st.download_button(
                label="ğŸ“¥ RAGìš© Markdown ë‹¤ìš´ë¡œë“œ",
                data=result['markdown'],
                file_name=f"{base_name}_{download_id}.md",
                mime="text/markdown",
                key=f"md_download_{download_id}"
            )
        
        with tab2:
            st.subheader(f"ì²­í¬ JSON ({len(result['chunks'])}ê°œ)")
            
            # ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
            for i, chunk in enumerate(result['chunks'][:3], 1):
                with st.expander(f"ì²­í¬ {i} ë¯¸ë¦¬ë³´ê¸° - {chunk['metadata']['type']} ({chunk['metadata']['char_count']}ì)"):
                    st.markdown(f"**ê²½ê³„:** `{chunk['metadata']['boundary']}`")
                    if chunk['metadata'].get('title'):
                        st.markdown(f"**ì œëª©:** {chunk['metadata']['title']}")
                    st.text_area(
                        "ë‚´ìš©",
                        chunk['content'][:200] + "...",
                        height=100,
                        key=f"chunk_preview_{i}"
                    )
            
            if len(result['chunks']) > 3:
                st.info(f"ğŸ’¡ ì „ì²´ {len(result['chunks'])}ê°œ ì²­í¬ëŠ” JSON íŒŒì¼ì—ì„œ í™•ì¸í•˜ì„¸ìš”")
            
            # âœ… UUIDë¡œ ìºì‹œ ë¬´íš¨í™”
            chunks_json = json.dumps(result['chunks'], ensure_ascii=False, indent=2)
            download_id = uuid.uuid4().hex[:8]
            st.download_button(
                label="ğŸ“¥ ì²­í¬ JSON ë‹¤ìš´ë¡œë“œ",
                data=chunks_json,
                file_name=f"{base_name}_{download_id}.json",
                mime="application/json",
                key=f"json_download_{download_id}"
            )
        
        with tab3:
            st.subheader("ğŸ“„ ë¦¬ë·°ìš© Markdown (ì‚¬ëŒ ëˆˆ ì¹œí™”)")
            st.info("âœ… ì¡°ë¬¸ë§ˆë‹¤ í—¤ë” + í•­ëª© ì¤„ë°”ê¿ˆ + ì½ê¸° ì¢‹ì€ í˜•ì‹")
            
            # âœ… GPT í”¼ë“œë°±: ë¦¬ë·°ìš© íŒŒì¼ ìƒì„±
            review_md = to_review_md(result['chunks'], result['markdown'])
            
            st.text_area(
                "ë¦¬ë·°ìš© ë‚´ìš©",
                review_md[:1000] + "\n\n... (í•˜ë‹¨ ìƒëµ, ì „ì²´ëŠ” ë‹¤ìš´ë¡œë“œì—ì„œ í™•ì¸)",
                height=400,
                key="review_display"
            )
            
            # âœ… UUIDë¡œ ìºì‹œ ë¬´íš¨í™”
            download_id = uuid.uuid4().hex[:8]
            st.download_button(
                label="ğŸ“¥ ë¦¬ë·°ìš© Markdown ë‹¤ìš´ë¡œë“œ",
                data=review_md,
                file_name=f"{base_name}_review_{download_id}.md",
                mime="text/markdown",
                key=f"review_download_{download_id}"
            )
        
        with tab4:
            st.subheader("ğŸ“Š ì²­í¬ í’ˆì§ˆ ë¶„ì„")
            
            # íƒ€ì…ë³„ ë¶„í¬
            type_counts = {}
            for chunk in result['chunks']:
                chunk_type = chunk['metadata']['type']
                type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
            
            st.markdown("### íƒ€ì…ë³„ ë¶„í¬")
            for chunk_type, count in sorted(type_counts.items()):
                percentage = (count / len(result['chunks'])) * 100
                st.markdown(f"- **{chunk_type}**: {count}ê°œ ({percentage:.1f}%)")
            
            # í¬ê¸° ë¶„ì„
            sizes = [c['metadata']['char_count'] for c in result['chunks']]
            if sizes:
                st.markdown("### í¬ê¸° ë¶„ì„")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    avg_size = sum(sizes) / len(sizes)
                    st.metric("í‰ê·  ì²­í¬ í¬ê¸°", f"{avg_size:.0f}ì")
                
                with col2:
                    st.metric("ìµœì†Œ í¬ê¸°", f"{min(sizes)}ì")
                
                with col3:
                    st.metric("ìµœëŒ€ í¬ê¸°", f"{max(sizes)}ì")
            
            # âœ… ì¡°ë¬¸ í—¤ë” ëª©ë¡
            st.markdown("### ê°ì§€ëœ ì¡°ë¬¸ í—¤ë”")
            headers = []
            for chunk in result['chunks']:
                boundary = chunk['metadata'].get('boundary', '')
                if boundary and ('ì¡°' in boundary or 'ì¥' in boundary):
                    headers.append(boundary)
            
            if headers:
                st.markdown(", ".join(headers))
            else:
                st.warning("ì¡°ë¬¸ í—¤ë”ë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
    
    else:
        st.info("ğŸ‘† PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”")
        
        # ìƒ˜í”Œ ê²°ê³¼ í‘œì‹œ
        st.markdown("---")
        st.header("ğŸ“– Phase 0.3.4 P2.5.3 ì£¼ìš” ê°œì„ ì‚¬í•­")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ¯ ì œ4ì¡° ëˆ„ë½ ë°©ì§€")
            st.code("""
# Before
ì œ4ì¡°(ì„ìš©ê¶Œ) â†’ JSONì—ì„œ ëˆ„ë½ âŒ

# After (GPT ë³´ì •)
í—¤ë” ì ˆëŒ€ ë³´í˜¸:
- 50ì ë¯¸ë§Œ ì¡°ë¬¸ë„ í—¤ë”ë¡œ ê°„ì£¼
- ë³‘í•© ì‹œ ì–‘ìª½ ëª¨ë‘ í—¤ë” ì•„ë‹ˆì–´ì•¼ ë³‘í•©
- ìë™ QA: MD vs JSON í—¤ë” ë¹„êµ

â†’ ì œ4ì¡° ì™„ì „ ë³´ì¡´ âœ…
            """, language="python")
            
            st.subheader("ğŸ”§ OCR ì˜¤íƒˆì 23ê°œ")
            st.code(r"""
# ìœ ì—°í•œ ì •ê·œì‹ (ì‹¤ì¸¡ ê¸°ë°˜)
ì±„\s*ì±„\s*ê·œì • â†’ ì±„ìš©ê·œì •
ì¸í„´\s*ì±„\s*í†µìƒ â†’ ì¸í„´Â·í†µìƒ
ì„¤\s*ì°¨\s*ì  â†’ ì ˆì°¨ì 
ì§ì›\s*ë°©ì‹\s*ì ˆì°¨ â†’ ì§ê¶Œë©´ì§
... ì™¸ 19ê°œ
            """, language="python")
        
        with col2:
            st.subheader("ğŸ“„ ë¦¬ë·°ìš© íŒŒì¼ ìƒì„±")
            st.code("""
# RAGìš© (AI ìµœì í™”)
ì œ1ì¡°(ëª©ì ) ì´ ê·œì •ì€...

# ë¦¬ë·°ìš© (ì‚¬ëŒ ëˆˆ ì¹œí™”)
### ì œ1ì¡°(ëª©ì )

ì´ ê·œì •ì€ í•œêµ­ë†ì–´ì´Œê³µì‚¬ ì§ì›ì—ê²Œ
ì ìš©í•  ì¸ì‚¬ê´€ë¦¬ì˜ ê¸°ì¤€ì„ ì •í•˜ì—¬...

â‘  ì œ1í•­
â‘¡ ì œ2í•­
            """, language="markdown")
            
            st.subheader("ğŸ” ìë™ QA ê²Œì´íŠ¸")
            st.code("""
# ëˆ„ë½ ì¡°ë¬¸ ìë™ ê°ì§€
MD í—¤ë”: {ì œ1ì¡°, ì œ2ì¡°, ..., ì œ92ì¡°}
JSON í—¤ë”: {ì œ1ì¡°, ì œ2ì¡°, ..., ì œ92ì¡°}

âš ï¸ ëˆ„ë½: ì—†ìŒ
âœ… QA í†µê³¼
            """, language="python")
    
    # í‘¸í„°
    st.markdown("---")
    st.caption("ğŸ”· PRISM Phase 0.3.4 P2.5.3 (ìƒìš© ë°°í¬ ë²„ì „) | ë§ˆì°½ìˆ˜ì‚°íŒ€ + GPT ë³´ì •")


if __name__ == "__main__":
    main()